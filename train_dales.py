"""
DALES æ•°æ®é›†è®­ç»ƒè„šæœ¬ï¼ˆçº¯ Python é…ç½®ï¼‰

æ–°åŠŸèƒ½æ¼”ç¤ºï¼š
- âœ… è‡ªåŠ¨ç±»åˆ«æƒé‡è®¡ç®—å’ŒåŠ æƒé‡‡æ ·
- âœ… ä¸­æ–‡ç±»åˆ«åç§°æ”¯æŒï¼ˆéªŒè¯æ—¥å¿— + hparams.yamlï¼‰
- âœ… CSV æ—¥å¿—è®°å½•ï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
- âœ… åŠ¨æ€æ‰¹æ¬¡é‡‡æ ·
- âœ… å¤šæ–‡ä»¶ LAS é¢„æµ‹æ”¯æŒ
- âœ… æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batchè®­ç»ƒï¼‰

æ¢¯åº¦ç´¯ç§¯è¯´æ˜ï¼š
- åŸç†ï¼šæ¯Nä¸ªbatchè®¡ç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œç´¯ç§¯Næ¬¡åæ‰æ›´æ–°å‚æ•°
- ä¼˜åŠ¿ï¼šåœ¨æ˜¾å­˜å—é™æ—¶æ¨¡æ‹Ÿæ›´å¤§çš„batch size
- ä¸åŠ¨æ€batchå®Œå…¨å…¼å®¹ï¼š
  * åŠ¨æ€batchæ§åˆ¶æ¯ä¸ªbatchçš„ç‚¹æ•°ï¼ˆmax_pointsï¼‰
  * æ¢¯åº¦ç´¯ç§¯æ§åˆ¶æ›´æ–°é¢‘ç‡ï¼ˆaccumulate_grad_batchesï¼‰
  * ç­‰æ•ˆbatch = max_points Ã— accumulate_grad_batches

é…ç½®å»ºè®®ï¼š
- å°æ˜¾å­˜(8GB):  max_points=100K, accumulate=4  â†’ 400Kç‚¹/æ›´æ–°
- ä¸­æ˜¾å­˜(16GB): max_points=150K, accumulate=2  â†’ 300Kç‚¹/æ›´æ–°
- å¤§æ˜¾å­˜(24GB): max_points=200K, accumulate=1  â†’ 200Kç‚¹/æ›´æ–°

æ¨ç†åŠ é€Ÿä¼˜åŒ–ï¼š
- å¤šè¿›ç¨‹åŠ è½½: NUM_WORKERS=4 (é¿å…æ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆ)
- å¤§batchæ¨ç†: max_points_inference=600K (æ— æ¢¯åº¦ï¼Œå¯ç”¨3-4å€è®­ç»ƒbatch)
- è‡ªåŠ¨ä¼˜åŒ–: Lightning 2.5+ é»˜è®¤ä½¿ç”¨ inference_mode (æ¯” no_grad æ›´å¿«)
- TF32åŠ é€Ÿ: å…¨å±€å¯ç”¨ï¼Œè®­ç»ƒå’Œæ¨ç†éƒ½ç”Ÿæ•ˆ
"""

import os
import sys
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import CSVLogger

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pointsuite.data import BinPklDataModule
from pointsuite.data.transforms import (
    RandomRotate, RandomScale, RandomFlip, RandomJitter,
    AddExtremeOutliers, Collect, ToTensor, CenterShift
)
from pointsuite.tasks import SemanticSegmentationTask
from pointsuite.models import PointTransformerV2, SegHead
from pointsuite.utils.callbacks import SemanticPredictLasWriter, AutoEmptyCacheCallback
from pointsuite.utils.progress_bar import CustomProgressBar


def main():
    # ========================================================================
    # é…ç½®
    # ========================================================================
    
    # æ•°æ®
    TRAIN_DATA = r"E:\data\DALES\dales_las\bin\test"
    TEST_DATA = r"E:\data\DALES\dales_las\bin\test"
    OUTPUT_DIR = r"E:\data\DALES\dales_las\bin\result"
    
    CLASS_MAPPING = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7}
    CLASS_NAMES = ['åœ°é¢', 'æ¤è¢«', 'è½¦è¾†', 'å¡è½¦', 'ç”µçº¿', 'ç¯±ç¬†', 'æ†çŠ¶ç‰©', 'å»ºç­‘']
    NUM_CLASSES = 8
    IGNORE_LABEL = -1
    
    # è®­ç»ƒ
    MAX_EPOCHS = 2
    BATCH_SIZE = 4
    NUM_WORKERS = 0  # å¤šè¿›ç¨‹æ•°æ®åŠ è½½ï¼ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†
    LEARNING_RATE = 0.001
    MAX_POINTS = 150000
    MAX_POINTS_INFERENCE = 300000  # æ¨ç†æ—¶ä½¿ç”¨æ›´å¤§batchï¼ˆæ— æ¢¯åº¦ï¼Œæ˜¾å­˜å ç”¨å°‘ï¼‰
    ACCUMULATE_GRAD_BATCHES = 4  # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯4ä¸ªbatchæ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œæ¨¡æ‹Ÿæ›´å¤§batch
    
    pl.seed_everything(42)
    
    # if torch.cuda.is_available():
    #     torch.set_float32_matmul_precision('high')
    #     torch.backends.cuda.matmul.allow_tf32 = True
    #     torch.backends.cudnn.allow_tf32 = True
    
    print("\n" + "=" * 80)
    print(f"DALES è¯­ä¹‰åˆ†å‰²è®­ç»ƒ - {NUM_CLASSES} ç±»")
    print("=" * 80)
    
    # ========================================================================
    # æ•°æ®å¢å¼º
    # ========================================================================
    
    train_transforms = [
        CenterShift(),  # ä¸­å¿ƒåŒ–åæ ‡
        RandomRotate(angle=[-180, 180], axis='z', p=0.5),
        RandomScale(scale=[0.9, 1.1]),
        RandomFlip(p=0.5),
        RandomJitter(sigma=0.01, clip=0.05),
        AddExtremeOutliers(
            ratio=0.001, height_range=(-10, 100), height_mode='bimodal',
            intensity_range=(0, 1), color_value=(128, 128, 128),
            class_label='ignore', p=0.5
        ),
        Collect(keys=['coord', 'class'], offset_key={'offset': 'coord'},
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    val_transforms = [
        CenterShift(),  # ä¸­å¿ƒåŒ–åæ ‡
        Collect(keys=['coord', 'class'], offset_key={'offset': 'coord'},
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    predict_transforms = [
        CenterShift(),  # ä¸­å¿ƒåŒ–åæ ‡
        Collect(keys=['coord', 'indices', 'bin_file', 'bin_path', 'pkl_path'],
                offset_key={'offset': 'coord'}, feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    # ========================================================================
    # DataModule
    # ========================================================================
    
    datamodule = BinPklDataModule(
        train_data=TRAIN_DATA,
        val_data=TEST_DATA,
        test_data=None,
        predict_data=TEST_DATA,
        assets=['coord', 'echo', 'class'],
        class_mapping=CLASS_MAPPING,
        class_names=CLASS_NAMES,
        ignore_label=IGNORE_LABEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True,
        use_dynamic_batch=True,
        max_points=MAX_POINTS,
        use_dynamic_batch_inference=True,
        max_points_inference=MAX_POINTS_INFERENCE,
        use_weighted_sampler=True,  # å¯ç”¨åŠ æƒé‡‡æ ·
        class_weights=None,  # None = è‡ªåŠ¨ä»æ•°æ®é›†è®¡ç®—
        train_loop=1,
        val_loop=1,
        test_loop=2,
        predict_loop=2,
        train_transforms=train_transforms,
        val_transforms=val_transforms,
        test_transforms=val_transforms,
        predict_transforms=predict_transforms,
    )
    
    datamodule.print_info()
    
    # ========================================================================
    # æ¨¡å‹
    # ========================================================================
    
    # ä½¿ç”¨é…ç½®å­—å…¸å®šä¹‰æ¨¡å‹ç»“æ„ï¼Œè€Œä¸æ˜¯ç›´æ¥å®ä¾‹åŒ–å¯¹è±¡
    # è¿™æ ·å¯ä»¥é¿å… PyTorch Lightning çš„ "attribute is already saved" è­¦å‘Š
    # å¹¶ä¸”è®© checkpoint æ›´è½»é‡ã€æ›´è§„èŒƒ
    model_config = {
        'backbone': {
            'class_path': 'pointsuite.models.PointTransformerV2',
            'init_args': {
                'in_channels': 5,
                'patch_embed_depth': 1,
                'patch_embed_channels': 24,
                'patch_embed_groups': 6,
                'patch_embed_neighbours': 24,
                'enc_depths': (1, 1, 1, 1),
                'enc_channels': (48, 96, 192, 256),
                'enc_groups': (6, 12, 24, 32),
                'enc_neighbours': (16, 16, 16, 16),
                'dec_depths': (1, 1, 1, 1),
                'dec_channels': (24, 48, 96, 192),
                'dec_groups': (4, 6, 12, 24),
                'dec_neighbours': (16, 16, 16, 16),
                'grid_sizes': (1, 2.5, 7.5, 15),
                'attn_qkv_bias': True,
                'pe_multiplier': False,
                'pe_bias': True,
                'attn_drop_rate': 0.0,
                'drop_path_rate': 0.2,
                'unpool_backend': "interp",
            }
        },
        'head': {
            'class_path': 'pointsuite.models.SegHead',
            'init_args': {
                'in_channels': 24,
                'num_classes': NUM_CLASSES
            }
        }
    }
    
    # æŸå¤±å‡½æ•°ï¼ˆä¸ä½¿ç”¨ç±»åˆ«æƒé‡ï¼Œè®©åŠ æƒé‡‡æ ·å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
    loss_configs = [
        {
            "name": "ce_loss",
            "class_path": "pointsuite.models.losses.CrossEntropyLoss",
            "init_args": {"ignore_index": IGNORE_LABEL},
            "weight": 1.0,
        },
        {
            "name": "lovasz_loss",
            "class_path": "pointsuite.models.losses.LovaszLoss",
            "init_args": {"mode": "multiclass", "ignore_index": IGNORE_LABEL},
            "weight": 1.0,
        },
    ]
    
    metric_configs = [
        {
            "name": "overall_accuracy",
            "class_path": "torchmetrics.classification.MulticlassAccuracy",
            "init_args": {"num_classes": NUM_CLASSES, "ignore_index": IGNORE_LABEL, "average": "micro"},
        },
        {
            "name": "mean_iou",
            "class_path": "torchmetrics.classification.MulticlassJaccardIndex",
            "init_args": {"num_classes": NUM_CLASSES, "ignore_index": IGNORE_LABEL, "average": "macro"},
        },
    ]
    
    task = SemanticSegmentationTask(
        model_config=model_config,  # ä¼ å…¥é…ç½®å­—å…¸
        learning_rate=LEARNING_RATE,
        class_mapping=CLASS_MAPPING,
        class_names=CLASS_NAMES,
        loss_configs=loss_configs,
        metric_configs=metric_configs,
    )
    
    # ä¼˜åŒ–å™¨
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS, eta_min=1e-6)
        return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "interval": "epoch", "frequency": 1}}
    
    import types
    task.configure_optimizers = types.MethodType(configure_optimizers, task)
    
    # ========================================================================
    # å›è°ƒå’Œ Trainer
    # ========================================================================
    
    callbacks = [
        ModelCheckpoint(monitor='mean_iou', mode='max', save_top_k=3,
                       filename='dales-{epoch:02d}-{mean_iou:.4f}', verbose=True),
        EarlyStopping(monitor='mean_iou', patience=20, mode='max', verbose=True, 
                     check_on_train_epoch_end=False),  # ğŸ”¥ ä¿®å¤ï¼šåœ¨éªŒè¯ç»“æŸæ—¶æ£€æŸ¥ï¼Œè€Œä¸æ˜¯è®­ç»ƒç»“æŸæ—¶
        LearningRateMonitor(logging_interval='step'),
        SemanticPredictLasWriter(output_dir=OUTPUT_DIR, save_logits=False, auto_infer_reverse_mapping=True),
        CustomProgressBar(refresh_rate=1),  # è‡ªå®šä¹‰è¿›åº¦æ¡
        AutoEmptyCacheCallback(slowdown_threshold=3.0, absolute_threshold=1.5, clear_interval=500, warmup_steps=10, verbose=True),  # è‡ªåŠ¨æ¸…ç†æ˜¾å­˜
    ]
    
    csv_logger = CSVLogger(save_dir='./outputs/dales', name='csv_logs', version=None)
    
    trainer = pl.Trainer(
        max_epochs=MAX_EPOCHS,
        devices=1,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        precision="bf16-mixed",
        log_every_n_steps=10,
        default_root_dir='./outputs/dales',
        logger=[csv_logger],
        callbacks=callbacks,
        accumulate_grad_batches=ACCUMULATE_GRAD_BATCHES,  # æ¢¯åº¦ç´¯ç§¯
        gradient_clip_val=1.0,
        gradient_clip_algorithm="norm",
        enable_progress_bar=True,
        enable_model_summary=True,
        num_sanity_val_steps=2,
    )
    
    print(f"\nè®¾å¤‡: {trainer.accelerator} | ç²¾åº¦: {trainer.precision} | Epochs: {MAX_EPOCHS}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {ACCUMULATE_GRAD_BATCHES} batches | ç­‰æ•ˆbatch: ~{MAX_POINTS * ACCUMULATE_GRAD_BATCHES / 1000:.0f}K points/update")
    print(f"æ¨ç†ä¼˜åŒ–: max_points={MAX_POINTS/1000:.0f}K (è®­ç»ƒ) â†’ {MAX_POINTS_INFERENCE/1000:.0f}K (æ¨ç†) | workers={NUM_WORKERS}")
    
    # ========================================================================
    # è®­ç»ƒæµç¨‹
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("å¼€å§‹è®­ç»ƒ")
    print("=" * 80)
    trainer.fit(task, datamodule)
    
    print("\n" + "=" * 80)
    print("å¼€å§‹æµ‹è¯•")
    print("=" * 80)
    trainer.test(task, datamodule)
    
    print("\n" + "=" * 80)
    print("å¼€å§‹é¢„æµ‹")
    print("=" * 80)
    trainer.predict(task, datamodule)
    
    print("\n" + "=" * 80)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"æ£€æŸ¥ç‚¹: {trainer.default_root_dir}")
    print(f"é¢„æµ‹ç»“æœ: {OUTPUT_DIR}")
    print(f"æœ€ä½³æ¨¡å‹: {trainer.checkpoint_callback.best_model_path}")
    print(f"æœ€ä½³ MeanIoU: {trainer.checkpoint_callback.best_model_score:.4f}")


if __name__ == '__main__':
    main()
