"""
DALES æ•°æ®é›†è®­ç»ƒè„šæœ¬ï¼ˆçº¯ Python é…ç½®ï¼‰

æ–°åŠŸèƒ½æ¼”ç¤ºï¼š
- âœ… è‡ªåŠ¨ç±»åˆ«æƒé‡è®¡ç®—å’ŒåŠ æƒé‡‡æ ·
- âœ… ä¸­æ–‡ç±»åˆ«åç§°æ”¯æŒï¼ˆéªŒè¯æ—¥å¿— + hparams.yamlï¼‰
- âœ… CSV æ—¥å¿—è®°å½•ï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
- âœ… åŠ¨æ€æ‰¹æ¬¡é‡‡æ ·
- âœ… å¤šæ–‡ä»¶ LAS é¢„æµ‹æ”¯æŒ
- âœ… æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batchè®­ç»ƒï¼‰

æ¢¯åº¦ç´¯ç§¯è¯´æ˜ï¼š
- åŸç†ï¼šæ¯Nä¸ªbatchè®¡ç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œç´¯ç§¯Næ¬¡åæ‰æ›´æ–°å‚æ•°
- ä¼˜åŠ¿ï¼šåœ¨æ˜¾å­˜å—é™æ—¶æ¨¡æ‹Ÿæ›´å¤§çš„batch size
- ä¸åŠ¨æ€batchå®Œå…¨å…¼å®¹ï¼š
  * åŠ¨æ€batchæ§åˆ¶æ¯ä¸ªbatchçš„ç‚¹æ•°ï¼ˆmax_pointsï¼‰
  * æ¢¯åº¦ç´¯ç§¯æ§åˆ¶æ›´æ–°é¢‘ç‡ï¼ˆaccumulate_grad_batchesï¼‰
  * ç­‰æ•ˆbatch = max_points Ã— accumulate_grad_batches

é…ç½®å»ºè®®ï¼š
- å°æ˜¾å­˜(8GB):  max_points=100K, accumulate=4  â†’ 400Kç‚¹/æ›´æ–°
- ä¸­æ˜¾å­˜(16GB): max_points=150K, accumulate=2  â†’ 300Kç‚¹/æ›´æ–°
- å¤§æ˜¾å­˜(24GB): max_points=200K, accumulate=1  â†’ 200Kç‚¹/æ›´æ–°

æ¨ç†åŠ é€Ÿä¼˜åŒ–ï¼š
- å¤šè¿›ç¨‹åŠ è½½: NUM_WORKERS=4 (é¿å…æ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆ)
- å¤§batchæ¨ç†: max_points_inference=600K (æ— æ¢¯åº¦ï¼Œå¯ç”¨3-4å€è®­ç»ƒbatch)
- è‡ªåŠ¨ä¼˜åŒ–: Lightning 2.5+ é»˜è®¤ä½¿ç”¨ inference_mode (æ¯” no_grad æ›´å¿«)
- TF32åŠ é€Ÿ: å…¨å±€å¯ç”¨ï¼Œè®­ç»ƒå’Œæ¨ç†éƒ½ç”Ÿæ•ˆ
"""

import os
import sys
import warnings
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

# å¿½ç•¥ Windows ä¸‹ num_workers çš„è­¦å‘Š
warnings.filterwarnings("ignore", ".*does not have many workers.*")

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pointsuite.data import BinPklDataModule
from pointsuite.data.transforms import *
from pointsuite.tasks import SemanticSegmentationTask
from pointsuite.utils.callbacks import SemanticPredictLasWriter, AutoEmptyCacheCallback, TextLoggingCallback
from pointsuite.utils.logger import setup_logger
# from pointsuite.utils.progress_bar import CustomProgressBar


def main():
    # ========================================================================
    # é…ç½®
    # ========================================================================
    
    # æ•°æ®
    TRAIN_DATA = r"E:\data\DALES\dales_las\bin\train"
    TEST_DATA = r"E:\data\DALES\dales_las\bin\test"
    OUTPUT_DIR = r"E:\data\DALES\dales_las\bin\result"
    
    # è®¾ç½®æ—¥å¿— (æ•è·æ‰€æœ‰ç»ˆç«¯è¾“å‡º)
    log_file_path = setup_logger(OUTPUT_DIR)
    
    CLASS_MAPPING = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7}
    CLASS_NAMES = ['åœ°é¢', 'æ¤è¢«', 'è½¦è¾†', 'å¡è½¦', 'ç”µçº¿', 'ç¯±ç¬†', 'æ†çŠ¶ç‰©', 'å»ºç­‘']
    NUM_CLASSES = 8
    IGNORE_LABEL = -1
    
    # è®­ç»ƒ
    MAX_EPOCHS = 5
    BATCH_SIZE = 4 
    NUM_WORKERS = 0  # å¤šè¿›ç¨‹æ•°æ®åŠ è½½ï¼ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†
    LEARNING_RATE = 1e-3
    MAX_POINTS = 120000
    MAX_POINTS_INFERENCE = 120000  # æ¨ç†æ—¶ä½¿ç”¨æ›´å¤§batchï¼ˆæ— æ¢¯åº¦ï¼Œæ˜¾å­˜å ç”¨å°‘ï¼‰
    ACCUMULATE_GRAD_BATCHES = 2  # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯4ä¸ªbatchæ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œæ¨¡æ‹Ÿæ›´å¤§batch
    
    pl.seed_everything(42)
    
    # if torch.cuda.is_available():
    #     torch.set_float32_matmul_precision('high')
    #     torch.backends.cuda.matmul.allow_tf32 = True
    #     torch.backends.cudnn.allow_tf32 = True
    
    print("\n" + "=" * 80)
    print(f"DALES è¯­ä¹‰åˆ†å‰²è®­ç»ƒ - {NUM_CLASSES} ç±»")
    print("=" * 80)
    
    # ========================================================================
    # æ•°æ®å¢å¼º
    # ========================================================================
    
    train_transforms = [
        CenterShift(),  # ä¸­å¿ƒåŒ–åæ ‡
        RandomDropout(dropout_ratio=0.2, p=0.5),
        RandomRotate(angle=[-1, 1], axis='z', p=0.5),
        RandomScale(scale=[0.9, 1.1]),
        RandomFlip(p=0.5),
        RandomJitter(sigma=0.005, clip=0.02),
        # AddExtremeOutliers(
        #     ratio=0.001, height_range=(-10, 100), height_mode='bimodal',
        #     intensity_range=(0, 1), color_value=(128, 128, 128),
        #     class_label='ignore', p=0.5
        # ),
        Collect(keys=['coord', 'class'],
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    val_transforms = [
        CenterShift(),  # ä¸­å¿ƒåŒ–åæ ‡
        RandomDropout(dropout_ratio=0.2, p=0.5),
        RandomRotate(angle=[-1, 1], axis='z', p=0.5),
        RandomScale(scale=[0.9, 1.1]),
        RandomFlip(p=0.5),
        RandomJitter(sigma=0.005, clip=0.02),
        # AddExtremeOutliers(
        #     ratio=0.001, height_range=(-10, 100), height_mode='bimodal',
        #     intensity_range=(0, 1), color_value=(128, 128, 128),
        #     class_label='ignore', p=0.5
        # ),
        Collect(keys=['coord', 'class'],
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    predict_transforms = [
        CenterShift(),  # ä¸­å¿ƒåŒ–åæ ‡
        Collect(keys=['coord', 'indices', 'bin_file', 'bin_path', 'pkl_path'],
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    # ========================================================================
    # DataModule
    # ========================================================================
    
    datamodule = BinPklDataModule(
        train_data=TRAIN_DATA,
        val_data=TEST_DATA,
        test_data=None,
        predict_data=TEST_DATA,
        assets=['coord', 'echo', 'class'],
        class_mapping=CLASS_MAPPING,
        class_names=CLASS_NAMES,
        ignore_label=IGNORE_LABEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True,
        use_dynamic_batch=True,
        max_points=MAX_POINTS,
        use_dynamic_batch_inference=True,
        max_points_inference=MAX_POINTS_INFERENCE,
        use_weighted_sampler=True,  # å¯ç”¨åŠ æƒé‡‡æ ·
        class_weights=None,  # None = è‡ªåŠ¨ä»æ•°æ®é›†è®¡ç®—
        train_loop=1,
        val_loop=1,
        test_loop=1,
        predict_loop=1,
        train_transforms=train_transforms,
        val_transforms=val_transforms,
        test_transforms=val_transforms,
        predict_transforms=predict_transforms,
    )
    
    # ğŸ”¥ æ‰‹åŠ¨ setup ä»¥ä¾¿è®¿é—®æ•°æ®é›†å¹¶è®¡ç®—æƒé‡
    datamodule.setup(stage='fit')
    
    # ========================================================================
    # æ¨¡å‹
    # ========================================================================
    
    # ä½¿ç”¨é…ç½®å­—å…¸å®šä¹‰æ¨¡å‹ç»“æ„ï¼Œè€Œä¸æ˜¯ç›´æ¥å®ä¾‹åŒ–å¯¹è±¡
    # è¿™æ ·å¯ä»¥é¿å… PyTorch Lightning çš„ "attribute is already saved" è­¦å‘Š
    # å¹¶ä¸”è®© checkpoint æ›´è½»é‡ã€æ›´è§„èŒƒ
    model_config = {
        'backbone': {
            'class_path': 'pointsuite.models.PointTransformerV2',
            'init_args': {
                'in_channels': 5,
                'patch_embed_depth': 1,
                'patch_embed_channels': 24,
                'patch_embed_groups': 6,
                'patch_embed_neighbours': 24,
                'enc_depths': (2, 2, 2, 2),
                'enc_channels': (48, 96, 192, 256),
                'enc_groups': (6, 12, 24, 32),
                'enc_neighbours': (32, 32, 32, 32),
                'dec_depths': (1, 1, 1, 1),
                'dec_channels': (24, 48, 96, 192),
                'dec_groups': (4, 6, 12, 24),
                'dec_neighbours': (32, 32, 32, 32),
                'grid_sizes': (1.5, 3.75, 9.375, 23.4375),
                'attn_qkv_bias': True,
                'pe_multiplier': False,
                'pe_bias': True,
                'attn_drop_rate': 0.0,
                'drop_path_rate': 0.3,
                'unpool_backend': "interp",
            }
        },
        'head': {
            'class_path': 'pointsuite.models.SegHead',
            'init_args': {
                'in_channels': 24,
                'num_classes': NUM_CLASSES
            }
        }
    }

    
    loss_configs = [
        {
            "name": "ce_loss",
            "class_path": "pointsuite.models.losses.CrossEntropyLoss",
            "init_args": {
                "ignore_index": IGNORE_LABEL,
                "weight": datamodule.train_dataset.class_weights, # ç›´æ¥è°ƒç”¨å±æ€§
            },
            "weight": 1.0,
        },
        {
            "name": "lac_loss",
            "class_path": "pointsuite.models.losses.LACLoss",
            "init_args": {"k_neighbors":16, "ignore_index": IGNORE_LABEL},
            "weight": 1.0,
        },
    ]
    
    metric_configs = [
        {
            "name": "seg_metrics",
            "class_path": "pointsuite.utils.metrics.semantic_segmentation.SegmentationMetrics",
            "init_args": {
                "num_classes": NUM_CLASSES, 
                "ignore_index": IGNORE_LABEL,
                "class_names": CLASS_NAMES
            },
        },
    ]
    
    task = SemanticSegmentationTask(
        model_config=model_config,  # ä¼ å…¥é…ç½®å­—å…¸
        learning_rate=LEARNING_RATE,
        class_mapping=CLASS_MAPPING,
        class_names=CLASS_NAMES,
        ignore_label=IGNORE_LABEL,
        loss_configs=loss_configs,
        metric_configs=metric_configs,
    )
    
    # ä¼˜åŒ–å™¨
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay= 1e-4)
        
        # ä½¿ç”¨ Trainer çš„ estimated_stepping_batches è‡ªåŠ¨è·å–æ€»ä¼˜åŒ–æ­¥æ•°
        # è¿™ä¼šè‡ªåŠ¨è€ƒè™‘ max_epochs, dataloader é•¿åº¦ä»¥åŠ accumulate_grad_batches
        # é¿å…äº†æ‰‹åŠ¨ä¼°ç®— steps_per_epoch
        total_steps = self.trainer.estimated_stepping_batches
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=total_steps, 
            eta_min=1e-6
        )
        
        return {
            "optimizer": optimizer, 
            "lr_scheduler": {
                "scheduler": scheduler, 
                "interval": "step", 
                "frequency": 1
            }
        }
    
    import types
    task.configure_optimizers = types.MethodType(configure_optimizers, task)
    
    # ========================================================================
    # å›è°ƒå’Œ Trainer
    # ========================================================================
    
    callbacks = [
        # ä¿å­˜æœ€ä½³æ¨¡å‹ (Top 3) å’Œ æœ€åä¸€ä¸ªæ¨¡å‹ (last.ckpt)
        ModelCheckpoint(
            monitor='mean_iou', 
            mode='max', 
            save_top_k=1,
            save_last=True,  # ğŸ”¥ ä¿å­˜æœ€åä¸€ä¸ªæ¨¡å‹ä¸º last.ckpt
            filename='dales-{epoch:02d}-{mean_iou:.4f}', 
            verbose=True
        ),
        EarlyStopping(monitor='mean_iou', patience=20, mode='max', verbose=True, 
                     check_on_train_epoch_end=False),  # ğŸ”¥ ä¿®å¤ï¼šåœ¨éªŒè¯ç»“æŸæ—¶æ£€æŸ¥ï¼Œè€Œä¸æ˜¯è®­ç»ƒç»“æŸæ—¶
        # LearningRateMonitor(logging_interval='step'), # âŒ ç§»é™¤ï¼šå› ä¸ºç¦ç”¨äº† loggerï¼Œæ— æ³•ä½¿ç”¨æ­¤å›è°ƒ
        SemanticPredictLasWriter(output_dir=OUTPUT_DIR, save_logits=False, auto_infer_reverse_mapping=True),
        # CustomProgressBar(refresh_rate=1),  # è‡ªå®šä¹‰è¿›åº¦æ¡
        TextLoggingCallback(log_interval=10), # é™æ€æ–‡æœ¬æ—¥å¿— (ä¸å†éœ€è¦ log_file å‚æ•°ï¼Œå› ä¸ºå…¨å±€æ•è·äº†)
        AutoEmptyCacheCallback(slowdown_threshold=3.0, absolute_threshold=1.5, clear_interval=0, warmup_steps=10, verbose=True),  # è‡ªåŠ¨æ¸…ç†æ˜¾å­˜
    ]
    
    # ç§»é™¤ CSVLogger å’Œ TensorBoardLoggerï¼Œæ”¹ç”¨ TextLoggingCallback è®°å½•åˆ°æ–‡ä»¶
    # csv_logger = CSVLogger(save_dir='./outputs/dales', name='csv_logs', version=None)
    # tb_logger = TensorBoardLogger(save_dir='./outputs/dales', name='tb_logs', version=None)
    
    trainer = pl.Trainer(
        max_epochs=MAX_EPOCHS,
        devices=1,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        precision="16-mixed",
        log_every_n_steps=10,
        default_root_dir='./outputs/dales',
        logger=False, # ğŸ”¥ ç¦ç”¨é»˜è®¤ Logger
        callbacks=callbacks,
        accumulate_grad_batches=ACCUMULATE_GRAD_BATCHES,  # æ¢¯åº¦ç´¯ç§¯
        gradient_clip_val=1.0,
        gradient_clip_algorithm="norm",
        enable_progress_bar=False, # ç¦ç”¨é»˜è®¤è¿›åº¦æ¡
        enable_model_summary=True,
        num_sanity_val_steps=2,
    )
    
    print(f"\nè®¾å¤‡: {trainer.accelerator} | ç²¾åº¦: {trainer.precision} | Epochs: {MAX_EPOCHS}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {ACCUMULATE_GRAD_BATCHES} batches | ç­‰æ•ˆbatch: ~{MAX_POINTS * ACCUMULATE_GRAD_BATCHES / 1000:.0f}K points/update")
    print(f"æ¨ç†ä¼˜åŒ–: max_points={MAX_POINTS/1000:.0f}K (è®­ç»ƒ) â†’ {MAX_POINTS_INFERENCE/1000:.0f}K (æ¨ç†) | workers={NUM_WORKERS}")
    # ========================================================================
    # è®­ç»ƒæµç¨‹
    # ========================================================================
    
    # 1. æ–­ç‚¹æ¢å¤ (Resume): æ¢å¤å®Œæ•´çš„è®­ç»ƒçŠ¶æ€ (æ¨¡å‹æƒé‡ + ä¼˜åŒ–å™¨ + Epoch)
    #    ç”¨äºè®­ç»ƒä¸­æ–­åç»§ç»­è®­ç»ƒ
    #    ä¾‹å¦‚: ckpt_path = "outputs/dales/csv_logs/version_0/checkpoints/last.ckpt"
    ckpt_path = None 
    
    # 2. é¢„è®­ç»ƒæƒé‡ (Pretrained): ä»…åŠ è½½æ¨¡å‹æƒé‡ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ (é‡ç½® Epoch å’Œ ä¼˜åŒ–å™¨)
    #    ç”¨äºå¾®è°ƒ (Fine-tuning) æˆ–è¿ç§»å­¦ä¹ 
    #    ä¾‹å¦‚: pretrained_path = "outputs/dales/csv_logs/version_0/checkpoints/best.ckpt"
    pretrained_path = None

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ (å¦‚æœæŒ‡å®š)
    if pretrained_path is not None and ckpt_path is None:
        print(f"\n[Info] åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
        # strict=False å…è®¸æƒé‡ä¸å®Œå…¨åŒ¹é… (ä¾‹å¦‚å¾®è°ƒæ—¶ä¿®æ”¹äº† head)
        # æ³¨æ„: è¿™é‡Œæˆ‘ä»¬åŠ è½½æƒé‡åˆ°å½“å‰çš„ task å®ä¾‹ä¸­
        checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=False)
        state_dict = checkpoint['state_dict']
        
        # å¤„ç†å¯èƒ½çš„ key ä¸åŒ¹é… (ä¾‹å¦‚æœ‰äº› checkpoint æœ‰ 'model.' å‰ç¼€)
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('model.'):
                new_state_dict[k[6:]] = v # å»æ‰ 'model.' å‰ç¼€
            else:
                new_state_dict[k] = v
                
        missing_keys, unexpected_keys = task.load_state_dict(new_state_dict, strict=False)
        if missing_keys:
            print(f"  - ç¼ºå¤±çš„é”® (å°†éšæœºåˆå§‹åŒ–): {missing_keys[:5]} ...")
        if unexpected_keys:
            print(f"  - æœªé¢„æœŸçš„é”® (å°†è¢«å¿½ç•¥): {unexpected_keys[:5]} ...")
        print(f"  - æƒé‡åŠ è½½å®Œæˆ (Epoch å°†ä» 0 å¼€å§‹)")

    print("\n" + "=" * 80)
    print("å¼€å§‹è®­ç»ƒ")
    print("=" * 80)
    trainer.fit(task, datamodule, ckpt_path=ckpt_path)
    
    if datamodule.test_data is not None:
        print("\n" + "=" * 80)
        print("å¼€å§‹æµ‹è¯•")
        print("=" * 80)
        trainer.test(task, datamodule)
    else:
        print("\n" + "=" * 80)
        print("è·³è¿‡æµ‹è¯• (æœªæä¾›æµ‹è¯•æ•°æ®)")
        print("=" * 80)
    
    if datamodule.predict_data is not None:
        print("\n" + "=" * 80)
        print("å¼€å§‹é¢„æµ‹")
        print("=" * 80)
        # ğŸ”¥ æ˜¾å¼è°ƒç”¨ predict
        # ä½¿ç”¨ "best" è‡ªåŠ¨åŠ è½½æœ€ä½³ checkpoint
        trainer.predict(task, datamodule=datamodule, ckpt_path="best")
        
    print("\n" + "=" * 80)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"æ£€æŸ¥ç‚¹: {trainer.default_root_dir}")
    print(f"é¢„æµ‹ç»“æœ: {OUTPUT_DIR}")
    
    if trainer.checkpoint_callback.best_model_path:
        print(f"æœ€ä½³æ¨¡å‹: {trainer.checkpoint_callback.best_model_path}")
    
    if trainer.checkpoint_callback.best_model_score is not None:
        print(f"æœ€ä½³ MeanIoU: {trainer.checkpoint_callback.best_model_score:.4f}")
    else:
        print("æœ€ä½³ MeanIoU: N/A (æœªç”Ÿæˆæˆ–æœªè®°å½•)")

if __name__ == '__main__':
    main()
