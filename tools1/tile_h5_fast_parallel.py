"""
è¶…å¿«H5ç”Ÿæˆ - è¿ç»­å­˜å‚¨ + å¹¶è¡Œå¤„ç†

æ ¸å¿ƒä¼˜åŒ–ï¼š
1. è¿ç»­å­˜å‚¨ï¼ˆæ— indicesï¼‰â†’ éšæœºè¯»å–å¿«1000å€
2. å¹¶è¡Œå¤„ç†å¤šä¸ªLASæ–‡ä»¶
3. æ— å‹ç¼© + contiguous layout
"""

import laspy
import numpy as np
import h5py
from pathlib import Path
from typing import Optional, Tuple, List, Union
from tqdm import tqdm
from sklearn.neighbors import KDTree
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import time


class FastH5Processor:
    """è¶…å¿«H5å¤„ç†å™¨"""
    
    def __init__(
        self,
        input_path: Union[str, Path],
        output_dir: Union[str, Path],
        window_size: Tuple[float, float] = (150., 150.),
        min_points: Optional[int] = 4096 * 2,
        max_points: Optional[int] = 4096 * 16 * 2,
        overlap: bool = False,
        n_workers: Optional[int] = None
    ):
        self.input_path = Path(input_path)
        self.output_dir = Path(output_dir)
        self.window_size = window_size
        self.min_points = min_points
        self.max_points = max_points
        self.overlap = overlap
        self.n_workers = n_workers or multiprocessing.cpu_count()
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.las_files = self._find_las_files()
    
    def _find_las_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰LASæ–‡ä»¶"""
        if self.input_path.is_file() and self.input_path.suffix.lower() in ['.las', '.laz']:
            return [self.input_path]
        elif self.input_path.is_dir():
            return list(self.input_path.glob('*.las')) + list(self.input_path.glob('*.laz'))
        else:
            raise ValueError(f"Invalid input path: {self.input_path}")
    
    def run(self):
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        print(f"\næ‰¾åˆ°{len(self.las_files)}ä¸ªLAS/LAZæ–‡ä»¶")
        
        if len(self.las_files) == 1:
            # å•æ–‡ä»¶ç›´æ¥å¤„ç†
            print(f"\nå¤„ç†: {self.las_files[0].name}")
            self.process_file(self.las_files[0])
        else:
            # å¤šæ–‡ä»¶å¹¶è¡Œå¤„ç†
            print(f"\nä½¿ç”¨{self.n_workers}ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†...")
            with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
                future_to_file = {
                    executor.submit(
                        self._process_file_worker,
                        las_file,
                        self.output_dir,
                        self.window_size,
                        self.min_points,
                        self.max_points,
                        self.overlap
                    ): las_file
                    for las_file in self.las_files
                }
                
                for future in tqdm(as_completed(future_to_file), 
                                 total=len(self.las_files),
                                 desc="å¤„ç†è¿›åº¦"):
                    las_file = future_to_file[future]
                    try:
                        num_segs, file_size, elapsed = future.result()
                        print(f"\n  âœ… {las_file.name}: {num_segs} segments, "
                              f"{file_size:.1f}MB, {elapsed:.2f}ç§’")
                    except Exception as e:
                        print(f"\n  âŒ {las_file.name}: é”™è¯¯ - {e}")
    
    @staticmethod
    def _process_file_worker(
        las_file: Path,
        output_dir: Path,
        window_size: Tuple[float, float],
        min_points: Optional[int],
        max_points: Optional[int],
        overlap: bool
    ) -> Tuple[int, float, float]:
        """
        Workerå‡½æ•° - ç”±å¤šè¿›ç¨‹è°ƒç”¨
        
        Returns:
            (num_segments, file_size_mb, elapsed_seconds)
        """
        processor = FastH5Processor.__new__(FastH5Processor)
        processor.output_dir = output_dir
        processor.window_size = window_size
        processor.min_points = min_points
        processor.max_points = max_points
        processor.overlap = overlap
        
        return processor.process_file(las_file)
    
    def process_file(self, las_file: Path) -> Tuple[int, float, float]:
        """
        å¤„ç†å•ä¸ªLASæ–‡ä»¶
        
        Returns:
            (num_segments, file_size_mb, elapsed_seconds)
        """
        start_time = time.time()
        
        # è¯»å–LAS
        with laspy.open(las_file) as fh:
            las_data = fh.read()
        
        # æå–ç‚¹äº‘æ•°æ®
        points = np.vstack([las_data.x, las_data.y, las_data.z]).T
        
        # åˆ†å‰²
        segments = self._segment_point_cloud(las_data, points)
        
        # å†™å…¥H5
        output_path = self.output_dir / f"{las_file.stem}.h5"
        self._save_to_h5(output_path, las_data, segments)
        
        # ç»Ÿè®¡
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        elapsed = time.time() - start_time
        
        return len(segments), file_size_mb, elapsed
    
    def _segment_point_cloud(
        self,
        las_data: laspy.LasData,
        points: np.ndarray
    ) -> List[np.ndarray]:
        """
        ç‚¹äº‘åˆ†å‰²
        
        overlapæ¨¡å¼ï¼š
        - å…ˆæŒ‰æ­£å¸¸ç½‘æ ¼åˆ†å‰²ä¸€æ¬¡ï¼Œåº”ç”¨é˜ˆå€¼å¤„ç†
        - å†æŒ‰åç§»åŠä¸ªç½‘æ ¼çš„ä½ç½®åˆ†å‰²ä¸€æ¬¡ï¼Œç‹¬ç«‹åº”ç”¨é˜ˆå€¼å¤„ç†
        - æœ€ååˆå¹¶ä¸¤æ¬¡åˆ†å‰²çš„ç»“æœï¼Œå®ç°é‡å åˆ†å—
        
        å…³é”®ï¼šä¸¤æ¬¡åˆ†å‰²çš„é˜ˆå€¼å¤„ç†æ˜¯ç‹¬ç«‹çš„ï¼Œé¿å…åˆå¹¶æ—¶çš„æ··ä¹±
        """
        min_x, min_y = np.min(points[:, 0]), np.min(points[:, 1])
        max_x, max_y = np.max(points[:, 0]), np.max(points[:, 1])
        x_size, y_size = self.window_size
        
        # ç¬¬ä¸€æ¬¡åˆ†å‰²ï¼šæ­£å¸¸ç½‘æ ¼
        segments = self._grid_segmentation(points, min_x, min_y, max_x, max_y, 
                                          x_size, y_size, offset_x=0, offset_y=0)
        
        # å¯¹ç¬¬ä¸€æ¬¡åˆ†å‰²åº”ç”¨é˜ˆå€¼å¤„ç†
        if self.max_points is not None:
            segments = self._apply_max_threshold(points, segments)
        if self.min_points is not None:
            segments = self._apply_min_threshold(points, segments)
        
        # Overlapæ¨¡å¼ï¼šæ·»åŠ åç§»ç½‘æ ¼åˆ†å‰²ï¼ˆç‹¬ç«‹å¤„ç†ï¼‰
        if self.overlap:
            offset_segments = self._grid_segmentation(
                points, min_x, min_y, max_x, max_y,
                x_size, y_size, 
                offset_x=x_size / 2,  # åç§»åŠä¸ªç½‘æ ¼
                offset_y=y_size / 2
            )
            
            # å¯¹åç§»åˆ†å‰²ç‹¬ç«‹åº”ç”¨é˜ˆå€¼å¤„ç†
            if self.max_points is not None:
                offset_segments = self._apply_max_threshold(points, offset_segments)
            if self.min_points is not None:
                offset_segments = self._apply_min_threshold(points, offset_segments)
            
            # åˆå¹¶ä¸¤æ¬¡åˆ†å‰²çš„ç»“æœ
            segments.extend(offset_segments)
        
        return segments
    
    def _grid_segmentation(
        self,
        points: np.ndarray,
        min_x: float,
        min_y: float,
        max_x: float,
        max_y: float,
        x_size: float,
        y_size: float,
        offset_x: float = 0,
        offset_y: float = 0
    ) -> List[np.ndarray]:
        """
        ç½‘æ ¼åˆ†å‰²ï¼ˆæ”¯æŒåç§»ï¼‰
        
        Args:
            points: ç‚¹äº‘æ•°æ®
            min_x, min_y, max_x, max_y: ç‚¹äº‘èŒƒå›´
            x_size, y_size: ç½‘æ ¼å¤§å°
            offset_x, offset_y: ç½‘æ ¼åç§»é‡ï¼ˆç”¨äºoverlapæ¨¡å¼ï¼‰
        
        Returns:
            segmentsåˆ—è¡¨
        """
        # åº”ç”¨åç§»
        min_x_offset = min_x + offset_x
        min_y_offset = min_y + offset_y
        
        # è®¡ç®—ç½‘æ ¼æ•°é‡
        num_windows_x = max(1, int(np.ceil((max_x - min_x_offset) / x_size)))
        num_windows_y = max(1, int(np.ceil((max_y - min_y_offset) / y_size)))
        
        # è®¡ç®—æ¯ä¸ªç‚¹æ‰€å±çš„ç½‘æ ¼
        x_bins = ((points[:, 0] - min_x_offset) / x_size).astype(int)
        y_bins = ((points[:, 1] - min_y_offset) / y_size).astype(int)
        
        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        x_bins = np.clip(x_bins, 0, num_windows_x - 1)
        y_bins = np.clip(y_bins, 0, num_windows_y - 1)
        
        # ç»„åˆçª—å£ID
        window_ids = x_bins * num_windows_y + y_bins
        
        # åˆ†ç»„
        unique_ids, indices = np.unique(window_ids, return_inverse=True)
        segments = [np.where(indices == i)[0] for i in range(len(unique_ids))]
        
        # è¿‡æ»¤ç©ºsegment
        segments = [seg for seg in segments if len(seg) > 0]
        
        return segments
    
    def _apply_max_threshold(
        self,
        points: np.ndarray,
        segments: List[np.ndarray]
    ) -> List[np.ndarray]:
        """é€’å½’äºŒåˆ†æ³•å¤„ç†å¤§segment"""
        large_indices = [i for i, seg in enumerate(segments) if len(seg) > self.max_points]
        
        if not large_indices:
            return segments
        
        result = [seg for i, seg in enumerate(segments) if i not in large_indices]
        large_segs = [segments[i] for i in large_indices]
        
        def subdivide(segment):
            """é€’å½’äºŒåˆ†"""
            if len(segment) <= self.max_points:
                return [segment]
            
            seg_points = points[segment]
            ranges = np.ptp(seg_points[:, :2], axis=0)
            split_dim = np.argmax(ranges)
            sorted_indices = np.argsort(seg_points[:, split_dim])
            
            mid = len(sorted_indices) // 2
            left = segment[sorted_indices[:mid]]
            right = segment[sorted_indices[mid:]]
            
            return subdivide(left) + subdivide(right)
        
        # çº¿ç¨‹å¹¶è¡Œï¼ˆCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
        from concurrent.futures import ThreadPoolExecutor
        max_workers = min(8, len(large_segs))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(subdivide, seg) for seg in large_segs]
            for future in futures:
                result.extend(future.result())
        
        return result
    
    def _apply_min_threshold(
        self,
        points: np.ndarray,
        segments: List[np.ndarray]
    ) -> List[np.ndarray]:
        """KD-Treeåˆå¹¶å°segment"""
        if len(segments) <= 1:
            return segments
        
        centroids = np.array([np.mean(points[seg][:, :2], axis=0) for seg in segments])
        small_indices = [i for i, seg in enumerate(segments) if len(seg) < self.min_points]
        
        if not small_indices:
            return segments
        
        valid_indices = [i for i in range(len(segments)) if i not in small_indices]
        if not valid_indices:
            return segments
        
        kdtree = KDTree(centroids[valid_indices])
        small_indices.sort(key=lambda i: len(segments[i]))
        
        for small_idx in small_indices:
            if small_idx >= len(segments):
                continue
            
            _, nearest = kdtree.query([centroids[small_idx]], k=1)
            nearest_idx = valid_indices[nearest[0][0]]
            
            if nearest_idx != small_idx and nearest_idx < len(segments):
                segments[nearest_idx] = np.concatenate([
                    segments[nearest_idx],
                    segments[small_idx]
                ])
                segments[small_idx] = np.array([], dtype=int)
        
        return [seg for seg in segments if len(seg) > 0]
    
    def _save_to_h5(
        self,
        output_path: Path,
        las_data: laspy.LasData,
        segments: List[np.ndarray]
    ):
        """
        ä¿å­˜ä¸ºå¿«é€ŸH5æ ¼å¼
        
        å…³é”®ï¼š
        - æ¯ä¸ªsegmentå•ç‹¬å­˜å‚¨æ‰€æœ‰LASå­—æ®µ
        - chunks=None â†’ contiguous layout
        - æ— å‹ç¼© â†’ æœ€å¿«è¯»å–
        """
        with h5py.File(output_path, 'w') as f:
            # Header
            header = f.create_group('header')
            header.attrs['num_points'] = len(las_data.points)
            header.attrs['point_format'] = las_data.header.point_format.id
            header.attrs['version_major'] = las_data.header.version.major
            header.attrs['version_minor'] = las_data.header.version.minor
            header.attrs['x_scale'] = las_data.header.x_scale
            header.attrs['y_scale'] = las_data.header.y_scale
            header.attrs['z_scale'] = las_data.header.z_scale
            header.attrs['x_offset'] = las_data.header.x_offset
            header.attrs['y_offset'] = las_data.header.y_offset
            header.attrs['z_offset'] = las_data.header.z_offset
            
            # CRSä¿¡æ¯
            if hasattr(las_data, 'crs') and las_data.crs is not None:
                header.attrs['crs'] = str(las_data.crs)
            
            # ç¡®å®šå¯ç”¨å­—æ®µ
            available_fields = []
            for field in las_data.point_format.dimension_names:
                # è·³è¿‡åŸå§‹æ•´æ•°åæ ‡(X,Y,Z)ï¼Œæˆ‘ä»¬ä¿å­˜æµ®ç‚¹åæ ‡(x,y,z)
                if field in ['X', 'Y', 'Z']:
                    continue
                if hasattr(las_data, field):
                    available_fields.append(field)
            
            # ç¡®ä¿åŒ…å«xyzåæ ‡
            for coord in ['x', 'y', 'z']:
                if coord not in available_fields:
                    available_fields.insert(0, coord)
            
            header.attrs['available_fields'] = ','.join(available_fields)
            
            # Segments
            segs_group = f.create_group('segments')
            segs_group.attrs['num_segments'] = len(segments)
            
            # å†™å…¥æ¯ä¸ªsegment
            for i, indices in enumerate(segments):
                seg_group = segs_group.create_group(f'segment_{i:04d}')
                
                # ä¿å­˜æ‰€æœ‰å¯ç”¨å­—æ®µï¼ˆcontiguous, no compressionï¼‰
                for field in available_fields:
                    field_data = getattr(las_data, field)[indices]
                    
                    # ç¡®å®šæ•°æ®ç±»å‹
                    if field in ['x', 'y', 'z']:
                        dtype = np.float64
                    elif field == 'classification':
                        dtype = np.int32
                    elif field in ['intensity', 'red', 'green', 'blue', 'point_source_id']:
                        dtype = np.uint16
                    elif field in ['return_number', 'number_of_returns', 'user_data']:
                        dtype = np.uint8
                    elif field == 'scan_angle_rank':
                        dtype = np.int8
                    elif field == 'gps_time':
                        dtype = np.float64
                    else:
                        dtype = field_data.dtype
                    
                    seg_group.create_dataset(
                        field,
                        data=field_data,
                        dtype=dtype,
                        chunks=None  # Contiguous!
                    )
                
                # Metadata
                seg_group.attrs['num_points'] = len(indices)


def process_las_to_fast_h5(
    input_path: str,
    output_dir: str,
    window_size: Tuple[float, float] = (150., 150.),
    min_points: Optional[int] = 4096 * 2,
    max_points: Optional[int] = 4096 * 16 * 2,
    overlap: bool = False,
    n_workers: Optional[int] = None
):
    """
    å¿«é€Ÿå¤„ç†LASåˆ°H5
    
    Args:
        input_path: LASæ–‡ä»¶æˆ–ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        window_size: çª—å£å¤§å°
        min_points: æœ€å°ç‚¹æ•°
        max_points: æœ€å¤§ç‚¹æ•°
        overlap: æ˜¯å¦å¯ç”¨é‡å æ¨¡å¼ï¼ˆä¼šç”Ÿæˆ2å€çš„segmentsï¼‰
        n_workers: å¹¶è¡Œè¿›ç¨‹æ•°
    """
    processor = FastH5Processor(
        input_path=input_path,
        output_dir=output_dir,
        window_size=window_size,
        min_points=min_points,
        max_points=max_points,
        overlap=overlap,
        n_workers=n_workers
    )
    processor.run()


if __name__ == "__main__":
    # é…ç½®
    input_path = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\train"
    output_dir = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\h5_fast\train"
    window_size = (150., 150.)
    min_points = 4096 * 2
    max_points = 4096 * 16 * 2
    overlap = True  # ğŸ”¥ æ˜¯å¦å¯ç”¨é‡å åˆ†å—ï¼ˆTrueä¼šç”Ÿæˆçº¦2å€segmentsï¼‰
    n_workers = 8  # å¹¶è¡Œè¿›ç¨‹æ•°
    
    print("="*70)
    print("è¶…å¿«H5ç”Ÿæˆ - è¿ç»­å­˜å‚¨ + å¹¶è¡Œå¤„ç†")
    print("="*70)
    print(f"è¾“å…¥: {input_path}")
    print(f"è¾“å‡º: {output_dir}")
    print(f"çª—å£å¤§å°: {window_size}")
    print(f"ç‚¹æ•°èŒƒå›´: {min_points} - {max_points}")
    print(f"é‡å æ¨¡å¼: {'âœ… å¼€å¯' if overlap else 'âŒ å…³é—­'}")
    print(f"å¹¶è¡Œè¿›ç¨‹: {n_workers}")
    print("="*70)
    
    start = time.time()
    process_las_to_fast_h5(
        input_path=input_path,
        output_dir=output_dir,
        window_size=window_size,
        min_points=min_points,
        max_points=max_points,
        overlap=overlap,
        n_workers=n_workers
    )
    elapsed = time.time() - start
    
    print("="*70)
    print(f"âœ… å®Œæˆï¼æ€»è€—æ—¶: {elapsed:.2f}ç§’")
    print("="*70)
