"""
å®Œæ•´åŸºå‡†æµ‹è¯• - è¯»å–æ‰€æœ‰H5æ–‡ä»¶çš„å…¨éƒ¨segments

æµ‹è¯•åœºæ™¯ï¼š
1. æŒ‰éœ€åŠ è½½ - éå†æ‰€æœ‰segments
2. é¢„åŠ è½½ - éå†æ‰€æœ‰segments
3. DataLoader with different num_workers
"""

import torch
from torch.utils.data import DataLoader
import time
from pathlib import Path
import numpy as np
from tqdm import tqdm

# å¯¼å…¥å¿«é€Ÿæ•°æ®é›†ç±»
import sys
sys.path.append(str(Path(__file__).parent))
from h5_dataset_fast import FastH5Dataset, FastMultiH5Dataset, collate_fn


def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds:.2f}ç§’"
    else:
        mins = int(seconds // 60)
        secs = seconds % 60
        return f"{mins}åˆ†{secs:.2f}ç§’"


def benchmark_full_dataset():
    """å®Œæ•´æ•°æ®é›†åŸºå‡†æµ‹è¯•"""
    
    print("="*80)
    print("å®Œæ•´æ•°æ®é›†åŸºå‡†æµ‹è¯• - 19ä¸ªH5æ–‡ä»¶")
    print("="*80)
    
    # æŸ¥æ‰¾æ‰€æœ‰H5æ–‡ä»¶
    h5_dir = Path(r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\h5_fast\train")
    h5_files = sorted(h5_dir.glob("*.h5"))
    
    if not h5_files:
        print(f"âŒ æœªæ‰¾åˆ°H5æ–‡ä»¶: {h5_dir}")
        return
    
    print(f"\næ‰¾åˆ° {len(h5_files)} ä¸ªH5æ–‡ä»¶")
    
    # åˆ›å»ºæ•°æ®é›†æŸ¥çœ‹æ€»segmentæ•°
    dataset_temp = FastMultiH5Dataset(
        [str(f) for f in h5_files],
        preload_strategy="none"
    )
    total_segments = len(dataset_temp)
    print(f"æ€»segmentsæ•°: {total_segments}")
    print("="*80)
    
    # ==================== æµ‹è¯•1: æŒ‰éœ€åŠ è½½ï¼Œå•è¿›ç¨‹ ====================
    print("\nã€æµ‹è¯•1ã€‘æŒ‰éœ€åŠ è½½ + num_workers=0")
    print("-"*80)
    
    dataset = FastMultiH5Dataset(
        [str(f) for f in h5_files],
        preload_strategy="none"
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )
    
    start = time.time()
    total_points = 0
    
    for batch_xyz, batch_labels in tqdm(dataloader, desc="è¯»å–è¿›åº¦", unit="batch"):
        for xyz in batch_xyz:
            total_points += len(xyz)
    
    elapsed_1 = time.time() - start
    speed_1 = total_segments / elapsed_1
    
    print(f"\nç»“æœ:")
    print(f"  æ€»segments: {total_segments}")
    print(f"  æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  è€—æ—¶: {format_time(elapsed_1)}")
    print(f"  é€Ÿåº¦: {speed_1:.2f} segments/ç§’")
    print(f"  å¹³å‡æ¯segment: {elapsed_1*1000/total_segments:.2f} ms")
    
    # ==================== æµ‹è¯•2: æŒ‰éœ€åŠ è½½ï¼Œå¤šè¿›ç¨‹ ====================
    print("\n" + "="*80)
    print("ã€æµ‹è¯•2ã€‘æŒ‰éœ€åŠ è½½ + num_workers=4")
    print("-"*80)
    
    dataset = FastMultiH5Dataset(
        [str(f) for f in h5_files],
        preload_strategy="none"
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=False,
        num_workers=4,
        collate_fn=collate_fn,
        persistent_workers=True
    )
    
    start = time.time()
    total_points = 0
    
    for batch_xyz, batch_labels in tqdm(dataloader, desc="è¯»å–è¿›åº¦", unit="batch"):
        for xyz in batch_xyz:
            total_points += len(xyz)
    
    elapsed_2 = time.time() - start
    speed_2 = total_segments / elapsed_2
    
    print(f"\nç»“æœ:")
    print(f"  æ€»segments: {total_segments}")
    print(f"  æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  è€—æ—¶: {format_time(elapsed_2)}")
    print(f"  é€Ÿåº¦: {speed_2:.2f} segments/ç§’")
    print(f"  å¹³å‡æ¯segment: {elapsed_2*1000/total_segments:.2f} ms")
    print(f"  å¯¹æ¯”æµ‹è¯•1: {elapsed_1/elapsed_2:.2f}x")
    
    # ==================== æµ‹è¯•3: å…¨é¢„åŠ è½½ï¼Œå•è¿›ç¨‹ ====================
    print("\n" + "="*80)
    print("ã€æµ‹è¯•3ã€‘å…¨é¢„åŠ è½½ + num_workers=0")
    print("-"*80)
    
    dataset = FastMultiH5Dataset(
        [str(f) for f in h5_files],
        preload_strategy="all"
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )
    
    start = time.time()
    total_points = 0
    
    for batch_xyz, batch_labels in tqdm(dataloader, desc="è¯»å–è¿›åº¦", unit="batch"):
        for xyz in batch_xyz:
            total_points += len(xyz)
    
    elapsed_3 = time.time() - start
    speed_3 = total_segments / elapsed_3
    
    print(f"\nç»“æœ:")
    print(f"  æ€»segments: {total_segments}")
    print(f"  æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  è€—æ—¶: {format_time(elapsed_3)}")
    print(f"  é€Ÿåº¦: {speed_3:.2f} segments/ç§’")
    print(f"  å¹³å‡æ¯segment: {elapsed_3*1000/total_segments:.2f} ms")
    print(f"  å¯¹æ¯”æµ‹è¯•1: {elapsed_1/elapsed_3:.2f}x")
    
    # ==================== æµ‹è¯•4: éšæœºè¯»å–æ¨¡æ‹ŸçœŸå®è®­ç»ƒ ====================
    print("\n" + "="*80)
    print("ã€æµ‹è¯•4ã€‘æŒ‰éœ€åŠ è½½ + shuffle=Trueï¼ˆæ¨¡æ‹ŸçœŸå®è®­ç»ƒï¼‰")
    print("-"*80)
    
    dataset = FastMultiH5Dataset(
        [str(f) for f in h5_files],
        preload_strategy="none"
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=True,  # éšæœºæ‰“ä¹±
        num_workers=0,
        collate_fn=collate_fn
    )
    
    start = time.time()
    total_points = 0
    
    for batch_xyz, batch_labels in tqdm(dataloader, desc="è¯»å–è¿›åº¦", unit="batch"):
        for xyz in batch_xyz:
            total_points += len(xyz)
    
    elapsed_4 = time.time() - start
    speed_4 = total_segments / elapsed_4
    
    print(f"\nç»“æœ:")
    print(f"  æ€»segments: {total_segments}")
    print(f"  æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  è€—æ—¶: {format_time(elapsed_4)}")
    print(f"  é€Ÿåº¦: {speed_4:.2f} segments/ç§’")
    print(f"  å¹³å‡æ¯segment: {elapsed_4*1000/total_segments:.2f} ms")
    print(f"  å¯¹æ¯”æµ‹è¯•1ï¼ˆé¡ºåºï¼‰: {elapsed_1/elapsed_4:.2f}x")
    
    # ==================== æœ€ç»ˆæ€»ç»“ ====================
    print("\n" + "="*80)
    print("æœ€ç»ˆæ€»ç»“")
    print("="*80)
    
    results = [
        ("æŒ‰éœ€+å•è¿›ç¨‹", elapsed_1, speed_1),
        ("æŒ‰éœ€+4è¿›ç¨‹", elapsed_2, speed_2),
        ("é¢„åŠ è½½+å•è¿›ç¨‹", elapsed_3, speed_3),
        ("éšæœº+å•è¿›ç¨‹", elapsed_4, speed_4)
    ]
    
    print(f"\n{'æ¨¡å¼':<20} {'è€—æ—¶':<15} {'é€Ÿåº¦ (seg/s)':<20} {'ç›¸å¯¹æ€§èƒ½':<10}")
    print("-"*80)
    
    baseline = elapsed_1
    for name, elapsed, speed in results:
        speedup = baseline / elapsed
        print(f"{name:<20} {format_time(elapsed):<15} {speed:>10.2f}{'':<10} {speedup:>6.2f}x")
    
    # æ‰¾å‡ºæœ€å¿«çš„
    best_idx = min(range(len(results)), key=lambda i: results[i][1])
    print(f"\nğŸ† æœ€ä¼˜é…ç½®: {results[best_idx][0]}")
    print(f"   - é€Ÿåº¦: {results[best_idx][2]:.2f} segments/ç§’")
    print(f"   - è€—æ—¶: {format_time(results[best_idx][1])}")
    
    # è®¡ç®—ä¸æ—§ç‰ˆçš„å¯¹æ¯”ï¼ˆå‡è®¾æ—§ç‰ˆ1.5 seg/sï¼‰
    print(f"\nğŸ“Š ä¸æ—§ç‰ˆH5æ ¼å¼å¯¹æ¯”ï¼ˆæ—§ç‰ˆçº¦1.5 seg/sï¼‰:")
    old_speed = 1.5
    for name, elapsed, speed in results:
        improvement = speed / old_speed
        print(f"   {name}: {improvement:.0f}x æå‡")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    benchmark_full_dataset()
