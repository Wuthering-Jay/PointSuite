import numpy as np
import pickle
from pathlib import Path
from typing import Union, Dict, List
from collections import Counter


def verify_segment_coverage(pkl_path: Union[str, Path], bin_path: Union[str, Path] = None):
    """
    éªŒè¯åˆ†å—ç»“æœä¸­æ¯ä¸ªç‚¹çš„è¦†ç›–æƒ…å†µå’Œé‡å¤é‡‡æ ·ç‡
    
    Args:
        pkl_path: pklæ–‡ä»¶è·¯å¾„
        bin_path: binæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºè·å–æ€»ç‚¹æ•°ï¼‰
    """
    pkl_path = Path(pkl_path)
    
    print("="*70)
    print(f"éªŒè¯æ–‡ä»¶: {pkl_path.name}")
    print("="*70)
    
    # åŠ è½½å…ƒæ•°æ®
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    
    total_points = metadata['num_points']
    num_segments = metadata['num_segments']
    segments_info = metadata['segments']
    
    print(f"\nåŸºæœ¬ä¿¡æ¯:")
    print(f"  - æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  - åˆ†å—æ•°: {num_segments:,}")
    print(f"  - Grid Sample: {metadata.get('grid_sample', False)}")
    if metadata.get('grid_sample'):
        print(f"  - Grid Size: {metadata.get('grid_size')}")
        print(f"  - Max Sample Loops: {metadata.get('max_sample_loops')}")
    print(f"  - Window Size: {metadata['window_size']}")
    print(f"  - Min Points: {metadata['min_points']}")
    print(f"  - Max Points: {metadata['max_points']}")
    print(f"  - Overlap: {metadata['overlap']}")
    
    # ç»Ÿè®¡æ¯ä¸ªç‚¹å‡ºç°çš„æ¬¡æ•°
    print(f"\n{'='*70}")
    print(f"åˆ†æç‚¹è¦†ç›–æƒ…å†µ...")
    print(f"{'='*70}")
    
    # 1. æ”¶é›†æ‰€æœ‰åˆ†å—ä¸­çš„æ‰€æœ‰ç‚¹ç´¢å¼•
    all_indices_in_segments = []
    segment_sizes = []
    
    for seg_info in segments_info:
        indices = seg_info['indices']
        segment_sizes.append(len(indices))
        all_indices_in_segments.extend(indices)
    
    all_indices_in_segments = np.array(all_indices_in_segments)
    
    # 2. ç»Ÿè®¡æ‰€æœ‰åˆ†å—ä¸­ç‚¹çš„æ€»æ•°ï¼ˆåŒ…å«é‡å¤ï¼‰
    total_point_instances_in_segments = len(all_indices_in_segments)
    
    # 3. ç»Ÿè®¡uniqueçš„ç‚¹æ•°
    unique_points_in_segments = np.unique(all_indices_in_segments)
    num_unique_points = len(unique_points_in_segments)
    
    # 4. ç»Ÿè®¡æ¯ä¸ªç‚¹å‡ºç°çš„æ¬¡æ•°
    point_count = np.zeros(total_points, dtype=np.int32)
    for idx in all_indices_in_segments:
        point_count[idx] += 1
    
    # 5. éªŒè¯binæ–‡ä»¶ä¸­çš„ç‚¹æ•°
    bin_path = pkl_path.with_suffix('.bin')
    if bin_path.exists():
        dtype = np.dtype(metadata['dtype'])
        bin_point_count = bin_path.stat().st_size // dtype.itemsize
    else:
        bin_path = None
        bin_point_count = None
    
    print(f"\nğŸ“Š ç‚¹æ•°ç»Ÿè®¡:")
    print(f"  - Metadataä¸­çš„æ€»ç‚¹æ•°: {total_points:,}")
    if bin_path and bin_point_count:
        print(f"  - Binæ–‡ä»¶ä¸­çš„ç‚¹æ•°: {bin_point_count:,}")
        if bin_point_count == total_points:
            print(f"    âœ… Binæ–‡ä»¶ä¸metadataåŒ¹é…")
        else:
            print(f"    âš ï¸  Binæ–‡ä»¶ä¸metadataä¸åŒ¹é… (å·®å¼‚: {abs(bin_point_count - total_points):,})")
    
    print(f"\n  - æ‰€æœ‰åˆ†å—ä¸­çš„ç‚¹å®ä¾‹æ€»æ•°: {total_point_instances_in_segments:,}")
    print(f"  - æ‰€æœ‰åˆ†å—ä¸­çš„uniqueç‚¹æ•°: {num_unique_points:,}")
    
    if num_unique_points == total_points:
        print(f"    âœ… Uniqueç‚¹æ•°ä¸æ€»ç‚¹æ•°åŒ¹é…")
    else:
        print(f"    âš ï¸  Uniqueç‚¹æ•°ä¸æ€»ç‚¹æ•°ä¸åŒ¹é… (å·®å¼‚: {abs(num_unique_points - total_points):,})")
    
    # 6. è®¡ç®—é‡å¤ç‡
    repetition_count = total_point_instances_in_segments - num_unique_points
    if num_unique_points > 0:
        repetition_rate = (repetition_count / num_unique_points) * 100
    else:
        repetition_rate = 0
    
    print(f"\nğŸ“ˆ é‡å¤é‡‡æ ·ç»Ÿè®¡:")
    print(f"  - é‡å¤çš„ç‚¹å®ä¾‹æ•°: {repetition_count:,}")
    print(f"  - é‡å¤ç‡: {repetition_rate:.2f}%")
    print(f"  - å¹³å‡æ¯ä¸ªç‚¹è¢«é‡‡æ ·: {total_point_instances_in_segments / num_unique_points:.2f} æ¬¡")
    
    # 7. è¦†ç›–ç‡ç»Ÿè®¡
    uncovered_points = np.sum(point_count == 0)
    covered_once = np.sum(point_count == 1)
    covered_multiple = np.sum(point_count > 1)
    
    coverage_rate = (total_points - uncovered_points) / total_points * 100
    
    print(f"\nğŸ“Š ç‚¹è¦†ç›–ç»Ÿè®¡:")
    print(f"  - æœªè¦†ç›–ç‚¹æ•°: {uncovered_points:,} ({uncovered_points/total_points*100:.2f}%)")
    print(f"  - è¦†ç›–1æ¬¡: {covered_once:,} ({covered_once/total_points*100:.2f}%)")
    print(f"  - è¦†ç›–å¤šæ¬¡: {covered_multiple:,} ({covered_multiple/total_points*100:.2f}%)")
    print(f"  - æ€»è¦†ç›–ç‡: {coverage_rate:.2f}%")
    
    if uncovered_points > 0:
        print(f"\nâš ï¸  è­¦å‘Š: æœ‰ {uncovered_points:,} ä¸ªç‚¹æœªè¢«ä»»ä½•åˆ†å—è¦†ç›–ï¼")
        # æ˜¾ç¤ºå‰10ä¸ªæœªè¦†ç›–çš„ç‚¹ç´¢å¼•
        uncovered_indices = np.where(point_count == 0)[0]
        print(f"  æœªè¦†ç›–ç‚¹ç´¢å¼•ç¤ºä¾‹: {uncovered_indices[:10]}")
    else:
        print(f"\nâœ… æ‰€æœ‰ç‚¹éƒ½è¢«è¦†ç›–ï¼")
    
    # 8. æ¯ä¸ªç‚¹å‡ºç°æ¬¡æ•°çš„åˆ†å¸ƒ
    if covered_multiple > 0 or covered_once > 0:
        print(f"\nï¿½ ç‚¹å‡ºç°æ¬¡æ•°åˆ†å¸ƒ:")
        max_coverage = point_count.max()
        print(f"  - æœ€å¤§å‡ºç°æ¬¡æ•°: {max_coverage}")
        
        # ç»Ÿè®¡æ¯ä¸ªå‡ºç°æ¬¡æ•°çš„ç‚¹æ•°
        coverage_distribution = Counter(point_count[point_count > 0])
        print(f"  - è¯¦ç»†åˆ†å¸ƒ:")
        for times in sorted(coverage_distribution.keys()):
            count = coverage_distribution[times]
            percentage = count / num_unique_points * 100 if num_unique_points > 0 else 0
            print(f"    å‡ºç°{times}æ¬¡: {count:,} ç‚¹ ({percentage:.2f}%)")
    
    # åˆ†å—å¤§å°ç»Ÿè®¡
    print(f"\nğŸ“¦ åˆ†å—å¤§å°ç»Ÿè®¡:")
    segment_sizes = np.array(segment_sizes)
    print(f"  - æœ€å°åˆ†å—: {segment_sizes.min():,} ç‚¹")
    print(f"  - æœ€å¤§åˆ†å—: {segment_sizes.max():,} ç‚¹")
    print(f"  - å¹³å‡åˆ†å—: {segment_sizes.mean():.0f} ç‚¹")
    print(f"  - ä¸­ä½æ•°: {np.median(segment_sizes):.0f} ç‚¹")
    print(f"  - æ ‡å‡†å·®: {segment_sizes.std():.0f} ç‚¹")
    
    # åˆ†å—å¤§å°åˆ†å¸ƒ
    print(f"\n  - åˆ†å—å¤§å°åˆ†å¸ƒ:")
    bins = [0, 1000, 5000, 10000, 50000, 100000, float('inf')]
    labels = ['<1K', '1K-5K', '5K-10K', '10K-50K', '50K-100K', '>100K']
    
    for i in range(len(bins)-1):
        count = np.sum((segment_sizes >= bins[i]) & (segment_sizes < bins[i+1]))
        if count > 0:
            percentage = count / len(segment_sizes) * 100
            print(f"    {labels[i]}: {count} ä¸ªåˆ†å— ({percentage:.1f}%)")
    
    print(f"\n{'='*70}")
    
    # è¿”å›ç»Ÿè®¡ç»“æœ
    return {
        'total_points': total_points,
        'num_segments': num_segments,
        'total_point_instances': total_point_instances_in_segments,
        'num_unique_points': num_unique_points,
        'repetition_rate': repetition_rate,
        'uncovered_points': uncovered_points,
        'coverage_rate': coverage_rate,
        'covered_once': covered_once,
        'covered_multiple': covered_multiple,
        'max_coverage': point_count.max(),
        'segment_sizes': segment_sizes,
        'bin_point_count': bin_point_count,
    }


def compare_multiple_files(pkl_paths: List[Union[str, Path]]):
    """
    æ¯”è¾ƒå¤šä¸ªpklæ–‡ä»¶çš„ç»Ÿè®¡ç»“æœ
    
    Args:
        pkl_paths: pklæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    print("\n" + "="*70)
    print("æ‰¹é‡éªŒè¯å¤šä¸ªæ–‡ä»¶")
    print("="*70)
    
    all_results = []
    
    for pkl_path in pkl_paths:
        pkl_path = Path(pkl_path)
        if not pkl_path.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
            continue
        
        try:
            result = verify_segment_coverage(pkl_path)
            result['filename'] = pkl_path.name
            all_results.append(result)
            print()
        except Exception as e:
            print(f"âŒ å¤„ç† {pkl_path.name} æ—¶å‡ºé”™: {e}\n")
    
    if len(all_results) > 1:
        print("\n" + "="*70)
        print("æ±‡æ€»å¯¹æ¯”")
        print("="*70)
        print(f"\n{'æ–‡ä»¶å':<30} {'æ€»ç‚¹æ•°':>12} {'åˆ†å—æ•°':>8} {'è¦†ç›–ç‡':>8} {'é‡å¤ç‡':>8}")
        print("-"*70)
        
        for result in all_results:
            print(f"{result['filename']:<30} "
                  f"{result['total_points']:>12,} "
                  f"{result['num_segments']:>8,} "
                  f"{result['coverage_rate']:>7.1f}% "
                  f"{result['repetition_rate']:>7.1f}%")


def analyze_grid_sample_effect(pkl_with_grid: Union[str, Path], pkl_without_grid: Union[str, Path]):
    """
    å¯¹æ¯”æœ‰æ— grid sampleçš„æ•ˆæœ
    
    Args:
        pkl_with_grid: å¸¦grid sampleçš„pklæ–‡ä»¶
        pkl_without_grid: ä¸å¸¦grid sampleçš„pklæ–‡ä»¶
    """
    print("\n" + "="*70)
    print("Grid Sample æ•ˆæœå¯¹æ¯”")
    print("="*70)
    
    print("\n[1] ä¸å¸¦ Grid Sample:")
    result_without = verify_segment_coverage(pkl_without_grid)
    
    print("\n[2] å¸¦ Grid Sample:")
    result_with = verify_segment_coverage(pkl_with_grid)
    
    print("\n" + "="*70)
    print("å¯¹æ¯”ç»“æœ")
    print("="*70)
    
    print(f"\nåˆ†å—æ•°å˜åŒ–:")
    print(f"  ä¸å¸¦ Grid Sample: {result_without['num_segments']:,} ä¸ª")
    print(f"  å¸¦ Grid Sample: {result_with['num_segments']:,} ä¸ª")
    print(f"  å¢åŠ : {result_with['num_segments'] - result_without['num_segments']:,} ä¸ª "
          f"({(result_with['num_segments'] / result_without['num_segments'] - 1) * 100:+.1f}%)")
    
    print(f"\nè¦†ç›–ç‡:")
    print(f"  ä¸å¸¦ Grid Sample: {result_without['coverage_rate']:.2f}%")
    print(f"  å¸¦ Grid Sample: {result_with['coverage_rate']:.2f}%")
    
    print(f"\né‡å¤ç‡:")
    print(f"  ä¸å¸¦ Grid Sample: {result_without['avg_repetition_rate']:.2f}%")
    print(f"  å¸¦ Grid Sample: {result_with['avg_repetition_rate']:.2f}%")
    
    print(f"\nå¹³å‡åˆ†å—å¤§å°:")
    print(f"  ä¸å¸¦ Grid Sample: {result_without['segment_sizes'].mean():.0f} ç‚¹")
    print(f"  å¸¦ Grid Sample: {result_with['segment_sizes'].mean():.0f} ç‚¹")


if __name__ == "__main__":
    # ç¤ºä¾‹1: éªŒè¯å•ä¸ªæ–‡ä»¶
    print("ç¤ºä¾‹1: éªŒè¯å•ä¸ªpklæ–‡ä»¶")
    
    pkl_file = Path(r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\bin\train") / "processed_02.pkl"
    
    if pkl_file.exists():
        result = verify_segment_coverage(pkl_file)
    else:
        print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {pkl_file}")
        print("\nè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„æ–‡ä»¶è·¯å¾„ä¸ºå®é™…è·¯å¾„")
    
    # ç¤ºä¾‹2: æ‰¹é‡éªŒè¯å¤šä¸ªæ–‡ä»¶
    print("\n\n" + "="*70)
    print("ç¤ºä¾‹2: æ‰¹é‡éªŒè¯å¤šä¸ªæ–‡ä»¶")
    print("="*70)
    
    data_dir = Path(r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\bin\train")
    if data_dir.exists():
        pkl_files = list(data_dir.glob("*.pkl"))[:5]  # åªéªŒè¯å‰5ä¸ª
        if pkl_files:
            compare_multiple_files(pkl_files)
        else:
            print("âš ï¸  ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°pklæ–‡ä»¶")
    
    # ç¤ºä¾‹3: å¯¹æ¯”æœ‰æ— grid sampleçš„æ•ˆæœï¼ˆå¦‚æœæœ‰ä¸¤ä¸ªç‰ˆæœ¬çš„è¯ï¼‰
    # print("\n\n" + "="*70)
    # print("ç¤ºä¾‹3: Grid Sample æ•ˆæœå¯¹æ¯”")
    # print("="*70)
    # 
    # pkl_with_grid = Path(r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\bin\train_with_grid") / "5080_54400.pkl"
    # pkl_without_grid = Path(r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\bin\train") / "5080_54400.pkl"
    # 
    # if pkl_with_grid.exists() and pkl_without_grid.exists():
    #     analyze_grid_sample_effect(pkl_with_grid, pkl_without_grid)
