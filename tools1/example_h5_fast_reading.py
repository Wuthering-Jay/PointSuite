"""
å¿«é€ŸH5æ ¼å¼æ•°æ®è¯»å–ç¤ºä¾‹ - å…¨é¢æŒ‡å—

æ¼”ç¤ºå¦‚ä½•è¯»å–å¿«é€ŸH5æ ¼å¼ä¸­çš„æ‰€æœ‰æ•°æ®ï¼š
1. Headerä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼‰
2. Segmentæ•°æ®ï¼ˆç‚¹äº‘ã€åˆ†ç±»ã€å¼ºåº¦ç­‰ï¼‰
3. å•ä¸ªsegmentè¯»å–
4. æ‰¹é‡segmentè¯»å–
5. ä½¿ç”¨Datasetç±»è®­ç»ƒ
6. å¤šæ–‡ä»¶è¯»å–
"""

import h5py
import numpy as np
from pathlib import Path
import time


def example_1_read_header_info(h5_path: str):
    """
    ç¤ºä¾‹1ï¼šè¯»å–H5æ–‡ä»¶çš„headerä¿¡æ¯
    
    HeaderåŒ…å«ï¼š
    - ç‚¹äº‘å…ƒæ•°æ®ï¼ˆscale, offset, åæ ‡èŒƒå›´ç­‰ï¼‰
    - LASæ ¼å¼ä¿¡æ¯
    - å¯ç”¨å­—æ®µåˆ—è¡¨
    """
    print("="*80)
    print("ç¤ºä¾‹1ï¼šè¯»å–Headerä¿¡æ¯")
    print("="*80)
    
    with h5py.File(h5_path, 'r') as f:
        header = f['header']
        
        # åŸºæœ¬å…ƒæ•°æ®
        print("\nã€åŸºæœ¬å…ƒæ•°æ®ã€‘")
        print(f"  æ€»ç‚¹æ•°: {header.attrs['num_points']:,}")
        print(f"  Point Format: {header.attrs.get('point_format', 'N/A')}")
        print(f"  LASç‰ˆæœ¬: {header.attrs.get('version_major', 1)}.{header.attrs.get('version_minor', 2)}")
        
        # Scaleå’ŒOffsetï¼ˆç”¨äºåæ ‡è½¬æ¢ï¼‰
        print("\nã€åæ ‡å‚æ•°ã€‘")
        print(f"  X scale/offset: {header.attrs['x_scale']} / {header.attrs['x_offset']}")
        print(f"  Y scale/offset: {header.attrs['y_scale']} / {header.attrs['y_offset']}")
        print(f"  Z scale/offset: {header.attrs['z_scale']} / {header.attrs['z_offset']}")
        
        # CRSä¿¡æ¯ï¼ˆåæ ‡ç³»ç»Ÿï¼‰
        if 'crs' in header.attrs:
            print(f"\nã€åæ ‡ç³»ç»Ÿã€‘")
            print(f"  CRS: {header.attrs['crs']}")
        
        # å¯ç”¨å­—æ®µ
        if 'available_fields' in header.attrs:
            fields_str = header.attrs['available_fields']
            if isinstance(fields_str, bytes):
                fields_str = fields_str.decode('utf-8')
            fields = fields_str.split(',')
            print(f"\nã€å¯ç”¨å­—æ®µã€‘({len(fields)}ä¸ª)")
            for i, field in enumerate(fields, 1):
                print(f"  {i:2d}. {field}")
        
        # Segmentä¿¡æ¯
        print("\nã€Segmentä¿¡æ¯ã€‘")
        num_segments = f['segments'].attrs['num_segments']
        print(f"  æ€»segmentsæ•°: {num_segments}")


def example_2_read_single_segment(h5_path: str, segment_idx: int = 0):
    """
    ç¤ºä¾‹2ï¼šè¯»å–å•ä¸ªsegmentçš„æ‰€æœ‰æ•°æ®
    
    æ¯ä¸ªsegmentåŒ…å«ï¼š
    - åæ ‡ (x, y, z)
    - åˆ†ç±» (classification)
    - å…¶ä»–å­—æ®µï¼ˆintensity, gps_timeç­‰ï¼‰
    """
    print("\n" + "="*80)
    print(f"ç¤ºä¾‹2ï¼šè¯»å–å•ä¸ªSegmentï¼ˆsegment_{segment_idx:04d}ï¼‰")
    print("="*80)
    
    start = time.time()
    
    with h5py.File(h5_path, 'r') as f:
        seg = f['segments'][f'segment_{segment_idx:04d}']
        
        # è¯»å–åæ ‡ï¼ˆå¿…éœ€å­—æ®µï¼‰
        print("\nã€åæ ‡æ•°æ®ã€‘")
        x = seg['x'][:]
        y = seg['y'][:]
        z = seg['z'][:]
        print(f"  ç‚¹æ•°: {len(x):,}")
        print(f"  XèŒƒå›´: [{x.min():.2f}, {x.max():.2f}]")
        print(f"  YèŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
        print(f"  ZèŒƒå›´: [{z.min():.2f}, {z.max():.2f}]")
        
        # ç»„åˆä¸ºNx3æ•°ç»„ï¼ˆå¸¸ç”¨äºè®­ç»ƒï¼‰
        xyz = np.stack([x, y, z], axis=1)
        print(f"  XYZ shape: {xyz.shape}")
        
        # è¯»å–åˆ†ç±»ï¼ˆå¿…éœ€å­—æ®µï¼‰
        print("\nã€åˆ†ç±»æ•°æ®ã€‘")
        classification = seg['classification'][:]
        unique_labels, counts = np.unique(classification, return_counts=True)
        print(f"  å”¯ä¸€ç±»åˆ«: {list(unique_labels)}")
        print(f"  ç±»åˆ«åˆ†å¸ƒ:")
        for label, count in zip(unique_labels, counts):
            percentage = count / len(classification) * 100
            print(f"    ç±»åˆ« {label}: {count:6,} ç‚¹ ({percentage:5.2f}%)")
        
        # è¯»å–å…¶ä»–å¯ç”¨å­—æ®µ
        print("\nã€å…¶ä»–å­—æ®µã€‘")
        optional_fields = {
            'intensity': 'å¼ºåº¦',
            'return_number': 'å›æ³¢ç¼–å·',
            'number_of_returns': 'å›æ³¢æ€»æ•°',
            'gps_time': 'GPSæ—¶é—´',
            'scan_angle_rank': 'æ‰«æè§’åº¦',
            'point_source_id': 'ç‚¹æºID',
            'user_data': 'ç”¨æˆ·æ•°æ®',
            'red': 'çº¢è‰²é€šé“',
            'green': 'ç»¿è‰²é€šé“',
            'blue': 'è“è‰²é€šé“'
        }
        
        for field, description in optional_fields.items():
            if field in seg:
                data = seg[field][:]
                print(f"  {field} ({description}):")
                print(f"    - èŒƒå›´: [{data.min()}, {data.max()}]")
                print(f"    - ç±»å‹: {data.dtype}")
        
        # Metadata
        if 'num_points' in seg.attrs:
            print(f"\nã€å…ƒæ•°æ®ã€‘")
            print(f"  num_points: {seg.attrs['num_points']}")
    
    elapsed = time.time() - start
    print(f"\nâ±ï¸  è¯»å–è€—æ—¶: {elapsed*1000:.2f} ms")


def example_3_read_multiple_segments(h5_path: str, num_segments: int = 10):
    """
    ç¤ºä¾‹3ï¼šæ‰¹é‡è¯»å–å¤šä¸ªsegments
    
    å±•ç¤ºï¼š
    - å¾ªç¯è¯»å–
    - æ€§èƒ½æµ‹è¯•
    - æ•°æ®ç»Ÿè®¡
    """
    print("\n" + "="*80)
    print(f"ç¤ºä¾‹3ï¼šæ‰¹é‡è¯»å–{num_segments}ä¸ªSegments")
    print("="*80)
    
    start = time.time()
    
    all_xyz = []
    all_labels = []
    total_points = 0
    
    with h5py.File(h5_path, 'r') as f:
        max_segments = f['segments'].attrs['num_segments']
        num_to_read = min(num_segments, max_segments)
        
        print(f"\nè¯»å–å‰{num_to_read}ä¸ªsegments...")
        
        for i in range(num_to_read):
            seg = f['segments'][f'segment_{i:04d}']
            
            # è¯»å–xyz
            xyz = np.stack([
                seg['x'][:],
                seg['y'][:],
                seg['z'][:]
            ], axis=1)
            
            # è¯»å–æ ‡ç­¾
            labels = seg['classification'][:]
            
            all_xyz.append(xyz)
            all_labels.append(labels)
            total_points += len(xyz)
        
    elapsed = time.time() - start
    
    print(f"\nã€ç»Ÿè®¡ç»“æœã€‘")
    print(f"  è¯»å–segments: {len(all_xyz)}")
    print(f"  æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  å¹³å‡ç‚¹æ•°/segment: {total_points/len(all_xyz):.0f}")
    print(f"  æ€»è€—æ—¶: {elapsed:.3f}ç§’")
    print(f"  é€Ÿåº¦: {len(all_xyz)/elapsed:.2f} segments/ç§’")
    print(f"  å¹³å‡: {elapsed*1000/len(all_xyz):.2f} ms/segment")
    
    return all_xyz, all_labels


def example_4_efficient_reading_patterns(h5_path: str):
    """
    ç¤ºä¾‹4ï¼šé«˜æ•ˆè¯»å–æ¨¡å¼
    
    å¯¹æ¯”ï¼š
    - æŒ‰éœ€è¯»å–ï¼ˆé€ä¸ªæ‰“å¼€æ–‡ä»¶ï¼‰
    - ä¸€æ¬¡æ€§è¯»å–ï¼ˆæ–‡ä»¶ä¿æŒæ‰“å¼€ï¼‰
    - é¢„åŠ è½½åˆ°å†…å­˜
    """
    print("\n" + "="*80)
    print("ç¤ºä¾‹4ï¼šä¸åŒè¯»å–æ¨¡å¼æ€§èƒ½å¯¹æ¯”")
    print("="*80)
    
    with h5py.File(h5_path, 'r') as f:
        num_segments = min(50, f['segments'].attrs['num_segments'])
    
    # æ¨¡å¼1ï¼šæ¯æ¬¡éƒ½æ‰“å¼€å…³é—­æ–‡ä»¶ï¼ˆæ…¢ï¼‰
    print("\nã€æ¨¡å¼1ï¼šåå¤æ‰“å¼€æ–‡ä»¶ã€‘")
    start = time.time()
    for i in range(num_segments):
        with h5py.File(h5_path, 'r') as f:
            seg = f['segments'][f'segment_{i:04d}']
            xyz = np.stack([seg['x'][:], seg['y'][:], seg['z'][:]], axis=1)
    elapsed_1 = time.time() - start
    print(f"  è€—æ—¶: {elapsed_1:.3f}ç§’ ({num_segments/elapsed_1:.2f} seg/s)")
    
    # æ¨¡å¼2ï¼šæ–‡ä»¶ä¿æŒæ‰“å¼€ï¼ˆå¿«ï¼‰
    print("\nã€æ¨¡å¼2ï¼šæ–‡ä»¶ä¿æŒæ‰“å¼€ã€‘")
    start = time.time()
    with h5py.File(h5_path, 'r') as f:
        for i in range(num_segments):
            seg = f['segments'][f'segment_{i:04d}']
            xyz = np.stack([seg['x'][:], seg['y'][:], seg['z'][:]], axis=1)
    elapsed_2 = time.time() - start
    print(f"  è€—æ—¶: {elapsed_2:.3f}ç§’ ({num_segments/elapsed_2:.2f} seg/s)")
    print(f"  æå‡: {elapsed_1/elapsed_2:.2f}x")
    
    # æ¨¡å¼3ï¼šé¢„åŠ è½½åˆ°å†…å­˜ï¼ˆæœ€å¿«ï¼‰
    print("\nã€æ¨¡å¼3ï¼šé¢„åŠ è½½åˆ°å†…å­˜ã€‘")
    start = time.time()
    
    # é¢„åŠ è½½é˜¶æ®µ
    cache = []
    with h5py.File(h5_path, 'r') as f:
        for i in range(num_segments):
            seg = f['segments'][f'segment_{i:04d}']
            xyz = np.stack([seg['x'][:], seg['y'][:], seg['z'][:]], axis=1)
            labels = seg['classification'][:]
            cache.append((xyz, labels))
    
    preload_time = time.time() - start
    
    # è®¿é—®é˜¶æ®µ
    start = time.time()
    for xyz, labels in cache:
        pass  # ç›´æ¥ä»å†…å­˜è¯»å–
    access_time = time.time() - start
    
    # é¿å…é™¤ä»¥é›¶
    if access_time < 0.001:
        access_time = 0.001
    
    print(f"  é¢„åŠ è½½: {preload_time:.3f}ç§’ ({num_segments/preload_time:.2f} seg/s)")
    print(f"  è®¿é—®: {access_time:.3f}ç§’ ({num_segments/access_time:.2f} seg/s)")
    print(f"  æ€»æå‡: {elapsed_1/(preload_time+access_time):.2f}x")
    
    print("\nğŸ’¡ å»ºè®®ï¼šè®­ç»ƒæ—¶ä½¿ç”¨æ¨¡å¼3ï¼ˆé¢„åŠ è½½ï¼‰ï¼Œæ¨ç†æ—¶ä½¿ç”¨æ¨¡å¼2")


def example_5_use_with_dataset(h5_path: str):
    """
    ç¤ºä¾‹5ï¼šä½¿ç”¨Datasetç±»ï¼ˆæ¨èç”¨äºè®­ç»ƒï¼‰
    
    å±•ç¤ºï¼š
    - FastH5DatasetåŸºæœ¬ä½¿ç”¨
    - DataLoaderé›†æˆ
    - æ•°æ®å¢å¼º
    """
    print("\n" + "="*80)
    print("ç¤ºä¾‹5ï¼šä½¿ç”¨Datasetç±»è¿›è¡Œè®­ç»ƒ")
    print("="*80)
    
    # éœ€è¦å¯¼å…¥
    try:
        from h5_dataset_fast import FastH5Dataset, collate_fn
        from torch.utils.data import DataLoader
        
        print("\nã€åˆ›å»ºDatasetã€‘")
        
        # æ–¹å¼1ï¼šæŒ‰éœ€åŠ è½½
        print("\n1. æŒ‰éœ€åŠ è½½æ¨¡å¼:")
        dataset = FastH5Dataset(h5_path, preload=False)
        print(f"   - Segmentsæ•°: {len(dataset)}")
        
        # è¯»å–å•ä¸ªæ ·æœ¬
        xyz, labels = dataset[0]
        print(f"   - æ ·æœ¬0: xyz={xyz.shape}, labels={labels.shape}")
        
        # æ–¹å¼2ï¼šé¢„åŠ è½½ï¼ˆæ¨èï¼‰
        print("\n2. é¢„åŠ è½½æ¨¡å¼:")
        dataset_preload = FastH5Dataset(h5_path, preload=True)
        
        # åˆ›å»ºDataLoader
        print("\nã€åˆ›å»ºDataLoaderã€‘")
        dataloader = DataLoader(
            dataset_preload,
            batch_size=8,
            shuffle=True,
            num_workers=0,  # é¢„åŠ è½½ç”¨0
            collate_fn=collate_fn
        )
        
        print(f"   - Batch size: 8")
        print(f"   - Total batches: {len(dataloader)}")
        
        # è¿­ä»£å‡ ä¸ªbatch
        print("\nã€è¿­ä»£æ•°æ®ã€‘")
        start = time.time()
        for i, (batch_xyz, batch_labels) in enumerate(dataloader):
            if i >= 5:
                break
            print(f"   Batch {i}: {len(batch_xyz)} segments")
            for j, (xyz, labels) in enumerate(zip(batch_xyz, batch_labels)):
                print(f"     - Segment {j}: {xyz.shape}, labels={labels.shape}")
        
        elapsed = time.time() - start
        print(f"\n   â±ï¸  5ä¸ªbatchè€—æ—¶: {elapsed:.3f}ç§’")
        
    except ImportError as e:
        print(f"\nâŒ éœ€è¦å®‰è£…PyTorchå’Œh5_dataset_fast.py")
        print(f"   é”™è¯¯: {e}")


def example_6_multi_file_reading(h5_dir: str, max_files: int = 3):
    """
    ç¤ºä¾‹6ï¼šå¤šæ–‡ä»¶è¯»å–
    
    å±•ç¤ºï¼š
    - å¤šä¸ªH5æ–‡ä»¶çš„ç®¡ç†
    - å…¨å±€ç´¢å¼•æ˜ å°„
    - è·¨æ–‡ä»¶è¯»å–
    """
    print("\n" + "="*80)
    print("ç¤ºä¾‹6ï¼šå¤šæ–‡ä»¶è¯»å–")
    print("="*80)
    
    # æŸ¥æ‰¾H5æ–‡ä»¶
    h5_dir = Path(h5_dir)
    h5_files = sorted(h5_dir.glob("*.h5"))[:max_files]
    
    if not h5_files:
        print(f"âŒ æœªæ‰¾åˆ°H5æ–‡ä»¶: {h5_dir}")
        return
    
    print(f"\næ‰¾åˆ° {len(h5_files)} ä¸ªH5æ–‡ä»¶:")
    for i, f in enumerate(h5_files):
        print(f"  {i}. {f.name}")
    
    # æ–¹å¼1ï¼šæ‰‹åŠ¨ç®¡ç†å¤šæ–‡ä»¶
    print("\nã€æ–¹å¼1ï¼šæ‰‹åŠ¨ç®¡ç†ã€‘")
    file_segment_map = []
    total_segments = 0
    
    for file_idx, h5_file in enumerate(h5_files):
        with h5py.File(h5_file, 'r') as f:
            num_segs = f['segments'].attrs['num_segments']
            for seg_idx in range(num_segs):
                file_segment_map.append((file_idx, seg_idx))
            total_segments += num_segs
            print(f"  æ–‡ä»¶ {file_idx}: {num_segs} segments")
    
    print(f"\n  æ€»segments: {total_segments}")
    
    # éšæœºè¯»å–ç¤ºä¾‹
    print("\n  éšæœºè¯»å–3ä¸ªsegments:")
    import random
    for global_idx in random.sample(range(total_segments), 3):
        file_idx, seg_idx = file_segment_map[global_idx]
        with h5py.File(h5_files[file_idx], 'r') as f:
            seg = f['segments'][f'segment_{seg_idx:04d}']
            num_points = len(seg['x'])
        print(f"    å…¨å±€ç´¢å¼•{global_idx} -> æ–‡ä»¶{file_idx}, segment{seg_idx}, {num_points}ç‚¹")
    
    # æ–¹å¼2ï¼šä½¿ç”¨FastMultiH5Datasetï¼ˆæ¨èï¼‰
    print("\nã€æ–¹å¼2ï¼šä½¿ç”¨FastMultiH5Datasetï¼ˆæ¨èï¼‰ã€‘")
    try:
        from h5_dataset_fast import FastMultiH5Dataset
        
        dataset = FastMultiH5Dataset(
            [str(f) for f in h5_files],
            preload_strategy="none"
        )
        
        print(f"  æ€»segments: {len(dataset)}")
        print(f"  éšæœºè¯»å–ç¤ºä¾‹:")
        
        xyz, labels = dataset[0]
        print(f"    æ ·æœ¬0: {xyz.shape}")
        
        xyz, labels = dataset[total_segments // 2]
        print(f"    æ ·æœ¬{total_segments // 2}: {xyz.shape}")
        
    except ImportError:
        print("  âš ï¸  éœ€è¦h5_dataset_fast.py")


def example_7_advanced_operations(h5_path: str):
    """
    ç¤ºä¾‹7ï¼šé«˜çº§æ“ä½œ
    
    å±•ç¤ºï¼š
    - ç»Ÿè®¡åˆ†æ
    - ç©ºé—´æŸ¥è¯¢
    - æ•°æ®ç­›é€‰
    """
    print("\n" + "="*80)
    print("ç¤ºä¾‹7ï¼šé«˜çº§æ“ä½œ")
    print("="*80)
    
    with h5py.File(h5_path, 'r') as f:
        num_segments = f['segments'].attrs['num_segments']
        
        # ç»Ÿè®¡æ‰€æœ‰segmentsçš„ç‚¹æ•°åˆ†å¸ƒ
        print("\nã€ç‚¹æ•°ç»Ÿè®¡ã€‘")
        point_counts = []
        for i in range(num_segments):
            seg = f['segments'][f'segment_{i:04d}']
            point_counts.append(len(seg['x']))
        
        point_counts = np.array(point_counts)
        print(f"  æœ€å°: {point_counts.min():,} ç‚¹")
        print(f"  æœ€å¤§: {point_counts.max():,} ç‚¹")
        print(f"  å¹³å‡: {point_counts.mean():.0f} ç‚¹")
        print(f"  ä¸­ä½æ•°: {np.median(point_counts):.0f} ç‚¹")
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒï¼ˆæ‰€æœ‰segmentsï¼‰
        print("\nã€å…¨å±€ç±»åˆ«åˆ†å¸ƒã€‘")
        all_labels = []
        for i in range(min(20, num_segments)):  # é‡‡æ ·å‰20ä¸ª
            seg = f['segments'][f'segment_{i:04d}']
            all_labels.append(seg['classification'][:])
        
        all_labels = np.concatenate(all_labels)
        unique_labels, counts = np.unique(all_labels, return_counts=True)
        
        print(f"  é‡‡æ ·ç‚¹æ•°: {len(all_labels):,}")
        print(f"  ç±»åˆ«åˆ†å¸ƒ:")
        for label, count in zip(unique_labels, counts):
            percentage = count / len(all_labels) * 100
            print(f"    ç±»åˆ« {label}: {count:8,} ({percentage:5.2f}%)")
        
        # æŸ¥æ‰¾ç‰¹å®šæ¡ä»¶çš„segments
        print("\nã€æ¡ä»¶ç­›é€‰ã€‘")
        large_segments = [i for i, count in enumerate(point_counts) if count > 50000]
        print(f"  å¤§segments (>50kç‚¹): {len(large_segments)}ä¸ª")
        if large_segments:
            print(f"    ç´¢å¼•: {large_segments[:5]}{'...' if len(large_segments) > 5 else ''}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    
    # é…ç½®H5æ–‡ä»¶è·¯å¾„
    h5_path = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\h5_fast\train\processed_02.h5"
    h5_dir = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\h5_fast\train"
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
    if not Path(h5_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {h5_path}")
        print("\nè¯·ä¿®æ”¹main()å‡½æ•°ä¸­çš„h5_pathå˜é‡")
        return
    
    print("\n" + "ğŸš€ "*40)
    print("å¿«é€ŸH5æ ¼å¼æ•°æ®è¯»å– - å…¨é¢ç¤ºä¾‹".center(80))
    print("ğŸš€ "*40)
    
    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    example_1_read_header_info(h5_path)
    example_2_read_single_segment(h5_path, segment_idx=0)
    example_3_read_multiple_segments(h5_path, num_segments=10)
    example_4_efficient_reading_patterns(h5_path)
    example_5_use_with_dataset(h5_path)
    example_6_multi_file_reading(h5_dir, max_files=3)
    example_7_advanced_operations(h5_path)
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("="*80)
    print("\nğŸ’¡ æç¤º:")
    print("  - è®­ç»ƒæ—¶ä½¿ç”¨ FastH5Dataset(preload=True) + DataLoader")
    print("  - å¤šæ–‡ä»¶ç”¨ FastMultiH5Dataset(preload_strategy='all')")
    print("  - è®°å¾—è®¾ç½® num_workers=0ï¼ˆé¢„åŠ è½½æ¨¡å¼ï¼‰")
    print("="*80)


if __name__ == "__main__":
    main()
