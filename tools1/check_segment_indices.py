"""
æ£€æŸ¥pklæ–‡ä»¶ä¸­å„ä¸ªåˆ†å—(segment)æ˜¯å¦å®Œå…¨ç›¸åŒæˆ–æœ‰å·®å¼‚
"""
import pickle
import numpy as np
from pathlib import Path
from collections import Counter


def check_segment_uniqueness(pkl_path):
    """
    æ£€æŸ¥å„ä¸ªsegmentä¹‹é—´æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„ï¼ˆæ— æ„ä¹‰é‡å¤ï¼‰
    
    Args:
        pkl_path: pklæ–‡ä»¶è·¯å¾„
    """
    pkl_path = Path(pkl_path)
    
    print("="*70)
    print(f"æ£€æŸ¥æ–‡ä»¶: {pkl_path.name}")
    print("="*70)
    
    # åŠ è½½pklæ–‡ä»¶
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    
    total_points = metadata['num_points']
    num_segments = metadata['num_segments']
    grid_size = metadata.get('grid_size', None)
    
    print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"  - æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  - åˆ†å—æ•°: {num_segments:,}")
    print(f"  - Grid Size: {grid_size if grid_size else 'N/A (æœªä½¿ç”¨grid sampling)'}")
    
    # æ”¶é›†æ‰€æœ‰åˆ†å—çš„ç´¢å¼•
    segments_list = []
    segment_sizes = []
    
    print(f"\nğŸ“¦ åˆ†å—è¯¦æƒ…:")
    for i, segment_info in enumerate(metadata['segments']):
        indices = segment_info['indices']
        segments_list.append(indices)
        segment_sizes.append(len(indices))
        
        if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªåˆ†å—çš„è¯¦ç»†ä¿¡æ¯
            print(f"  Segment {i}: {len(indices):,} ç‚¹ "
                  f"[ç´¢å¼•èŒƒå›´: {indices.min()}-{indices.max()}]")
        elif i == 5:
            print(f"  ... (çœç•¥ä¸­é—´ {num_segments - 10} ä¸ªåˆ†å—)")
        elif i >= num_segments - 5:
            print(f"  Segment {i}: {len(indices):,} ç‚¹ "
                  f"[ç´¢å¼•èŒƒå›´: {indices.min()}-{indices.max()}]")
    
    print(f"\nğŸ“ˆ åˆ†å—å¤§å°ç»Ÿè®¡:")
    segment_sizes = np.array(segment_sizes)
    print(f"  - æœ€å°: {segment_sizes.min():,} ç‚¹")
    print(f"  - æœ€å¤§: {segment_sizes.max():,} ç‚¹")
    print(f"  - å¹³å‡: {segment_sizes.mean():.1f} ç‚¹")
    print(f"  - ä¸­ä½æ•°: {np.median(segment_sizes):.0f} ç‚¹")
    print(f"  - æ ‡å‡†å·®: {segment_sizes.std():.1f}")
    
    # ==================== å…³é”®æ£€æŸ¥ï¼šsegmentä¹‹é—´æ˜¯å¦å®Œå…¨ç›¸åŒ ====================
    print(f"\nğŸ” Segmentå”¯ä¸€æ€§æ£€æŸ¥ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„segmentï¼‰:")
    
    # å°†æ¯ä¸ªsegmentè½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å½¢å¼ï¼ˆæ’åºåçš„tupleï¼‰
    segment_hashes = []
    for i, seg in enumerate(segments_list):
        # æ’åºåè½¬ä¸ºtupleï¼Œæ–¹ä¾¿æ¯”è¾ƒ
        sorted_seg = tuple(sorted(seg.tolist()))
        segment_hashes.append(sorted_seg)
    
    # ç»Ÿè®¡æ¯ä¸ªsegmentå‡ºç°çš„æ¬¡æ•°
    hash_counter = Counter(segment_hashes)
    duplicate_segments = {h: count for h, count in hash_counter.items() if count > 1}
    
    print(f"  - æ€»segmentæ•°: {num_segments:,}")
    print(f"  - å”¯ä¸€segmentæ•°: {len(hash_counter):,}")
    print(f"  - å®Œå…¨ç›¸åŒçš„segmentç»„æ•°: {len(duplicate_segments)}")
    
    if len(duplicate_segments) == 0:
        print(f"  âœ… æ‰€æœ‰segmentéƒ½ä¸ç›¸åŒï¼Œæ²¡æœ‰æ— æ„ä¹‰çš„é‡å¤ï¼")
    else:
        print(f"  âš ï¸ å‘ç°å®Œå…¨ç›¸åŒçš„segmentï¼")
        print(f"\n  é‡å¤segmentè¯¦æƒ…ï¼ˆå‰10ç»„ï¼‰:")
        
        # æ‰¾å‡ºå“ªäº›segmentæ˜¯é‡å¤çš„
        for idx, (seg_hash, count) in enumerate(list(duplicate_segments.items())[:10]):
            # æ‰¾åˆ°æ‰€æœ‰å…·æœ‰ç›¸åŒhashçš„segmentç´¢å¼•
            duplicate_indices = [i for i, h in enumerate(segment_hashes) if h == seg_hash]
            seg_size = len(seg_hash)
            
            print(f"    ç»„{idx+1}: {count}ä¸ªç›¸åŒsegment (æ¯ä¸ª{seg_size:,}ç‚¹)")
            print(f"      Segment IDs: {duplicate_indices}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç‚¹ç´¢å¼•
            first_few = list(seg_hash[:5])
            print(f"      å‰5ä¸ªç‚¹ç´¢å¼•: {first_few}")
    
    # ==================== è¡¥å……æ£€æŸ¥ï¼šsegmentä¹‹é—´çš„ç›¸ä¼¼åº¦ ====================
    print(f"\nğŸ”¬ Segmentç›¸ä¼¼åº¦åˆ†æï¼ˆæ£€æŸ¥é‡å ç¨‹åº¦ï¼‰:")
    
    # è®¡ç®—ç›¸é‚»segmentä¹‹é—´çš„äº¤é›†æ¯”ä¾‹ï¼ˆé‡‡æ ·æ£€æŸ¥ï¼Œé¿å…è¿‡æ…¢ï¼‰
    sample_size = min(50, num_segments - 1)
    if num_segments > 1:
        overlap_ratios = []
        for i in range(sample_size):
            seg1 = set(segments_list[i].tolist())
            seg2 = set(segments_list[i + 1].tolist())
            intersection = len(seg1 & seg2)
            union = len(seg1 | seg2)
            overlap_ratio = intersection / union if union > 0 else 0
            overlap_ratios.append(overlap_ratio)
        
        avg_overlap = np.mean(overlap_ratios)
        max_overlap = np.max(overlap_ratios)
        
        print(f"  - é‡‡æ ·æ£€æŸ¥: å‰{sample_size}å¯¹ç›¸é‚»segment")
        print(f"  - å¹³å‡é‡å ç‡: {avg_overlap*100:.2f}%")
        print(f"  - æœ€å¤§é‡å ç‡: {max_overlap*100:.2f}%")
        
        if avg_overlap < 0.1:
            print(f"  âœ… ç›¸é‚»segmenté‡å å¾ˆå°‘ï¼Œè¯´æ˜æ˜¯ä¸åŒçš„åˆ†å—")
        elif avg_overlap > 0.8:
            print(f"  âš ï¸ ç›¸é‚»segmenté‡å å¾ˆå¤šï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
        else:
            print(f"  â„¹ï¸ ç›¸é‚»segmentæœ‰ä¸€å®šé‡å ï¼ˆå¯èƒ½æ˜¯grid samplingå¯¼è‡´ï¼‰")
    
    # ==================== åŸæœ‰çš„æ£€æŸ¥ï¼šç‚¹ç´¢å¼•è¦†ç›–æƒ…å†µ ====================
    print(f"\nğŸ“Š ç‚¹ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡:")
    all_indices = []
    for seg in segments_list:
        all_indices.extend(seg.tolist())
    
    all_indices_array = np.array(all_indices)
    unique_indices = np.unique(all_indices_array)
    
    print(f"  - æ‰€æœ‰ç´¢å¼•æ€»æ•°: {len(all_indices_array):,}")
    print(f"  - å”¯ä¸€ç´¢å¼•æ•°é‡: {len(unique_indices):,}")
    print(f"  - é‡å¤ä½¿ç”¨æ¬¡æ•°: {len(all_indices_array) - len(unique_indices):,}")
    
    if len(all_indices_array) > len(unique_indices):
        # ç»Ÿè®¡æ¯ä¸ªç‚¹è¢«ä½¿ç”¨çš„æ¬¡æ•°
        counter = Counter(all_indices)
        reuse_counts = list(counter.values())
        avg_reuse = np.mean(reuse_counts)
        max_reuse = np.max(reuse_counts)
        
        print(f"  - å¹³å‡æ¯ä¸ªç‚¹è¢«ä½¿ç”¨: {avg_reuse:.2f} æ¬¡")
        print(f"  - æœ€å¤šè¢«ä½¿ç”¨: {max_reuse} æ¬¡")
        print(f"  â„¹ï¸ è¿™æ˜¯Grid Sampling Testæ¨¡å¼çš„æ­£å¸¸è¡Œä¸º")
    
    # æ£€æŸ¥è¦†ç›–ç‡
    print(f"\nğŸ¯ ç´¢å¼•è¦†ç›–ç‡æ£€æŸ¥:")
    expected_indices = set(range(total_points))
    actual_indices = set(all_indices)
    
    coverage = len(actual_indices) / total_points * 100
    print(f"  - åº”è¯¥è¦†ç›–: {total_points:,} ä¸ªç´¢å¼• (0-{total_points-1})")
    print(f"  - å®é™…è¦†ç›–: {len(actual_indices):,} ä¸ªç´¢å¼•")
    print(f"  - è¦†ç›–ç‡: {coverage:.2f}%")
    
    if actual_indices == expected_indices:
        print(f"  âœ… å®Œå…¨è¦†ç›–ï¼Œæ‰€æœ‰ç‚¹éƒ½åœ¨æŸä¸ªåˆ†å—ä¸­ï¼")
    else:
        missing = expected_indices - actual_indices
        extra = actual_indices - expected_indices
        
        if missing:
            print(f"  âš ï¸ æœ‰ {len(missing)} ä¸ªç‚¹æœªè¢«ä»»ä½•åˆ†å—åŒ…å«")
            if len(missing) <= 10:
                print(f"    ç¼ºå¤±ç´¢å¼•: {sorted(list(missing))}")
            else:
                print(f"    ç¼ºå¤±ç´¢å¼•ç¤ºä¾‹: {sorted(list(missing))[:10]} ...")
        
        if extra:
            print(f"  âš ï¸ æœ‰ {len(extra)} ä¸ªç´¢å¼•è¶…å‡ºèŒƒå›´")
            if len(extra) <= 10:
                print(f"    è¶…å‡ºç´¢å¼•: {sorted(list(extra))}")
            else:
                print(f"    è¶…å‡ºç´¢å¼•ç¤ºä¾‹: {sorted(list(extra))[:10]} ...")
    
    # å¦‚æœä½¿ç”¨äº†grid samplingï¼Œæ£€æŸ¥é‡‡æ ·ç‡
    if grid_size:
        print(f"\nğŸ”¬ Grid Sampling ç»Ÿè®¡:")
        total_sampled_points = sum(segment_sizes)
        sampling_ratio = total_sampled_points / total_points
        print(f"  - åŸå§‹æ€»ç‚¹æ•°: {total_points:,}")
        print(f"  - é‡‡æ ·åæ€»ç‚¹æ•°: {total_sampled_points:,}")
        print(f"  - é‡‡æ ·å€ç‡: {sampling_ratio:.2f}x")
        
        if sampling_ratio > 1:
            print(f"  â„¹ï¸ é‡‡æ ·å€ç‡>1è¡¨ç¤ºtestæ¨¡å¼äº§ç”Ÿäº†å¤šæ¬¡é‡‡æ ·")
        elif sampling_ratio < 1:
            print(f"  â„¹ï¸ é‡‡æ ·å€ç‡<1è¡¨ç¤ºè¿›è¡Œäº†ä¸‹é‡‡æ ·")
        else:
            print(f"  â„¹ï¸ é‡‡æ ·å€ç‡=1è¡¨ç¤ºæ¯ä¸ªç‚¹åªé‡‡æ ·ä¸€æ¬¡")
    
    print(f"\n" + "="*70)


def check_multiple_pkl_files(pkl_dir):
    """
    æ‰¹é‡æ£€æŸ¥ç›®å½•ä¸‹æ‰€æœ‰pklæ–‡ä»¶
    
    Args:
        pkl_dir: åŒ…å«pklæ–‡ä»¶çš„ç›®å½•
    """
    pkl_dir = Path(pkl_dir)
    pkl_files = list(pkl_dir.glob('*.pkl'))
    
    if not pkl_files:
        print(f"âŒ ç›®å½• {pkl_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°pklæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(pkl_files)} ä¸ªpklæ–‡ä»¶\n")
    
    for pkl_file in pkl_files:
        check_segment_uniqueness(pkl_file)
        print("\n")


if __name__ == "__main__":
    # ç¤ºä¾‹1: æ£€æŸ¥å•ä¸ªpklæ–‡ä»¶
    # pkl_file = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\bin\train_with_gridsample\processed_02.pkl"
    
    # if Path(pkl_file).exists():
    #     check_segment_uniqueness(pkl_file)
    # else:
    #     print(f"æ–‡ä»¶ä¸å­˜åœ¨: {pkl_file}")
    #     print("\nè¯·ä¿®æ”¹è·¯å¾„ä¸ºä½ çš„å®é™…pklæ–‡ä»¶è·¯å¾„")
    
    # ç¤ºä¾‹2: æ‰¹é‡æ£€æŸ¥ç›®å½•ä¸‹æ‰€æœ‰pklæ–‡ä»¶
    pkl_dir = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\bin\train_with_gridsample"
    check_multiple_pkl_files(pkl_dir)
