import numpy as np
import laspy
import pickle
from pathlib import Path
from typing import Union, List, Tuple, Optional, Dict, Any
from sklearn.neighbors import KDTree
from tqdm import tqdm
from collections import defaultdict


class LASProcessorToBin:
    def __init__(self,
                 input_path: Union[str, Path],
                 output_dir: Union[str, Path] = None,
                 window_size: Tuple[float, float] = (50.0, 50.0),
                 min_points: Optional[int] = 1000,
                 max_points: Optional[int] = 5000,
                 overlap: bool = False):
        """
        Initialize LAS point cloud processor that saves to bin+pkl format.
        
        Args:
            input_path: Path to LAS file or directory containing LAS files
            output_dir: Directory to save processed files (default: same as input)
            window_size: (x_size, y_size) for rectangular windows (in units of the LAS file)
            min_points: Minimum points threshold for a valid segment (None to skip)
            max_points: Maximum points threshold before further segmentation (None to skip)
            overlap: Whether to use overlap mode (offset grid by half window size)
        """
        self.input_path = Path(input_path)
        self.output_dir = Path(output_dir) if output_dir else self.input_path.parent
        self.window_size = window_size
        self.min_points = min_points
        self.max_points = max_points
        self.overlap = overlap
        
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True)
            
        self.las_files = self._find_las_files()
    
    def _find_las_files(self) -> List[Path]:
        """Find all LAS files in the input path."""
        if self.input_path.is_file() and self.input_path.suffix.lower() in ['.las', '.laz']:
            return [self.input_path]
        elif self.input_path.is_dir():
            return list(self.input_path.glob('*.las')) + list(self.input_path.glob('*.laz'))
        else:
            raise ValueError(f"Input path {self.input_path} is not a valid LAS file or directory")
    
    def process_all_files(self, n_workers: int = None):
        """
        Process all discovered LAS files.
        å¹¶è¡Œå¤„ç†åœ¨å•ä¸ªLASæ–‡ä»¶å†…éƒ¨è¿›è¡Œï¼Œè€Œä¸æ˜¯è·¨æ–‡ä»¶å¹¶è¡Œã€‚
        
        Args:
            n_workers: Number of parallel workers for segment processing (None = auto)
        """
        import time
        import multiprocessing
        
        if n_workers is None:
            n_workers = max(1, multiprocessing.cpu_count() - 1)
        
        start_time = time.time()
        
        print("="*70)
        print(f"Starting LAS to BIN/PKL conversion")
        print("="*70)
        print(f"Total files: {len(self.las_files)}")
        print(f"Window size: {self.window_size}")
        print(f"Min points: {self.min_points}")
        print(f"Max points: {self.max_points}")
        print(f"Overlap mode: {'âœ… Enabled' if self.overlap else 'âŒ Disabled'}")
        print(f"Parallel workers: {n_workers} (per file)")
        print("-"*70)
        
        # é¡ºåºå¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼Œä½†æ–‡ä»¶å†…éƒ¨å¹¶è¡Œå¤„ç†segments
        for las_file in tqdm(self.las_files, desc="Processing files", unit="file",
                            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'):
            try:
                self.process_file(las_file, n_workers=n_workers)
            except Exception as e:
                print(f"\n[ERROR] {las_file.name}: {e}")
                import traceback
                traceback.print_exc()
        
        elapsed_time = time.time() - start_time
        print("\n" + "="*70)
        print(f"Conversion completed successfully!")
        print(f"Total time: {elapsed_time:.2f}s ({elapsed_time/60:.2f}min)")
        print(f"Average: {elapsed_time/len(self.las_files):.2f}s per file")
        print("="*70)
    
    def process_file(self, las_file: Union[str, Path], n_workers: int = 4):
        """
        Process a single LAS file and save to bin+pkl format.
        
        Args:
            las_file: Path to LAS file
            n_workers: Number of parallel workers for segment processing
        """
        import time
        las_file = Path(las_file)
        
        file_start = time.time()
        print(f"\n{'='*70}")
        print(f"ğŸ“„ Processing: {las_file.name}")
        print(f"{'='*70}")
        
        # 1. è¯»å–LASæ–‡ä»¶
        t0 = time.time()
        with laspy.open(las_file) as fh:
            las_data = fh.read()
        t1 = time.time()
        print(f"  âœ“ è¯»å–LASæ–‡ä»¶: {t1-t0:.2f}s ({len(las_data.points):,} ç‚¹)")
        
        # 2. å‡†å¤‡ç‚¹äº‘æ•°æ®
        t0 = time.time()
        point_data = np.vstack((
            las_data.x, 
            las_data.y, 
            las_data.z
        )).transpose()
        t1 = time.time()
        print(f"  âœ“ å‡†å¤‡ç‚¹äº‘æ•°æ®: {t1-t0:.2f}s")
        
        # 3. åˆ†å‰²å¤„ç†ï¼ˆè¿™é‡Œä¼šæœ‰è¯¦ç»†å­é˜¶æ®µè¾“å‡ºï¼‰
        t0 = time.time()
        segments = self.segment_point_cloud(point_data, n_workers=n_workers)
        t1 = time.time()
        print(f"  âœ“ æ€»åˆ†å‰²æ—¶é—´: {t1-t0:.2f}s â†’ {len(segments)} segments")
        
        # 4. ä¿å­˜æ–‡ä»¶
        t0 = time.time()
        self.save_segments_as_bin_pkl(las_file, las_data, segments)
        t1 = time.time()
        print(f"  âœ“ ä¿å­˜æ–‡ä»¶: {t1-t0:.2f}s")
        
        file_total = time.time() - file_start
        print(f"  ğŸ¯ æ€»è®¡: {file_total:.2f}s")
        print(f"{'='*70}")
    
    def segment_point_cloud(self, points: np.ndarray, n_workers: int = 4) -> List[np.ndarray]:
        """
        Segment point cloud into tiles based on window size.
        
        Args:
            points: Point cloud array (N, 3)
            n_workers: Number of parallel workers
            
        Returns:
            List of segment indices
        """
        import time
        
        print(f"  ğŸ“¦ å¼€å§‹åˆ†å‰² ({n_workers} workers)...")
        
        if not self.overlap:
            # æ­£å¸¸æ¨¡å¼ï¼šå•æ¬¡ç½‘æ ¼åˆ†å‰²
            t0 = time.time()
            segments = self._grid_segmentation(points, offset_x=0, offset_y=0, n_workers=n_workers)
            t1 = time.time()
            print(f"     å•æ¬¡ç½‘æ ¼åˆ†å‰²: {t1-t0:.2f}s")
            return segments
        else:
            # Overlapæ¨¡å¼ï¼šä¸¤æ¬¡ç½‘æ ¼åˆ†å‰²ï¼ˆåç§»åŠä¸ªçª—å£ï¼‰
            x_size, y_size = self.window_size
            
            # ç¬¬ä¸€æ¬¡åˆ†å‰²ï¼šæ­£å¸¸ç½‘æ ¼
            t0 = time.time()
            segments1 = self._grid_segmentation(points, offset_x=0, offset_y=0, n_workers=n_workers)
            t1 = time.time()
            print(f"     ç¬¬1æ¬¡ç½‘æ ¼åˆ†å‰²: {t1-t0:.2f}s â†’ {len(segments1)} segments")
            
            # ç¬¬äºŒæ¬¡åˆ†å‰²ï¼šåç§»åŠä¸ªçª—å£
            t0 = time.time()
            segments2 = self._grid_segmentation(points, offset_x=x_size/2, offset_y=y_size/2, n_workers=n_workers)
            t1 = time.time()
            print(f"     ç¬¬2æ¬¡ç½‘æ ¼åˆ†å‰²: {t1-t0:.2f}s â†’ {len(segments2)} segments")
            
            # åˆå¹¶ä¸¤æ¬¡åˆ†å‰²ç»“æœ
            all_segments = segments1 + segments2
            
            return all_segments
    
    def _grid_segmentation(self, points: np.ndarray, offset_x: float = 0, offset_y: float = 0, n_workers: int = 4) -> List[np.ndarray]:
        """
        Perform grid-based segmentation with optional offset.
        
        Args:
            points: Point cloud array (N, 3)
            offset_x: X offset for grid origin
            offset_y: Y offset for grid origin
            n_workers: Number of parallel workers
            
        Returns:
            List of segment indices
        """
        import time
        
        # 1. çª—å£åˆ†ç»„
        t0 = time.time()
        min_x, min_y = np.min(points[:, 0]), np.min(points[:, 1])
        max_x, max_y = np.max(points[:, 0]), np.max(points[:, 1])
        x_size, y_size = self.window_size
        
        # åº”ç”¨åç§»
        origin_x = min_x - offset_x
        origin_y = min_y - offset_y
        
        num_windows_x = max(1, int(np.ceil((max_x - origin_x) / x_size)))
        num_windows_y = max(1, int(np.ceil((max_y - origin_y) / y_size)))
        
        # è®¡ç®—æ¯ä¸ªç‚¹æ‰€å±çš„çª—å£ç´¢å¼•
        x_bins = np.clip(((points[:, 0] - origin_x) / x_size).astype(int), 0, num_windows_x - 1)
        y_bins = np.clip(((points[:, 1] - origin_y) / y_size).astype(int), 0, num_windows_y - 1)
        
        # ç»„åˆçª—å£ç´¢å¼•
        window_ids = x_bins * num_windows_y + y_bins
        
        # åˆ†ç»„
        unique_ids, indices = np.unique(window_ids, return_inverse=True)
        segments = [np.where(indices == i)[0] for i in range(len(unique_ids))]
        t1 = time.time()
        print(f"       - çª—å£åˆ†ç»„: {t1-t0:.3f}s â†’ {len(segments)} çª—å£")
        
        # 2. Miné˜ˆå€¼å¤„ç†ï¼ˆä¼˜å…ˆå¤„ç†ï¼Œåˆå¹¶è¾¹ç•Œä¸Šç‚¹å°‘çš„æ— æ•ˆçª—å£ï¼‰
        if self.min_points is not None:
            t0 = time.time()
            before_count = len(segments)
            segments = self.apply_min_threshold(points, segments, min_threshold=self.min_points)
            t1 = time.time()
            print(f"       - Miné˜ˆå€¼å¤„ç†: {t1-t0:.3f}s ({before_count} â†’ {len(segments)} segments)")
        
        # 3. Maxé˜ˆå€¼å¤„ç†ï¼ˆæœ€åå¤„ç†ï¼‰
        if self.max_points is not None:
            t0 = time.time()
            before_count = len(segments)
            segments = self.apply_max_threshold(points, segments, n_workers=n_workers)
            t1 = time.time()
            print(f"       - Maxé˜ˆå€¼å¤„ç†: {t1-t0:.3f}s ({before_count} â†’ {len(segments)} segments)")
            
        return segments
    
    def apply_max_threshold(self, points: np.ndarray, segments: List[np.ndarray], n_workers: int = 4) -> List[np.ndarray]:
        """
        Apply max_points threshold to segments, subdividing large segments.
        
        Args:
            points: Point cloud array
            segments: List of segment indices
            n_workers: Number of parallel workers
            
        Returns:
            List of processed segment indices
        """
        large_segment_indices = [i for i, segment in enumerate(segments) if len(segment) > self.max_points]
        
        if not large_segment_indices:
            return segments
        
        result_segments = [segment for i, segment in enumerate(segments) if i not in large_segment_indices]
        large_segments = [segments[i] for i in large_segment_indices]
        
        def process_segment(segment):
            if len(segment) <= self.max_points:
                return [segment]
            
            segment_points = points[segment]
            ranges = np.ptp(segment_points[:, :2], axis=0)
            split_dim = np.argmax(ranges[:2])
            sorted_indices = np.argsort(segment_points[:, split_dim])
            
            mid = len(sorted_indices) // 2
            left_half = segment[sorted_indices[:mid]]
            right_half = segment[sorted_indices[mid:]]
            
            result = []
            result.extend(process_segment(left_half))
            result.extend(process_segment(right_half))
            return result
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # ä½¿ç”¨ä¼ å…¥çš„n_workerså‚æ•°
        max_workers = min(n_workers, len(large_segments)) if len(large_segments) > 0 else 1
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_segment, segment) for segment in large_segments]
            for future in as_completed(futures):
                result_segments.extend(future.result())
        
        return result_segments
    
    def apply_min_threshold(self, points: np.ndarray, segments: List[np.ndarray], 
                           min_threshold: Optional[int] = None) -> List[np.ndarray]:
        """
        Apply min_points threshold using KD-Tree.
        
        Args:
            points: Point cloud array
            segments: List of segment indices
            min_threshold: Minimum points threshold (if None, use self.min_points)
        
        Returns:
            List of processed segment indices
        """
        if len(segments) <= 1:
            return segments
        
        # ä½¿ç”¨ä¼ å…¥çš„min_thresholdï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨self.min_points
        effective_min = min_threshold if min_threshold is not None else self.min_points
        
        centroids = np.array([np.mean(points[segment][:, :2], axis=0) for segment in segments])
        small_segments = [i for i, segment in enumerate(segments) if len(segment) < effective_min]
        
        if not small_segments:
            return segments
        
        valid_indices = [i for i in range(len(segments)) if i not in small_segments]
        if not valid_indices:
            return segments
        
        valid_centroids = centroids[valid_indices]
        kdtree = KDTree(valid_centroids)
        
        small_segments.sort(key=lambda i: len(segments[i]))
        
        for small_idx in small_segments:
            if small_idx >= len(segments):
                continue
            
            _, nearest_idx = kdtree.query([centroids[small_idx]], k=1)
            nearest_idx = valid_indices[nearest_idx[0][0]]
            
            if nearest_idx != small_idx and nearest_idx < len(segments):
                segments[nearest_idx] = np.concatenate([segments[nearest_idx], segments[small_idx]])
                segments[small_idx] = np.array([], dtype=int)
        
        return [segment for segment in segments if len(segment) > 0]
    
    def save_segments_as_bin_pkl(self, las_file: Path, las_data: laspy.LasData, segments: List[np.ndarray]):
        """
        Save segmented point clouds to bin+pkl format.
        
        Args:
            las_file: Original LAS file path
            las_data: Original LAS data
            segments: List of index arrays for segments
        """
        import time
        
        base_name = las_file.stem
        
        # å‡†å¤‡ä¿å­˜æ‰€æœ‰ç‚¹äº‘æ•°æ®åˆ°ä¸€ä¸ªbinæ–‡ä»¶
        bin_path = self.output_dir / f"{base_name}.bin"
        pkl_path = self.output_dir / f"{base_name}.pkl"
        
        print(f"  ğŸ’¾ ä¿å­˜åˆ° bin+pkl...")
        
        # 1. æ”¶é›†å­—æ®µ
        t0 = time.time()
        
        # åªä¿å­˜çœŸæ­£æœ‰æ„ä¹‰æ•°æ®çš„å­—æ®µ
        # å¿…é¡»ä¿å­˜çš„æ ¸å¿ƒå­—æ®µ
        core_fields = ['X', 'Y', 'Z']
        
        # å¯é€‰ä½†å¸¸ç”¨çš„å­—æ®µï¼ˆéœ€è¦æ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼‰
        optional_fields = ['intensity', 'return_number', 'number_of_returns', 
                          'classification', 'scan_angle_rank', 'user_data', 
                          'point_source_id', 'gps_time', 
                          'red', 'green', 'blue', 'nir']
        
        # æ„å»ºå­—æ®µåˆ—è¡¨ï¼šåªä¿å­˜å®é™…å­˜åœ¨ä¸”æœ‰æ•°æ®çš„å­—æ®µ
        fields_to_save = []
        dtype_list = []
        data_dict = {}
        
        # ä¿å­˜æ ¸å¿ƒå­—æ®µï¼ˆå¿…é¡»æœ‰ï¼‰
        for field in core_fields:
            field_lower = field.lower()
            if hasattr(las_data, field_lower):
                data = getattr(las_data, field_lower)
                fields_to_save.append(field)
                data_dict[field] = data
                dtype_list.append((field, data.dtype))
        
        # ä¿å­˜å¯é€‰å­—æ®µï¼ˆåªæœ‰å­˜åœ¨æ—¶æ‰ä¿å­˜ï¼‰
        has_classification = False
        for field in optional_fields:
            field_lower = field.lower()
            if hasattr(las_data, field_lower):
                data = getattr(las_data, field_lower)
                fields_to_save.append(field)
                data_dict[field] = data
                dtype_list.append((field, data.dtype))
                if field_lower == 'classification':
                    has_classification = True
        
        # å¦‚æœæ²¡æœ‰classificationï¼Œæ·»åŠ é»˜è®¤å€¼0ï¼ˆè¿™æ˜¯å”¯ä¸€æ·»åŠ é»˜è®¤å€¼çš„å­—æ®µï¼‰
        if not has_classification:
            fields_to_save.append('classification')
            data_dict['classification'] = np.zeros(len(las_data.points), dtype=np.uint8)
            dtype_list.append(('classification', np.uint8))
        
        # åˆ›å»ºç»“æ„åŒ–æ•°ç»„
        structured_array = np.zeros(len(las_data.points), dtype=dtype_list)
        for field in fields_to_save:
            structured_array[field] = data_dict[field]
        
        t1 = time.time()
        print(f"     - æ”¶é›†å­—æ®µ: {t1-t0:.3f}s ({len(fields_to_save)} å­—æ®µ)")
        
        # 2. ä¿å­˜ä¸ºbinæ–‡ä»¶
        t0 = time.time()
        structured_array.tofile(bin_path)
        t1 = time.time()
        bin_size_mb = bin_path.stat().st_size / (1024**2)
        print(f"     - å†™å…¥bin: {t1-t0:.3f}s ({bin_size_mb:.1f} MB)")
        
        # å‡†å¤‡pklæ–‡ä»¶çš„å…ƒæ•°æ®
        metadata = {
            'las_file': las_file.name,
            'num_points': len(las_data.points),
            'num_segments': len(segments),
            'fields': fields_to_save,
            'dtype': dtype_list,
            'window_size': self.window_size,
            'min_points': self.min_points,
            'max_points': self.max_points,
            'overlap': self.overlap,
        }
        
        # æ”¶é›†å®Œæ•´çš„LASå¤´æ–‡ä»¶ä¿¡æ¯
        header_info = {
            'version': f"{las_data.header.version.major}.{las_data.header.version.minor}",
            'point_format': las_data.header.point_format.id,
            'point_count': las_data.header.point_count,
            'x_scale': las_data.header.x_scale,
            'y_scale': las_data.header.y_scale,
            'z_scale': las_data.header.z_scale,
            'x_offset': las_data.header.x_offset,
            'y_offset': las_data.header.y_offset,
            'z_offset': las_data.header.z_offset,
            'x_min': las_data.header.x_min,
            'x_max': las_data.header.x_max,
            'y_min': las_data.header.y_min,
            'y_max': las_data.header.y_max,
            'z_min': las_data.header.z_min,
            'z_max': las_data.header.z_max,
        }
        
        # ä¿å­˜å…¶ä»–å¤´æ–‡ä»¶å±æ€§
        if hasattr(las_data.header, 'system_identifier'):
            header_info['system_identifier'] = las_data.header.system_identifier
        if hasattr(las_data.header, 'generating_software'):
            header_info['generating_software'] = las_data.header.generating_software
        if hasattr(las_data.header, 'creation_date'):
            header_info['creation_date'] = str(las_data.header.creation_date)
        if hasattr(las_data.header, 'global_encoding'):
            try:
                # global_encodingå¯èƒ½æ˜¯å¯¹è±¡ï¼Œéœ€è¦è½¬æ¢
                ge = las_data.header.global_encoding
                if hasattr(ge, 'value'):
                    header_info['global_encoding'] = int(ge.value)
                else:
                    header_info['global_encoding'] = int(ge)
            except:
                pass
        
        # ä¿å­˜åæ ‡ç³»ä¿¡æ¯ï¼ˆVLRs - Variable Length Recordsï¼‰
        vlrs_info = []
        if hasattr(las_data.header, 'vlrs'):
            for vlr in las_data.header.vlrs:
                vlr_dict = {
                    'user_id': vlr.user_id,
                    'record_id': vlr.record_id,
                    'description': vlr.description,
                }
                # ä¿å­˜VLRæ•°æ®ï¼ˆäºŒè¿›åˆ¶ï¼‰
                if hasattr(vlr, 'record_data'):
                    vlr_dict['record_data'] = bytes(vlr.record_data)
                vlrs_info.append(vlr_dict)
        header_info['vlrs'] = vlrs_info
        
        # ä¿å­˜CRSä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(las_data, 'crs'):
            try:
                header_info['crs'] = str(las_data.crs)
            except:
                header_info['crs'] = None
        
        metadata['header_info'] = header_info
        
        # ç»Ÿè®¡æ•´ä¸ªæ–‡ä»¶çš„ç±»åˆ«åˆ†å¸ƒ
        if has_classification:
            unique_labels, counts = np.unique(las_data.classification, return_counts=True)
            label_counts = {int(label): int(count) for label, count in zip(unique_labels, counts)}
        else:
            label_counts = {0: len(las_data.points)}
        metadata['label_counts'] = label_counts
        
        t1 = time.time()
        print(f"     - å‡†å¤‡metadata: {t1-t0:.3f}s")
        
        # 3. æ”¶é›†æ¯ä¸ªåˆ†å—çš„ä¿¡æ¯
        t0 = time.time()
        segments_info = []
        for i, segment_indices in enumerate(segments):
            segment_info = {
                'segment_id': i,
                'indices': segment_indices,  # åœ¨binæ–‡ä»¶ä¸­çš„ç´¢å¼•
                'num_points': len(segment_indices),
            }
            
            # ç»Ÿè®¡è¯¥åˆ†å—ä¸­çš„ç±»åˆ«ä¿¡æ¯
            if has_classification:
                segment_labels = las_data.classification[segment_indices]
                unique_segment_labels, segment_counts = np.unique(segment_labels, return_counts=True)
                segment_label_counts = {int(label): int(count) for label, count in zip(unique_segment_labels, segment_counts)}
                segment_info['unique_labels'] = [int(label) for label in unique_segment_labels]
                segment_info['label_counts'] = segment_label_counts
            else:
                segment_info['unique_labels'] = [0]
                segment_info['label_counts'] = {0: len(segment_indices)}
            
            # è®¡ç®—åˆ†å—çš„è¾¹ç•Œä¿¡æ¯ - ç›´æ¥ä»åŸå§‹las_dataè·å–
            segment_info['x_min'] = float(np.min(las_data.x[segment_indices]))
            segment_info['x_max'] = float(np.max(las_data.x[segment_indices]))
            segment_info['y_min'] = float(np.min(las_data.y[segment_indices]))
            segment_info['y_max'] = float(np.max(las_data.y[segment_indices]))
            segment_info['z_min'] = float(np.min(las_data.z[segment_indices]))
            segment_info['z_max'] = float(np.max(las_data.z[segment_indices]))
            
            segments_info.append(segment_info)
        
        metadata['segments'] = segments_info
        
        t1 = time.time()
        print(f"     - æ”¶é›†segmentsä¿¡æ¯: {t1-t0:.3f}s ({len(segments)} segments)")
        
        # 4. ä¿å­˜pklæ–‡ä»¶
        t0 = time.time()
        with open(pkl_path, 'wb') as f:
            pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)
        t1 = time.time()
        pkl_size_mb = pkl_path.stat().st_size / (1024**2)
        print(f"     - å†™å…¥pkl: {t1-t0:.3f}s ({pkl_size_mb:.1f} MB)")


def process_las_files_to_bin(input_path, output_dir=None, window_size=(50.0, 50.0), 
                              min_points=None, max_points=None,
                              overlap=False,
                              n_workers=None):
    """
    Process LAS files and save to bin+pkl format.
    å¹¶è¡Œå¤„ç†åœ¨å•ä¸ªLASæ–‡ä»¶å†…éƒ¨è¿›è¡Œï¼ˆå¤„ç†segmentsï¼‰ï¼Œè€Œä¸æ˜¯è·¨æ–‡ä»¶å¹¶è¡Œã€‚
    
    Args:
        input_path: Path to LAS file or directory containing LAS files
        output_dir: Directory to save processed files (default: same as input)
        window_size: (x_size, y_size) for rectangular windows
        min_points: Minimum points threshold for a valid segment
        max_points: Maximum points threshold before further segmentation
        overlap: Whether to use overlap mode (offset grid by half window size)
        n_workers: Number of parallel workers for segment processing (None = auto, uses CPU count - 1)
    """
    processor = LASProcessorToBin(
        input_path=input_path,
        output_dir=output_dir,
        window_size=window_size,
        min_points=min_points,
        max_points=max_points,
        overlap=overlap
    )
    processor.process_all_files(n_workers=n_workers)


# æä¾›ä¸€ä¸ªè¾…åŠ©å‡½æ•°ç”¨äºåŠ è½½æ•°æ®
def load_segment_from_bin(bin_path: Union[str, Path], 
                          pkl_path: Union[str, Path], 
                          segment_id: int) -> Tuple[np.ndarray, Dict[str, Any]]:
    """
    ä½¿ç”¨np.memmapä»binæ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šåˆ†å—çš„æ•°æ®ã€‚
    
    Args:
        bin_path: binæ–‡ä»¶è·¯å¾„
        pkl_path: pklæ–‡ä»¶è·¯å¾„
        segment_id: è¦åŠ è½½çš„åˆ†å—ID
        
    Returns:
        (segment_data, segment_info): åˆ†å—çš„ç‚¹äº‘æ•°æ®å’Œå…ƒæ•°æ®
    """
    bin_path = Path(bin_path)
    pkl_path = Path(pkl_path)
    
    # åŠ è½½å…ƒæ•°æ®
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    
    # è·å–åˆ†å—ä¿¡æ¯
    segment_info = metadata['segments'][segment_id]
    indices = segment_info['indices']
    
    # ä½¿ç”¨memmapåŠ è½½æ•°æ®
    dtype = np.dtype(metadata['dtype'])
    mmap_data = np.memmap(bin_path, dtype=dtype, mode='r')
    
    # è¯»å–æŒ‡å®šåˆ†å—çš„æ•°æ®
    segment_data = mmap_data[indices]
    
    return segment_data, segment_info


def load_all_segments_info(pkl_path: Union[str, Path]) -> List[Dict[str, Any]]:
    """
    åŠ è½½æ‰€æœ‰åˆ†å—çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ˆä¸åŠ è½½å®é™…ç‚¹äº‘æ•°æ®ï¼‰ã€‚
    
    Args:
        pkl_path: pklæ–‡ä»¶è·¯å¾„
        
    Returns:
        æ‰€æœ‰åˆ†å—çš„å…ƒæ•°æ®åˆ—è¡¨
    """
    pkl_path = Path(pkl_path)
    
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    
    return metadata['segments']


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¤„ç†LASæ–‡ä»¶
    input_path = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\train"
    output_dir = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\bin\train"
    window_size = (150.0, 150.0)
    min_points = 4096 * 2
    max_points = 4096 * 16 * 2
    overlap = False
    use_parallel = True
    max_workers = None  # è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
    
    # å¤„ç†æ–‡ä»¶ï¼ˆå¹¶è¡Œå¤„ç†åœ¨å•ä¸ªLASæ–‡ä»¶å†…éƒ¨è¿›è¡Œï¼‰
    process_las_files_to_bin(
        input_path=input_path,
        output_dir=output_dir,
        window_size=window_size,
        min_points=min_points,
        max_points=max_points,
        overlap=overlap,    # ğŸ”¥ è®¾ç½®ä¸ºTrueå¯ç”¨overlapæ¨¡å¼ï¼ˆsegmentæ•°é‡çº¦ç¿»å€ï¼‰
        n_workers=max_workers # ğŸ”¥ å¹¶è¡Œworkeræ•°ï¼ˆNone=è‡ªåŠ¨ï¼Œæ¯ä¸ªæ–‡ä»¶å†…éƒ¨å¹¶è¡Œå¤„ç†segmentsï¼‰
    )
    
    # ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½æ•°æ®
    print("\n" + "="*50)
    print("ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½åˆ†å—æ•°æ®")
    print("="*50)
    
    bin_file = Path(output_dir) / "5080_54400.bin"
    pkl_file = Path(output_dir) / "5080_54400.pkl"
    
    if bin_file.exists() and pkl_file.exists():
        # åŠ è½½æ‰€æœ‰åˆ†å—ä¿¡æ¯
        all_segments = load_all_segments_info(pkl_file)
        print(f"\næ€»å…±æœ‰ {len(all_segments)} ä¸ªåˆ†å—")
        
        # åŠ è½½ç¬¬ä¸€ä¸ªåˆ†å—çš„æ•°æ®
        if len(all_segments) > 0:
            segment_data, segment_info = load_segment_from_bin(bin_file, pkl_file, 0)
            print(f"\nç¬¬ä¸€ä¸ªåˆ†å—ä¿¡æ¯:")
            print(f"  - ç‚¹æ•°: {segment_info['num_points']}")
            print(f"  - ç±»åˆ«: {segment_info['unique_labels']}")
            print(f"  - ç±»åˆ«åˆ†å¸ƒ: {segment_info['label_counts']}")
            print(f"\nç‚¹äº‘æ•°æ®shape: {segment_data.shape}")
            print(f"å¯ç”¨å­—æ®µ: {segment_data.dtype.names}")
            print(f"\nå‰5ä¸ªç‚¹çš„xyzåæ ‡:")
            # å­—æ®µåæ˜¯å¤§å†™çš„ X, Y, Z
            for i in range(min(5, len(segment_data))):
                print(f"  Point {i}: X={segment_data['X'][i]:.2f}, Y={segment_data['Y'][i]:.2f}, Z={segment_data['Z'][i]:.2f}, class={segment_data['classification'][i]}")
