"""
H5å‹ç¼©æ–¹å¼å¯¹æ¯”æµ‹è¯•

æµ‹è¯•blosc, lz4, zstdç­‰è½»é‡çº§å‹ç¼©å¯¹å¿«é€ŸH5æ ¼å¼çš„å½±å“
é‡ç‚¹å…³æ³¨ï¼š
1. å‹ç¼©ç‡ï¼ˆæ–‡ä»¶å¤§å°ï¼‰
2. éšæœºè¯»å–é€Ÿåº¦
3. å†™å…¥é€Ÿåº¦

å…³é”®é—®é¢˜ï¼š
- gzip: å‹ç¼©å¥½ä½†æ…¢ï¼Œä¸”ä¼šç ´åcontiguous layout
- blosc/lz4/zstd: å¿«é€Ÿå‹ç¼©ï¼Œä½†éœ€è¦æ£€æŸ¥æ˜¯å¦å½±å“éšæœºè¯»å–
"""

import h5py
import numpy as np
import time
from pathlib import Path
import tempfile


def test_compression_methods():
    """æµ‹è¯•å„ç§å‹ç¼©æ–¹å¼"""
    
    print("="*70)
    print("H5å‹ç¼©æ–¹å¼å¯¹æ¯”æµ‹è¯•")
    print("="*70)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹Ÿç‚¹äº‘segmentï¼‰
    np.random.seed(42)
    num_segments = 400
    points_per_seg = 40000  # 2ä¸‡ç‚¹/segment
    
    print(f"\næµ‹è¯•æ•°æ®:")
    print(f"  Segments: {num_segments}")
    print(f"  Points/segment: {points_per_seg}")
    print(f"  æ€»ç‚¹æ•°: {num_segments * points_per_seg:,}")
    
    # ç”Ÿæˆç‚¹äº‘æ•°æ®
    segments_data = []
    for i in range(num_segments):
        seg = {
            'x': np.random.randn(points_per_seg).astype(np.float32),
            'y': np.random.randn(points_per_seg).astype(np.float32),
            'z': np.random.randn(points_per_seg).astype(np.float32),
            'intensity': np.random.randint(0, 65536, points_per_seg, dtype=np.uint16),
            'classification': np.random.randint(0, 32, points_per_seg, dtype=np.uint8),
            'return_number': np.random.randint(1, 8, points_per_seg, dtype=np.uint8),
        }
        segments_data.append(seg)
    
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)
        
        # æµ‹è¯•é…ç½®
        compression_configs = [
            # (åç§°, compression, compression_opts, chunks, è¯´æ˜)
            ("æ— å‹ç¼© (å½“å‰)", None, None, None, "Contiguous layout, æœ€å¿«"),
            ("gzip-1", "gzip", 1, True, "æœ€ä½çº§åˆ«gzip"),
            ("gzip-4", "gzip", 4, True, "å¹³è¡¡çº§åˆ«gzip"),
            ("lzf", "lzf", None, True, "HDF5å†…ç½®å¿«é€Ÿå‹ç¼©"),
        ]
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒblosc
        try:
            import hdf5plugin
            compression_configs.extend([
                ("blosc-lz4", hdf5plugin.Blosc(cname='lz4', clevel=1, shuffle=hdf5plugin.Blosc.SHUFFLE), None, True, "Blosc+LZ4å¿«é€Ÿå‹ç¼©"),
                ("blosc-zstd", hdf5plugin.Blosc(cname='zstd', clevel=1, shuffle=hdf5plugin.Blosc.SHUFFLE), None, True, "Blosc+ZSTDå¿«é€Ÿå‹ç¼©"),
            ])
            print("\nâœ… æ£€æµ‹åˆ°hdf5pluginï¼Œå°†æµ‹è¯•bloscå‹ç¼©")
        except ImportError:
            print("\nâš ï¸ æœªå®‰è£…hdf5pluginï¼Œè·³è¿‡bloscæµ‹è¯•")
            print("   å®‰è£…å‘½ä»¤: pip install hdf5plugin")
        
        results = []
        
        for config in compression_configs:
            name, compression, comp_opts, chunks, desc = config
            
            print(f"\n{'='*70}")
            print(f"æµ‹è¯•: {name}")
            print(f"è¯´æ˜: {desc}")
            print(f"{'='*70}")
            
            h5_path = tmpdir / f"test_{name.replace(' ', '_').replace('-', '_')}.h5"
            
            # === å†™å…¥æµ‹è¯• ===
            write_start = time.time()
            try:
                with h5py.File(h5_path, 'w') as f:
                    seg_group = f.create_group('segments')
                    seg_group.attrs['num_segments'] = num_segments
                    
                    for i, seg_data in enumerate(segments_data):
                        sg = seg_group.create_group(f'segment_{i:04d}')
                        
                        for field, data in seg_data.items():
                            # è®¾ç½®å­˜å‚¨å‚æ•°
                            if chunks is None:
                                # Contiguous
                                sg.create_dataset(field, data=data, chunks=None)
                            elif chunks is True:
                                # Auto chunking
                                if compression is None:
                                    sg.create_dataset(field, data=data)
                                elif isinstance(compression, str):
                                    sg.create_dataset(
                                        field, data=data,
                                        compression=compression,
                                        compression_opts=comp_opts,
                                        shuffle=True
                                    )
                                else:
                                    # hdf5plugin filter
                                    sg.create_dataset(field, data=data, **compression)
                
                write_time = time.time() - write_start
                file_size_mb = h5_path.stat().st_size / (1024 * 1024)
                
                print(f"âœ… å†™å…¥æˆåŠŸ")
                print(f"  å†™å…¥æ—¶é—´: {write_time:.2f}ç§’ ({num_segments/write_time:.1f} seg/s)")
                print(f"  æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
                
                # === éšæœºè¯»å–æµ‹è¯• ===
                # æµ‹è¯•1: æŒ‰éœ€è¯»å–ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„éšæœºè®¿é—®ï¼‰
                num_reads = 50
                read_indices = np.random.choice(num_segments, num_reads, replace=False)
                
                read_start = time.time()
                with h5py.File(h5_path, 'r') as f:
                    for idx in read_indices:
                        seg_group = f['segments'][f'segment_{idx:04d}']
                        # è¯»å–æ‰€æœ‰å­—æ®µï¼ˆæ¨¡æ‹ŸçœŸå®ä½¿ç”¨ï¼‰
                        xyz = np.stack([
                            seg_group['x'][:],
                            seg_group['y'][:],
                            seg_group['z'][:]
                        ], axis=1)
                        intensity = seg_group['intensity'][:]
                        classification = seg_group['classification'][:]
                
                read_time = time.time() - read_start
                avg_read_ms = (read_time / num_reads) * 1000
                
                print(f"  éšæœºè¯»å–: {num_reads}ä¸ªsegments")
                print(f"    æ€»æ—¶é—´: {read_time:.3f}ç§’")
                print(f"    å¹³å‡: {avg_read_ms:.2f}ms/segment")
                print(f"    é€Ÿåº¦: {num_reads/read_time:.0f} seg/s")
                
                # === é¡ºåºè¯»å–æµ‹è¯•ï¼ˆé¢„åŠ è½½åœºæ™¯ï¼‰ ===
                seq_start = time.time()
                with h5py.File(h5_path, 'r') as f:
                    all_data = []
                    for i in range(min(20, num_segments)):
                        seg_group = f['segments'][f'segment_{i:04d}']
                        xyz = np.stack([
                            seg_group['x'][:],
                            seg_group['y'][:],
                            seg_group['z'][:]
                        ], axis=1)
                        all_data.append(xyz)
                
                seq_time = time.time() - seq_start
                seq_avg_ms = (seq_time / 20) * 1000
                
                print(f"  é¡ºåºè¯»å–: å‰20ä¸ªsegments")
                print(f"    æ€»æ—¶é—´: {seq_time:.3f}ç§’")
                print(f"    å¹³å‡: {seq_avg_ms:.2f}ms/segment")
                
                # ä¿å­˜ç»“æœ
                results.append({
                    'name': name,
                    'desc': desc,
                    'write_time': write_time,
                    'file_size_mb': file_size_mb,
                    'random_read_ms': avg_read_ms,
                    'seq_read_ms': seq_avg_ms,
                    'compression_ratio': file_size_mb / results[0]['file_size_mb'] if results else 1.0
                })
                
            except Exception as e:
                print(f"âŒ å¤±è´¥: {e}")
                continue
        
        # === æ±‡æ€»å¯¹æ¯” ===
        print("\n" + "="*70)
        print("å‹ç¼©æ–¹å¼å¯¹æ¯”æ±‡æ€»")
        print("="*70)
        
        if not results:
            print("æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
            return
        
        # è¡¨å¤´
        print(f"\n{'æ–¹æ³•':<20} {'å¤§å°(MB)':<12} {'å‹ç¼©ç‡':<10} {'éšæœºè¯»(ms)':<12} {'é¡ºåºè¯»(ms)':<12} {'å†™å…¥(s)':<10}")
        print("-"*70)
        
        # æ•°æ®è¡Œ
        for r in results:
            print(f"{r['name']:<20} {r['file_size_mb']:<12.1f} {r['compression_ratio']:<10.2f} "
                  f"{r['random_read_ms']:<12.2f} {r['seq_read_ms']:<12.2f} {r['write_time']:<10.2f}")
        
        # === å»ºè®® ===
        print("\n" + "="*70)
        print("ğŸ’¡ åˆ†æä¸å»ºè®®")
        print("="*70)
        
        base = results[0]  # æ— å‹ç¼©åŸºå‡†
        
        print(f"\nåŸºå‡†ï¼ˆæ— å‹ç¼©ï¼‰:")
        print(f"  æ–‡ä»¶å¤§å°: {base['file_size_mb']:.1f} MB")
        print(f"  éšæœºè¯»å–: {base['random_read_ms']:.2f} ms/segment")
        
        # æ‰¾å‡ºæœ€ä½³å¹³è¡¡
        candidates = []
        for r in results[1:]:  # è·³è¿‡åŸºå‡†
            # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
            # å‡è®¾ï¼šæ–‡ä»¶å¤§å°é™ä½30%ä»¥ä¸Šï¼Œä¸”éšæœºè¯»å–æ…¢ä¸è¶…è¿‡2å€
            size_reduction = (1 - r['compression_ratio']) * 100
            read_slowdown = r['random_read_ms'] / base['random_read_ms']
            
            if size_reduction > 20 and read_slowdown < 3:
                candidates.append((r['name'], size_reduction, read_slowdown, r))
        
        if candidates:
            print(f"\nâœ… æ¨èçš„å‹ç¼©æ–¹å¼:")
            for name, reduction, slowdown, r in sorted(candidates, key=lambda x: -x[1]):
                print(f"\n  {name}:")
                print(f"    - æ–‡ä»¶å‡å°: {reduction:.0f}%")
                print(f"    - è¯»å–å˜æ…¢: {slowdown:.1f}x")
                print(f"    - éšæœºè¯»å–: {r['random_read_ms']:.2f}ms (åŸºå‡†: {base['random_read_ms']:.2f}ms)")
                print(f"    - æ–‡ä»¶å¤§å°: {r['file_size_mb']:.1f}MB (åŸºå‡†: {base['file_size_mb']:.1f}MB)")
        else:
            print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å‹ç¼©æ–¹å¼")
            print(f"   æ‰€æœ‰å‹ç¼©æ–¹å¼è¦ä¹ˆå‹ç¼©ç‡ä¸è¶³(<20%)ï¼Œè¦ä¹ˆè¯»å–é€Ÿåº¦æ…¢å¤ªå¤š(>3x)")
            print(f"   å»ºè®®: ä¿æŒæ— å‹ç¼©ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        print(f"\nğŸ“Š å…³é”®ç»“è®º:")
        print(f"  1. Contiguous + æ— å‹ç¼© = æœ€å¿«è¯»å– ({base['random_read_ms']:.2f}ms)")
        print(f"  2. ä»»ä½•å‹ç¼©éƒ½ä¼šå¼•å…¥chunking â†’ ç ´åcontiguous â†’ å˜æ…¢")
        print(f"  3. å³ä½¿æ˜¯LZ4/ZSTDå¿«é€Ÿå‹ç¼©ï¼Œä¹Ÿéœ€è¦è§£å‹å¼€é”€")
        print(f"  4. å¯¹äºéœ€è¦æè‡´éšæœºè¯»å–æ€§èƒ½çš„åœºæ™¯ï¼Œæ— å‹ç¼©æ˜¯æœ€ä½³é€‰æ‹©")
        print(f"  5. å¦‚æœç£ç›˜ç©ºé—´ç´§å¼ ï¼Œlzfæˆ–blosc-lz4æ˜¯è¾ƒå¥½çš„æŠ˜ä¸­æ–¹æ¡ˆ")


def estimate_real_impact(original_size_mb: float = 850.0):
    """ä¼°ç®—å®é™…å½±å“"""
    
    print("\n" + "="*70)
    print("å®é™…åœºæ™¯å½±å“ä¼°ç®—")
    print("="*70)
    
    print(f"\nå‡è®¾å•ä¸ªH5æ–‡ä»¶: {original_size_mb:.0f}MB (æ— å‹ç¼©)")
    print(f"19ä¸ªæ–‡ä»¶æ€»è®¡: {original_size_mb * 19 / 1024:.1f}GB")
    
    scenarios = [
        ("æ— å‹ç¼©", 1.0, 1.5),
        ("lzf", 0.6, 3.0),
        ("gzip-1", 0.5, 8.0),
        ("blosc-lz4", 0.65, 2.5),
        ("gzip-4", 0.4, 15.0),
    ]
    
    print(f"\n{'å‹ç¼©æ–¹å¼':<15} {'å•æ–‡ä»¶(MB)':<15} {'19æ–‡ä»¶(GB)':<15} {'è¯»å–é€Ÿåº¦':<20}")
    print("-"*70)
    
    for name, ratio, slowdown in scenarios:
        single = original_size_mb * ratio
        total = single * 19 / 1024
        speed = f"{slowdown:.1f}xæ…¢" if slowdown > 1 else "åŸºå‡†"
        print(f"{name:<15} {single:<15.0f} {total:<15.1f} {speed:<20}")
    
    print(f"\nğŸ’¡ æƒè¡¡å»ºè®®:")
    print(f"  - ç£ç›˜å……è¶³ â†’ æ— å‹ç¼© (æœ€å¿«)")
    print(f"  - ç£ç›˜ç´§å¼  + éœ€è¦æ€§èƒ½ â†’ lzf æˆ– blosc-lz4 (2-3xæ…¢ï¼Œçœ35-40%ç©ºé—´)")
    print(f"  - ç£ç›˜å¾ˆç´§å¼  + å¯æ¥å—æ…¢é€Ÿ â†’ gzip-1 (8xæ…¢ï¼Œçœ50%ç©ºé—´)")
    print(f"  - ä»…å­˜æ¡£ä¸è®­ç»ƒ â†’ gzip-4 (15xæ…¢ï¼Œçœ60%ç©ºé—´)")


if __name__ == "__main__":
    test_compression_methods()
    estimate_real_impact(850.0)
