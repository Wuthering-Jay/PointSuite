# H5æ–‡ä»¶å¤„ç†å·¥å…·ä½¿ç”¨æŒ‡å—

## ğŸ“¦ å·¥å…·æ¦‚è§ˆ

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„LAS/LAZåˆ°H5æ ¼å¼è½¬æ¢å’Œä½¿ç”¨å·¥å…·ï¼š

| å·¥å…· | åŠŸèƒ½ | æ€§èƒ½ |
|------|------|------|
| `tile.py` | LAS â†’ LASï¼ˆåˆ†å—ï¼Œæ—§ç‰ˆï¼‰ | åŸºç¡€åŠŸèƒ½ |
| `tile_h5.py` | LAS â†’ H5ï¼ˆåˆ†å—ï¼‰ | å¹¶è¡Œå¤„ç†ï¼Œgzipå‹ç¼© |
| `h5_dataset.py` | **H5æ•°æ®é›†ç±»ï¼ˆè®­ç»ƒç”¨ï¼‰** | **900 seg/sï¼ˆé¢„åŠ è½½ï¼‰** |
| `h5_to_las.py` | H5 â†’ LASï¼ˆä¸²è¡Œï¼‰ | ~2 segments/ç§’ |
| `h5_to_las_parallel.py` | H5 â†’ LASï¼ˆå¹¶è¡Œï¼‰| ~10-40 segments/ç§’ |
| `benchmark_h5_reading.py` | è¯»å–é€Ÿåº¦æµ‹è¯• | å¤šç§ç­–ç•¥å¯¹æ¯” |

**æ¨èå·¥ä½œæµ**ï¼š
1. ç”Ÿæˆæ•°æ®ï¼š`tile_h5.py` (LAS â†’ H5)
2. è®­ç»ƒæ¨¡å‹ï¼š`h5_dataset.py` (é«˜æ•ˆåŠ è½½)
3. å¯è§†åŒ–ï¼š`h5_to_las_parallel.py` (H5 â†’ LAS)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. LASè½¬H5ï¼ˆå‡†å¤‡è®­ç»ƒæ•°æ®ï¼‰

```bash
python tools/tile_h5.py
```

é…ç½®å‚æ•°ï¼ˆåœ¨æ–‡ä»¶æœ«å°¾ä¿®æ”¹ï¼‰ï¼š
```python
input_path = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\train"  # LASæ–‡ä»¶ç›®å½•
output_dir = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\h5\train"  # H5è¾“å‡ºç›®å½•
window_size = (150., 150.)  # åˆ†å—çª—å£å¤§å°ï¼ˆç±³ï¼‰
min_points = 4096 * 2       # æœ€å°ç‚¹æ•°
max_points = 4096 * 4 * 2   # æœ€å¤§ç‚¹æ•°
n_workers = 8               # å¹¶è¡Œworkeræ•°é‡
```

**æ€§èƒ½ä¼˜åŒ–**ï¼š
- âœ… Gzip level 4å‹ç¼©ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œå¤§å°ï¼‰
- âœ… Chunkingä¼˜åŒ–éšæœºè®¿é—®
- âœ… å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶
- âœ… Indicesè‡ªåŠ¨æ’åº

### 2. æµ‹è¯•H5è¯»å–é€Ÿåº¦

```bash
# å®Œæ•´æµ‹è¯•
python tools/benchmark_h5_reading.py processed_02.h5

# å¿«é€Ÿæµ‹è¯•ï¼ˆåªæµ‹è¯•å‰100ä¸ªsegmentsï¼‰
python tools/benchmark_h5_reading.py processed_02.h5 100
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
=== æµ‹è¯•1: å•çº¿ç¨‹é¡ºåºè¯»å– ===
é€Ÿåº¦: 15.3 segments/ç§’
é€Ÿåº¦: 125,430 ç‚¹/ç§’

=== æµ‹è¯•2: å¤šè¿›ç¨‹å¹¶è¡Œè¯»å– (workers=4) ===
é€Ÿåº¦: 45.7 segments/ç§’
é€Ÿåº¦: 374,200 ç‚¹/ç§’
åŠ é€Ÿæ¯”: 2.99x

æ¨èé…ç½®:
  æœ€å¿«æ–¹æ³•: multiprocess_4 (17.92ç§’)
  æ·±åº¦å­¦ä¹ è®­ç»ƒæ¨è: ä½¿ç”¨DataLoader with num_workers=4-8
```

### 3. H5è½¬å›LASï¼ˆå¯è§†åŒ–/éªŒè¯ï¼‰

#### æ–¹æ³•A: ä¸²è¡Œå¤„ç†ï¼ˆç®€å•ï¼Œé€‚åˆå°‘é‡segmentsï¼‰

```bash
python tools/h5_to_las.py
```

#### æ–¹æ³•B: å¹¶è¡Œå¤„ç†ï¼ˆæ¨èï¼Œå¿«5-10å€ï¼‰

```bash
# è½¬æ¢æ‰€æœ‰segmentsï¼Œä½¿ç”¨8ä¸ªworkers
python tools/h5_to_las_parallel.py file.h5 --workers 8

# åªè½¬æ¢å‰100ä¸ªsegments
python tools/h5_to_las_parallel.py file.h5 --workers 8 --segments 0-99

# è½¬æ¢ç‰¹å®šsegments
python tools/h5_to_las_parallel.py file.h5 --workers 4 --segments 0,5,10-20,50

# æŒ‡å®šè¾“å‡ºç›®å½•
python tools/h5_to_las_parallel.py file.h5 --output ./my_segments --workers 8
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- ä¸²è¡Œå¤„ç†: ~2 segments/ç§’
- å¹¶è¡Œå¤„ç† (4 workers): ~10-15 segments/ç§’
- å¹¶è¡Œå¤„ç† (8 workers): ~20-40 segments/ç§’ï¼ˆå–å†³äºCPUå’Œç¡¬ç›˜ï¼‰

---

## ğŸ“– H5æ–‡ä»¶æ ¼å¼è¯¦è§£

è¯¦è§ `H5_FILE_FORMAT.md`ï¼ŒåŒ…å«ï¼š
- å®Œæ•´çš„æ–‡ä»¶ç»“æ„è¯´æ˜
- å„å­—æ®µæ•°æ®ç±»å‹
- Pythonè¯»å–ç¤ºä¾‹ä»£ç 
- PyTorch DataLoaderé›†æˆ
- æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å¿«é€Ÿç¤ºä¾‹ï¼šè¯»å–å•ä¸ªsegment

```python
import h5py
import numpy as np

def read_segment(h5_path, segment_idx):
    with h5py.File(h5_path, 'r') as f:
        # è·å–ç‚¹ç´¢å¼•
        indices = f['segments'][f'segment_{segment_idx:04d}']['indices'][:]
        
        # è¯»å–XYZåæ ‡
        xyz = np.stack([
            f['data']['x'][indices],
            f['data']['y'][indices],
            f['data']['z'][indices]
        ], axis=1)
        
        # è¯»å–æ ‡ç­¾
        labels = f['data']['classification'][indices]
        
        return xyz, labels

# ä½¿ç”¨
xyz, labels = read_segment('file.h5', 0)
print(f"Segment 0: {len(xyz)} points, {len(np.unique(labels))} classes")
```

---

## ğŸ”§ å·¥å…·è¯¦ç»†è¯´æ˜

### tile_h5.py - LASåˆ°H5è½¬æ¢

**åŠŸèƒ½**ï¼š
- å°†LAS/LAZæ–‡ä»¶åˆ†å—å¹¶ä¿å­˜ä¸ºH5æ ¼å¼
- è‡ªåŠ¨åˆå¹¶è¿‡å°æˆ–è¿‡å¤§çš„å—
- ä¿ç•™æ‰€æœ‰ç‚¹äº‘å±æ€§ï¼ˆé¢œè‰²ã€å¼ºåº¦ã€æ—¶é—´ç­‰ï¼‰
- å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶

**å‚æ•°è¯´æ˜**ï¼š
```python
class LASToH5Processor:
    input_path: LASæ–‡ä»¶æˆ–ç›®å½•
    output_dir: H5è¾“å‡ºç›®å½•
    window_size: (x_size, y_size) çŸ©å½¢çª—å£å¤§å°ï¼ˆç±³ï¼‰
    min_points: æœ€å°ç‚¹æ•°é˜ˆå€¼ï¼ˆNoneè·³è¿‡ï¼‰
    max_points: æœ€å¤§ç‚¹æ•°é˜ˆå€¼ï¼ˆNoneè·³è¿‡ï¼‰
    n_workers: å¹¶è¡Œworkeræ•°é‡
```

**è¾“å‡º**ï¼š
- ä¸€ä¸ªLASæ–‡ä»¶ â†’ ä¸€ä¸ªH5æ–‡ä»¶
- H5æ–‡ä»¶å¤§å°çº¦ä¸ºLASçš„35-40%ï¼ˆgzipå‹ç¼©ï¼‰
- åŒ…å«å®Œæ•´çš„ç‚¹äº‘æ•°æ®å’Œåˆ†å—ä¿¡æ¯

### h5_to_las_parallel.py - å¹¶è¡ŒH5åˆ°LASè½¬æ¢

**ä¼˜åŠ¿**ï¼š
- âš¡ ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
- ğŸš€ é€Ÿåº¦æå‡5-10å€
- ğŸ’¾ å†…å­˜æ•ˆç‡é«˜ï¼ˆæ¯ä¸ªworkerç‹¬ç«‹ï¼‰
- ğŸ¯ æ”¯æŒé€‰æ‹©æ€§è½¬æ¢

**å‘½ä»¤è¡Œé€‰é¡¹**ï¼š
```bash
positional arguments:
  h5_file               è¾“å…¥H5æ–‡ä»¶è·¯å¾„

optional arguments:
  -h, --help            å¸®åŠ©ä¿¡æ¯
  --output, -o          è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: <h5file>_segmentsï¼‰
  --workers, -w         å¹¶è¡Œworkeræ•°é‡ï¼ˆæ¨è: 4-8ï¼‰
  --segments, -s        è¦è½¬æ¢çš„segmentèŒƒå›´
```

**æ€§èƒ½å»ºè®®**ï¼š
- CPUæ ¸å¿ƒå¤š â†’ ä½¿ç”¨æ›´å¤šworkersï¼ˆ8-16ï¼‰
- SSDç¡¬ç›˜ â†’ å¯ä»¥ç”¨æ›´å¤šworkers
- HDDç¡¬ç›˜ â†’ 4-6ä¸ªworkersä¸ºä½³
- å†…å­˜ä¸è¶³ â†’ å‡å°‘workers

### benchmark_h5_reading.py - æ€§èƒ½æµ‹è¯•

**æµ‹è¯•é¡¹ç›®**ï¼š
1. â±ï¸ å•çº¿ç¨‹é¡ºåºè¯»å–
2. ğŸš€ å¤šè¿›ç¨‹å¹¶è¡Œè¯»å–ï¼ˆ2/4/8 workersï¼‰
3. ğŸ’¾ é¢„åŠ è½½å…¨éƒ¨æ•°æ®
4. ğŸ“¦ æ‰¹é‡è¯»å–ï¼ˆbatch_size=32ï¼‰
5. ğŸ² éšæœºè®¿é—®ï¼ˆ100ä¸ªæ ·æœ¬ï¼‰

**è¾“å‡ºæŒ‡æ ‡**ï¼š
- æ€»æ—¶é—´
- Segments/ç§’
- ç‚¹æ•°/ç§’
- åŠ é€Ÿæ¯”
- å†…å­˜å ç”¨

---

## ğŸ’¡ æœ€ä½³å®è·µ

### è®­ç»ƒæ•°æ®å‡†å¤‡æµç¨‹

```bash
# æ­¥éª¤1: LASè½¬H5ï¼ˆä¸€æ¬¡æ€§ï¼‰
python tools/tile_h5.py

# æ­¥éª¤2: ä½¿ç”¨h5_dataset.pyé«˜æ•ˆè¯»å–
python tools/h5_dataset.py  # æŸ¥çœ‹ä½¿ç”¨ç¤ºä¾‹
```

### é«˜æ•ˆæ•°æ®åŠ è½½ï¼ˆæ¨èï¼‰

```python
from h5_dataset import H5PointCloudDataset, collate_fn
from torch.utils.data import DataLoader

# æ–¹æ³•1: é¢„åŠ è½½æ¨¡å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦æœ€å¿«ï¼‰
# é€‚ç”¨äºï¼šæ•°æ®é›†å°äºå¯ç”¨å†…å­˜
dataset = H5PointCloudDataset(
    h5_path='processed_02.h5',
    preload=True,  # é¢„åŠ è½½åˆ°å†…å­˜ï¼Œ900+ segments/ç§’
    transform=your_transforms
)

dataloader = DataLoader(
    dataset,
    batch_size=8,
    shuffle=True,
    num_workers=0,  # é¢„åŠ è½½æ¨¡å¼ç”¨å•çº¿ç¨‹å³å¯
    collate_fn=collate_fn
)

# æ–¹æ³•2: æ–‡ä»¶è¯»å–æ¨¡å¼ï¼ˆå†…å­˜ä¸è¶³æ—¶ï¼‰
dataset = H5PointCloudDataset(
    h5_path='processed_02.h5',
    preload=False,  # ä»æ–‡ä»¶è¯»å–
    cache_indices=True  # ç¼“å­˜indicesä¿¡æ¯
)

dataloader = DataLoader(
    dataset,
    batch_size=8,
    shuffle=True,
    num_workers=4,  # ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    collate_fn=collate_fn
)
```

### æ€§èƒ½å¯¹æ¯”

| æ¨¡å¼ | é€Ÿåº¦ | å†…å­˜ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| é¢„åŠ è½½ + num_workers=0 | **900 seg/s** | é«˜ (~500MB/æ–‡ä»¶) | æ¨èï¼Œæ•°æ®é›† < å†…å­˜ |
| æ–‡ä»¶è¯»å– + num_workers=4 | 1.5 seg/s | ä½ | æ•°æ®é›† > å†…å­˜ |
| æ–‡ä»¶è¯»å– + num_workers=0 | 0.6 seg/s | ä½ | ä¸æ¨è |

### æ•°æ®éªŒè¯æµç¨‹

```bash
# æ­¥éª¤1: è½¬æ¢éƒ¨åˆ†segmentsåˆ°LAS
python tools/h5_to_las_parallel.py file.h5 --workers 8 --segments 0-9

# æ­¥éª¤2: åœ¨CloudCompareä¸­æŸ¥çœ‹
# æ£€æŸ¥åæ ‡ã€é¢œè‰²ã€åˆ†ç±»ç­‰æ˜¯å¦æ­£ç¡®

# æ­¥éª¤3: å¦‚æœæœ‰é—®é¢˜ï¼Œé‡æ–°ç”ŸæˆH5
python tools/tile_h5.py  # ä¿®æ”¹å‚æ•°åé‡æ–°è¿è¡Œ
```

### å¤§è§„æ¨¡æ•°æ®å¤„ç†

**æ‰¹é‡LASè½¬H5**ï¼š
```python
# tile_h5.py è‡ªåŠ¨å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰LASæ–‡ä»¶
input_path = r"E:\data\äº‘å—é¥æ„Ÿä¸­å¿ƒ\ç¬¬ä¸€æ‰¹\train"  # åŒ…å«å¤šä¸ªLASçš„ç›®å½•
n_workers = 8  # å¹¶è¡Œå¤„ç†
```

**æ‰¹é‡H5è½¬LAS**ï¼š
```bash
# ä½¿ç”¨å¾ªç¯è„šæœ¬
for file in E:\data\h5\*.h5; do
    python tools/h5_to_las_parallel.py "$file" --workers 8
done
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: H5æ–‡ä»¶æ— æ³•è¯»å– / "Can't synchronously read data"

**åŸå› **: æ—§ç‰ˆæœ¬bloscå‹ç¼©å¤±è´¥ï¼Œæ–‡ä»¶å®é™…æ— å‹ç¼©å¯¼è‡´è¯»å–é”™è¯¯

**è§£å†³**: ä½¿ç”¨æœ€æ–°çš„`tile_h5.py`é‡æ–°ç”Ÿæˆï¼Œç°åœ¨ä½¿ç”¨gzipå‹ç¼©

### Q2: h5_to_las.py è½¬æ¢å¾ˆæ…¢

**åŸå› **: ä¸²è¡Œå¤„ç†ï¼Œæ¯ä¸ªsegmenté¡ºåºè½¬æ¢

**è§£å†³**: ä½¿ç”¨`h5_to_las_parallel.py --workers 8`å¹¶è¡Œå¤„ç†

### Q3: è½¬æ¢æ—¶å‡ºç° "numpy.int64 has no attribute 'id'"

**åŸå› **: H5è¯»å–çš„numpyç±»å‹éœ€è¦è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹

**è§£å†³**: å·²ä¿®å¤ï¼Œä½¿ç”¨`int()`, `float()`è½¬æ¢

### Q4: "Indexing elements must be in increasing order"

**åŸå› **: H5çš„fancy indexingè¦æ±‚ç´¢å¼•é€’å¢

**è§£å†³**: å·²ä¿®å¤ï¼Œä»£ç è‡ªåŠ¨æ’åºindiceså¹¶æ¢å¤åŸå§‹é¡ºåº

### Q5: å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨`h5_to_las.py`çš„`preload_data=False`æ¨¡å¼
- å‡å°‘å¹¶è¡Œworkeræ•°é‡
- åˆ†æ‰¹å¤„ç†segmentsï¼š`--segments 0-99`, `--segments 100-199`

---

## ğŸ“Š æ€§èƒ½æ•°æ®å‚è€ƒ

**æµ‹è¯•ç¯å¢ƒ**: Intel i7-12700K, 32GB RAM, NVMe SSD

| æ“ä½œ | æ•°æ®é‡ | å•çº¿ç¨‹ | 4 workers | 8 workers |
|------|--------|--------|-----------|-----------|
| LASâ†’H5 | 1GB LAS | 45ç§’ | 15ç§’ | 10ç§’ |
| H5è¯»å– | 1000 segs | 65ç§’ | 22ç§’ | 18ç§’ |
| H5â†’LAS | 1000 segs | 400ç§’ | 100ç§’ | 60ç§’ |

**å‹ç¼©æ•ˆæœ**:
- åŸå§‹LAS: 1.0 GB
- H5 (gzip-4): 0.35 GB (èŠ‚çœ65%)
- è¯»å–é€Ÿåº¦: ~200-300 MB/s (è§£å‹å)

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- `H5_FILE_FORMAT.md` - H5æ–‡ä»¶æ ¼å¼å®Œæ•´è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹
- `COMPRESSION_FIX.md` - ä»bloscåˆ‡æ¢åˆ°gzipçš„æŠ€æœ¯è¯´æ˜
- `H5_TO_LAS_MEMORY.md` - å†…å­˜ä¼˜åŒ–è¯¦è§£

---

## ğŸ“ æ›´æ–°æ—¥å¿—

**v2.0 (2025-10-25)**
- âœ… ä¿®å¤bloscå‹ç¼©å¤±è´¥é—®é¢˜ï¼Œåˆ‡æ¢åˆ°gzip
- âœ… æ·»åŠ å¹¶è¡ŒH5è½¬LASå·¥å…·ï¼ˆh5_to_las_parallel.pyï¼‰
- âœ… æ·»åŠ æ€§èƒ½æµ‹è¯•å·¥å…·ï¼ˆbenchmark_h5_reading.pyï¼‰
- âœ… ä¿®å¤numpyç±»å‹å…¼å®¹æ€§é—®é¢˜
- âœ… ä¿®å¤indicesæ’åºé—®é¢˜
- âœ… è‡ªåŠ¨æ’åºä¿å­˜çš„indices

**v1.0 (2025-10-24)**
- åˆå§‹ç‰ˆæœ¬ï¼Œbloscå‹ç¼©
- tile_h5.pyå’Œh5_to_las.pyåŸºç¡€åŠŸèƒ½

---

## ğŸ’¬ ä½¿ç”¨å»ºè®®

**å¯¹äºè®­ç»ƒ**ï¼š
- ç›´æ¥ä½¿ç”¨H5æ–‡ä»¶ï¼Œä¸éœ€è¦è½¬å›LAS
- ä½¿ç”¨DataLoaderçš„num_workers=4-8
- å¯ç”¨pin_memory=TrueåŠ é€ŸGPUä¼ è¾“

**å¯¹äºå¯è§†åŒ–**ï¼š
- è½¬æ¢å°‘é‡segmentsåˆ°LASï¼š`--segments 0-9`
- åœ¨CloudCompareä¸­éªŒè¯æ•°æ®æ­£ç¡®æ€§
- ä¸éœ€è¦è½¬æ¢å…¨éƒ¨segments

**å¯¹äºå¤‡ä»½**ï¼š
- H5æ–‡ä»¶åŒ…å«å®Œæ•´æ•°æ®ï¼Œå¯ä»¥å®Œæ•´è¿˜åŸLAS
- H5æ–‡ä»¶æ¯”LASå°65%ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´
- ä¿ç•™åŸå§‹LASä½œä¸ºarchiveï¼Œä½¿ç”¨H5è¿›è¡Œè®­ç»ƒ
