# å¿«é€ŸH5è¯»å– - é€ŸæŸ¥è¡¨

## ğŸ“– åŸºæœ¬è¯»å–

### 1. è¯»å–Headerä¿¡æ¯
```python
import h5py

with h5py.File('file.h5', 'r') as f:
    header = f['header']
    
    # åŸºæœ¬ä¿¡æ¯
    num_points = header.attrs['num_points']
    num_segments = f['segments'].attrs['num_segments']
    
    # åæ ‡å‚æ•°
    x_scale = header.attrs['x_scale']
    x_offset = header.attrs['x_offset']
    
    # å¯ç”¨å­—æ®µ
    fields = header.attrs['available_fields'].split(',')
```

### 2. è¯»å–å•ä¸ªSegment
```python
with h5py.File('file.h5', 'r') as f:
    seg = f['segments']['segment_0000']
    
    # è¯»å–åæ ‡
    x = seg['x'][:]
    y = seg['y'][:]
    z = seg['z'][:]
    xyz = np.stack([x, y, z], axis=1)  # Shape: (N, 3)
    
    # è¯»å–åˆ†ç±»
    labels = seg['classification'][:]
    
    # è¯»å–å…¶ä»–å­—æ®µ
    intensity = seg['intensity'][:]
    gps_time = seg['gps_time'][:]
```

### 3. æ‰¹é‡è¯»å–Segments
```python
# âš ï¸ ä¿æŒæ–‡ä»¶æ‰“å¼€ä»¥æå‡æ€§èƒ½
with h5py.File('file.h5', 'r') as f:
    data = []
    for i in range(10):
        seg = f['segments'][f'segment_{i:04d}']
        xyz = np.stack([seg['x'][:], seg['y'][:], seg['z'][:]], axis=1)
        labels = seg['classification'][:]
        data.append((xyz, labels))
```

## ğŸš€ è®­ç»ƒä½¿ç”¨ï¼ˆæ¨èï¼‰

### å•æ–‡ä»¶è®­ç»ƒ
```python
from tools.h5_dataset_fast import FastH5Dataset, collate_fn
from torch.utils.data import DataLoader

# åˆ›å»ºDatasetï¼ˆé¢„åŠ è½½åˆ°å†…å­˜ï¼‰
dataset = FastH5Dataset(
    'file.h5',
    preload=True  # é€Ÿåº¦ï¼š5000+ seg/s
)

# åˆ›å»ºDataLoader
dataloader = DataLoader(
    dataset,
    batch_size=16,
    shuffle=True,
    num_workers=0,  # âš ï¸ é¢„åŠ è½½æ—¶å¿…é¡»ç”¨0
    collate_fn=collate_fn
)

# è®­ç»ƒå¾ªç¯
for batch_xyz, batch_labels in dataloader:
    # batch_xyz: List[Tensor[N_i, 3]]
    # batch_labels: List[Tensor[N_i]]
    
    for xyz, labels in zip(batch_xyz, batch_labels):
        # xyz: Tensor[N, 3]
        # labels: Tensor[N]
        pass
```

### å¤šæ–‡ä»¶è®­ç»ƒ
```python
from tools.h5_dataset_fast import FastMultiH5Dataset, collate_fn

# æŸ¥æ‰¾æ‰€æœ‰H5æ–‡ä»¶
h5_files = sorted(Path('h5_dir').glob('*.h5'))

# åˆ›å»ºDataset
dataset = FastMultiH5Dataset(
    [str(f) for f in h5_files],
    preload_strategy="all"  # æˆ– "none" æˆ– "first-10"
)

# DataLoaderé…ç½®ç›¸åŒ
dataloader = DataLoader(dataset, batch_size=16, num_workers=0, collate_fn=collate_fn)
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### ä¸åŒè¯»å–æ¨¡å¼
| æ¨¡å¼ | ä»£ç  | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| åå¤æ‰“å¼€ | æ¯æ¬¡`with h5py.File` | 580 seg/s | âŒ ä¸æ¨è |
| ä¿æŒæ‰“å¼€ | å¤–å±‚`with h5py.File` | 700 seg/s | æ¨ç† |
| é¢„åŠ è½½ | `FastH5Dataset(preload=True)` | **5000+ seg/s** | âœ… è®­ç»ƒ |

### num_workersè®¾ç½®
| preload | num_workers | é€Ÿåº¦ | è¯´æ˜ |
|---------|------------|------|------|
| True | 0 | **5000+ seg/s** | âœ… æœ€å¿« |
| True | 4 | 100 seg/s | âŒ åºåˆ—åŒ–å¼€é”€ |
| False | 0 | 650 seg/s | å•è¿›ç¨‹I/O |
| False | 4 | 700 seg/s | å¹¶è¡ŒI/Oï¼ˆæå‡å°ï¼‰ |

**ç»“è®º**ï¼šé¢„åŠ è½½æ—¶**å¿…é¡»**ç”¨`num_workers=0`ï¼

## ğŸ” å¸¸ç”¨æ“ä½œ

### è·å–Segmentä¿¡æ¯
```python
with h5py.File('file.h5', 'r') as f:
    seg = f['segments']['segment_0000']
    
    # ç‚¹æ•°
    num_points = len(seg['x'])
    # æˆ–
    num_points = seg.attrs['num_points']
    
    # å­—æ®µåˆ—è¡¨
    fields = list(seg.keys())
```

### ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
```python
with h5py.File('file.h5', 'r') as f:
    all_labels = []
    for i in range(10):  # å‰10ä¸ªsegments
        labels = f['segments'][f'segment_{i:04d}']['classification'][:]
        all_labels.append(labels)
    
    all_labels = np.concatenate(all_labels)
    unique, counts = np.unique(all_labels, return_counts=True)
    
    for label, count in zip(unique, counts):
        print(f"ç±»åˆ« {label}: {count} ç‚¹")
```

### ç­›é€‰ç‰¹å®šç±»åˆ«çš„ç‚¹
```python
with h5py.File('file.h5', 'r') as f:
    seg = f['segments']['segment_0000']
    
    xyz = np.stack([seg['x'][:], seg['y'][:], seg['z'][:]], axis=1)
    labels = seg['classification'][:]
    
    # åªä¿ç•™ç±»åˆ«1å’Œ2
    mask = np.isin(labels, [1, 2])
    xyz_filtered = xyz[mask]
    labels_filtered = labels[mask]
```

### æŸ¥æ‰¾å¤§Segments
```python
with h5py.File('file.h5', 'r') as f:
    num_segs = f['segments'].attrs['num_segments']
    
    large_segments = []
    for i in range(num_segs):
        seg = f['segments'][f'segment_{i:04d}']
        if len(seg['x']) > 100000:
            large_segments.append(i)
    
    print(f"å¤§segments (>100kç‚¹): {large_segments}")
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å†…å­˜ç®¡ç†
```python
# âœ… å¥½ï¼š19ä¸ªæ–‡ä»¶çº¦10GBï¼Œ64GB RAMå¯å…¨é¢„åŠ è½½
dataset = FastMultiH5Dataset(files, preload_strategy="all")

# âœ… å¥½ï¼šå†…å­˜æœ‰é™æ—¶æŒ‰éœ€åŠ è½½ï¼ˆä»æœ‰650 seg/sï¼‰
dataset = FastMultiH5Dataset(files, preload_strategy="none")

# âœ… å¥½ï¼šæŠ˜ä¸­æ–¹æ¡ˆ
dataset = FastMultiH5Dataset(files, preload_strategy="first-10")
```

### 2. å­—æ®µå¤§å°å†™
```python
# H5ä¸­å­—æ®µåæ˜¯å°å†™
with h5py.File('file.h5', 'r') as f:
    seg = f['segments']['segment_0000']
    x = seg['x'][:]  # âœ… å°å†™
    # X = seg['X'][:]  # âŒ ä¼šæŠ¥é”™
```

### 3. æ•°æ®ç±»å‹
```python
# åæ ‡å’ŒGPSæ—¶é—´æ˜¯float64
xyz = seg['x'][:]  # dtype: float64

# åˆ†ç±»æ˜¯int32
labels = seg['classification'][:]  # dtype: int32

# å¼ºåº¦ã€é¢œè‰²æ˜¯uint16
intensity = seg['intensity'][:]  # dtype: uint16
```

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### æœ€å°è®­ç»ƒä»£ç 
```python
from tools.h5_dataset_fast import FastMultiH5Dataset, collate_fn
from torch.utils.data import DataLoader
from pathlib import Path

# æ•°æ®å‡†å¤‡
h5_files = sorted(Path('h5_dir').glob('*.h5'))
dataset = FastMultiH5Dataset(
    [str(f) for f in h5_files],
    preload_strategy="all"
)
dataloader = DataLoader(dataset, batch_size=16, num_workers=0, collate_fn=collate_fn)

# è®­ç»ƒ
for epoch in range(10):
    for batch_xyz, batch_labels in dataloader:
        # ä½ çš„è®­ç»ƒä»£ç 
        for xyz, labels in zip(batch_xyz, batch_labels):
            # xyz: Tensor[N, 3]
            # labels: Tensor[N]
            
            # Forward
            output = model(xyz)
            loss = criterion(output, labels)
            
            # Backward
            loss.backward()
            optimizer.step()
```

### æœ€å°æ¨ç†ä»£ç 
```python
import h5py
import numpy as np

with h5py.File('file.h5', 'r') as f:
    num_segs = f['segments'].attrs['num_segments']
    
    for i in range(num_segs):
        seg = f['segments'][f'segment_{i:04d}']
        xyz = np.stack([seg['x'][:], seg['y'][:], seg['z'][:]], axis=1)
        
        # æ¨ç†
        pred = model.predict(xyz)
```

## ğŸ”— ç›¸å…³æ–‡ä»¶

- **å®Œæ•´ç¤ºä¾‹**: `example_h5_fast_reading.py`
- **Datasetç±»**: `h5_dataset_fast.py`
- **æ ¼å¼å¯¹æ¯”**: `H5_FORMAT_COMPARISON.md`
- **å·¥å…·æ€»è§ˆ**: `README.md`
