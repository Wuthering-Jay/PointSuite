# åŠ¨æ€ Batch æ–¹æ³•çš„ DDP å…¼å®¹æ€§åˆ†æ

## ğŸ“‹ é—®é¢˜æ¦‚è¿°

**ç”¨æˆ·é—®é¢˜**ï¼š
1. åŠ¨æ€batchæ–¹æ³•èƒ½å…¼å®¹ddpå—ï¼Ÿ
2. èƒ½æ”¯æŒä¸åŒæ˜¾å¡batchæ•°ä¸åŒå—ï¼Ÿ

**ç®€çŸ­å›ç­”**ï¼š
- âœ… **DynamicBatchSampler å®Œå…¨å…¼å®¹ DDP**
- âœ… **ä¸åŒæ˜¾å¡å¯ä»¥æœ‰ä¸åŒçš„batchæ•°**
- âš ï¸ **LimitedPointsCollateFn éœ€è¦è°¨æ…ä½¿ç”¨**

---

## ğŸ” æ·±åº¦åˆ†æ

### 1. ä¸¤ç§åŠ¨æ€ Batch å®ç°æ–¹å¼

ä»£ç ä¸­æä¾›äº†ä¸¤ç§é™åˆ¶ç‚¹æ•°çš„æ–¹æ³•ï¼š

#### æ–¹æ³•ä¸€ï¼š`DynamicBatchSampler`ï¼ˆâœ… æ¨èï¼ŒDDP å®Œå…¨å…¼å®¹ï¼‰

**ä½ç½®**ï¼š`pointsuite/data/datasets/collate.py` ç¬¬219è¡Œ

**å·¥ä½œåŸç†**ï¼š
```python
class DynamicBatchSampler:
    """åœ¨é‡‡æ ·é˜¶æ®µå°±æ§åˆ¶ batch å¤§å°"""
    def __iter__(self):
        # éå†æ‰€æœ‰ç´¢å¼•
        for idx in indices:
            # æ ¹æ®ç‚¹æ•°åŠ¨æ€å†³å®šä½•æ—¶ yield ä¸€ä¸ª batch
            if batch_points + num_points <= self.max_points:
                batch.append(idx)  # ç»§ç»­æ·»åŠ 
            else:
                yield batch  # å½“å‰batchå·²æ»¡ï¼Œyield
                batch = [idx]  # å¼€å§‹æ–°batch
```

**DDP å…¼å®¹æ€§åˆ†æ**ï¼š

âœ… **ä¼˜åŠ¿**ï¼š
1. **æ¯ä¸ª GPU ç‹¬ç«‹å†³å®š batch å¤§å°**
   - GPU 0 å¯èƒ½äº§ç”Ÿ [3, 2, 4, 3] ä¸ªæ ·æœ¬çš„batches
   - GPU 1 å¯èƒ½äº§ç”Ÿ [2, 3, 3, 2] ä¸ªæ ·æœ¬çš„batches
   - **è¿™æ˜¯å®Œå…¨æ­£å¸¸çš„ï¼**

2. **PyTorch Lightning è‡ªåŠ¨å¤„ç†**
   ```python
   # Lightning å†…éƒ¨ä¼šè‡ªåŠ¨ä¸º DDP è®¾ç½® DistributedSampler
   # æ¯ä¸ª GPU è·å¾—ä¸åŒçš„æ ·æœ¬å­é›†
   # å³ä½¿ batch æ•°é‡ä¸åŒä¹Ÿä¸ä¼šæ­»é”
   ```

3. **æŒ‡æ ‡èšåˆæ­£ç¡®**
   ```python
   # torchmetrics ä¼šæ­£ç¡®å¤„ç†ä¸åŒ batch æ•°é‡
   # æ¯ä¸ª GPU ç´¯ç§¯è‡ªå·±çš„æ··æ·†çŸ©é˜µ
   # åœ¨ compute() æ—¶é€šè¿‡ all_gather èšåˆ
   ```

4. **ä¸å½±å“è®­ç»ƒåŒæ­¥**
   - DDP åŒæ­¥å‘ç”Ÿåœ¨ `backward()` æ—¶
   - ä¸ batch æ•°é‡æ— å…³ï¼Œåªä¸ iteration æ•°æœ‰å…³
   - åªè¦æ‰€æœ‰ GPU å®Œæˆè‡ªå·±çš„ iterationsï¼Œå°±å¯ä»¥è¿›å…¥ä¸‹ä¸€ä¸ª epoch

**ä»£ç ä¸­çš„å®é™…ä½¿ç”¨**ï¼š
```python
# pointsuite/data/datamodule_base.py ç¬¬240è¡Œ
batch_sampler = DynamicBatchSampler(
    dataset=dataset,
    max_points=self.max_points,
    shuffle=(shuffle and base_sampler is None),
    drop_last=drop_last,
    sampler=base_sampler  # å¯ä¸ WeightedRandomSampler ç»“åˆ
)

dataloader = DataLoader(
    dataset,
    batch_sampler=batch_sampler,  # âœ… å…³é”®ï¼šä½¿ç”¨ batch_sampler
    num_workers=self.num_workers,
    collate_fn=self.collate_fn,
    pin_memory=self.pin_memory,
)
```

---

#### æ–¹æ³•äºŒï¼š`LimitedPointsCollateFn`ï¼ˆâš ï¸ éœ€è¦è°¨æ…ï¼Œæœ‰æ½œåœ¨é—®é¢˜ï¼‰

**ä½ç½®**ï¼š`pointsuite/data/datasets/collate.py` ç¬¬105è¡Œ

**å·¥ä½œåŸç†**ï¼š
```python
class LimitedPointsCollateFn:
    """åœ¨ collate é˜¶æ®µä¸¢å¼ƒæ ·æœ¬"""
    def __call__(self, batch):
        # å…ˆç”± sampler ç”Ÿæˆå›ºå®šå¤§å°çš„ batchï¼ˆå¦‚ batch_size=4ï¼‰
        # è®¡ç®—æ€»ç‚¹æ•°
        total_points = sum(len(sample['coord']) for sample in batch)
        
        # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œä¸¢å¼ƒæ ·æœ¬
        if total_points > self.max_points:
            if self.strategy == 'drop_largest':
                # æŒ‰å¤§å°æ’åºï¼Œä¿ç•™æœ€å°çš„
                batch = self._drop_largest(batch)
            elif self.strategy == 'drop_last':
                # ä¸¢å¼ƒæœ«å°¾çš„
                batch = batch[:n]
            # ...
        
        return collate_fn(batch)
```

**DDP é—®é¢˜åˆ†æ**ï¼š

âš ï¸ **æ½œåœ¨é£é™©**ï¼š
1. **ä¸åŒ GPU å¯èƒ½ä¸¢å¼ƒä¸åŒæ•°é‡çš„æ ·æœ¬**
   - GPU 0 æ¥æ”¶åˆ° [large, medium, medium, small] â†’ ä¸¢å¼ƒ [large] â†’ è¿”å› 3 ä¸ªæ ·æœ¬
   - GPU 1 æ¥æ”¶åˆ° [small, small, small, small] â†’ ä¸ä¸¢å¼ƒ â†’ è¿”å› 4 ä¸ªæ ·æœ¬
   - **ç»“æœï¼šGPU ä¹‹é—´çš„ batch æ•°é‡ä¸åŒ**

2. **å¯èƒ½å¯¼è‡´çš„é—®é¢˜**
   ```python
   # å‡è®¾ epoch æœ‰ 100 ä¸ªåŸå§‹ batches
   # DistributedSampler ç»™æ¯ä¸ª GPU åˆ†é… 50 ä¸ª batches
   
   # GPU 0 å¤„ç† 50 ä¸ª batchesï¼ˆéƒ¨åˆ†è¢«ä¸¢å¼ƒæ ·æœ¬åä»æ˜¯ 50 ä¸ªï¼‰
   # GPU 1 å¤„ç† 50 ä¸ª batchesï¼ˆéƒ¨åˆ†è¢«ä¸¢å¼ƒæ ·æœ¬åä»æ˜¯ 50 ä¸ªï¼‰
   # âœ… batch æ•°é‡ç›¸åŒï¼Œä¸ä¼šæ­»é”
   
   # ä½†æ˜¯ï¼š
   # - GPU 0 å®é™…å¤„ç†äº† 120 ä¸ªæ ·æœ¬ï¼ˆå¹³å‡æ¯ batch 2.4 ä¸ªï¼‰
   # - GPU 1 å®é™…å¤„ç†äº† 180 ä¸ªæ ·æœ¬ï¼ˆå¹³å‡æ¯ batch 3.6 ä¸ªï¼‰
   # âš ï¸ æ ·æœ¬åˆ†å¸ƒä¸å‡ï¼Œä½†ä¸å½±å“æ­£ç¡®æ€§
   ```

3. **ä¸ºä»€ä¹ˆé€šå¸¸ä¸ä¼šæ­»é”**
   - `LimitedPointsCollateFn` ä¸ä¼šæ”¹å˜ batch çš„**æ•°é‡**
   - åªæ”¹å˜æ¯ä¸ª batch ä¸­çš„**æ ·æœ¬æ•°é‡**
   - DDP åŒæ­¥ç‚¹åœ¨ epoch ç»“æŸï¼ˆæ‰€æœ‰ GPU å®Œæˆç›¸åŒæ•°é‡çš„ iterationsï¼‰
   - âœ… åªè¦ batch æ•°é‡ç›¸åŒï¼Œå°±ä¸ä¼šæ­»é”

**ç»“è®º**ï¼š
- âœ… **æŠ€æœ¯ä¸Šå…¼å®¹ DDP**ï¼ˆä¸ä¼šæ­»é”ï¼‰
- âš ï¸ **ä½†ä¸å¦‚ DynamicBatchSampler ä¼˜é›…**
- âš ï¸ **å¯èƒ½å¯¼è‡´ GPU é—´æ ·æœ¬åˆ†å¸ƒä¸å‡**

---

### 2. DDP ä¸‹çš„ Batch æ•°é‡å·®å¼‚é—®é¢˜

#### PyTorch DDP çš„åŒæ­¥æœºåˆ¶

```python
# ç®€åŒ–çš„ DDP è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch in dataloader:  # â† å…³é”®ï¼šæ¯ä¸ª GPU ç‹¬ç«‹è¿­ä»£
        # 1. Forward
        output = model(batch)
        loss = criterion(output, target)
        
        # 2. Backward (âœ… DDP åŒæ­¥ç‚¹)
        loss.backward()  # DDP ä¼šè‡ªåŠ¨åŒæ­¥æ¢¯åº¦
        
        # 3. Update
        optimizer.step()
    
    # Epoch ç»“æŸï¼ˆâš ï¸ å¯èƒ½çš„é—®é¢˜ç‚¹ï¼‰
    # å¦‚æœä¸åŒ GPU çš„ iteration æ•°é‡ä¸åŒï¼Œä¼šåœ¨è¿™é‡Œæ­»é”
```

#### ä¸åŒ GPU æœ‰ä¸åŒ Batch æ•°é‡çš„å®‰å…¨æ€§

**æƒ…å†µ 1ï¼šä½¿ç”¨ `DynamicBatchSampler`ï¼ˆâœ… å®‰å…¨ï¼‰**

```python
# PyTorch Lightning çš„ DDP å®ç°
# æ¯ä¸ª GPU çš„ dataloader æœ‰ç‹¬ç«‹çš„ DistributedSampler

# GPU 0: 100 ä¸ªæ ·æœ¬ â†’ DynamicBatchSampler â†’ äº§ç”Ÿ 25 ä¸ª batches
# GPU 1: 100 ä¸ªæ ·æœ¬ â†’ DynamicBatchSampler â†’ äº§ç”Ÿ 23 ä¸ª batches

# âœ… ä¸ºä»€ä¹ˆæ˜¯å®‰å…¨çš„ï¼Ÿ
# 1. Lightning ä½¿ç”¨ DistributedSampler ç¡®ä¿æ¯ä¸ª GPU çœ‹åˆ°ä¸åŒçš„æ ·æœ¬
# 2. DynamicBatchSampler åœ¨æ¯ä¸ª GPU ä¸Šç‹¬ç«‹è¿è¡Œ
# 3. Epoch ç»“æŸæ—¶ï¼ŒLightning ä¸ä¼šç­‰å¾…æ‰€æœ‰ GPU å®Œæˆç›¸åŒæ•°é‡çš„ iterations
# 4. æŒ‡æ ‡èšåˆåœ¨ validation_step_end / epoch_end é€šè¿‡ all_reduce å®Œæˆ
```

**PyTorch Lightning çš„å¤„ç†æ–¹å¼**ï¼š
```python
# lightning/pytorch/loops/fit_loop.py (ä¼ªä»£ç )
class FitLoop:
    def run(self):
        for epoch in range(max_epochs):
            # æ¯ä¸ª GPU ç‹¬ç«‹è¿è¡Œè‡ªå·±çš„ dataloader
            for batch_idx, batch in enumerate(self.trainer.train_dataloader):
                self.trainer.training_step(batch, batch_idx)
            
            # âœ… Epoch ç»“æŸï¼šLightning è‡ªåŠ¨å¤„ç†
            # - é€šè¿‡ barrier åŒæ­¥æ‰€æœ‰è¿›ç¨‹
            # - ä½†ä¸è¦æ±‚ç›¸åŒçš„ iteration æ•°é‡
            dist.barrier()  # ç®€åŒ–ç¤ºä¾‹
            
            # èšåˆæŒ‡æ ‡
            self.trainer.on_train_epoch_end()
```

**æƒ…å†µ 2ï¼šä½¿ç”¨å›ºå®š `batch_size`ï¼ˆâš ï¸ éœ€è¦æ³¨æ„ï¼‰**

```python
# æ ‡å‡† DataLoader with DistributedSampler
sampler = DistributedSampler(dataset, shuffle=True)
dataloader = DataLoader(dataset, batch_size=4, sampler=sampler, drop_last=False)

# å‡è®¾ dataset æœ‰ 100 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ª GPU
# GPU 0: 50 ä¸ªæ ·æœ¬ â†’ 13 ä¸ª batches (batch_size=4, drop_last=False)
#        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2]
# GPU 1: 50 ä¸ªæ ·æœ¬ â†’ 13 ä¸ª batches
#        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2]

# âœ… Batch æ•°é‡ç›¸åŒï¼Œå®‰å…¨

# ä½†å¦‚æœ drop_last=Trueï¼š
# GPU 0: 50 ä¸ªæ ·æœ¬ â†’ 12 ä¸ª batches
# GPU 1: 50 ä¸ªæ ·æœ¬ â†’ 12 ä¸ª batches
# âœ… ä»ç„¶å®‰å…¨ï¼ˆbatch æ•°é‡ç›¸åŒï¼‰
```

#### å…³é”®ç»“è®º

1. **ä¸åŒ GPU å¯ä»¥æœ‰ä¸åŒçš„ batch å¤§å°**ï¼ˆæ¯ä¸ª batch å†…çš„æ ·æœ¬æ•°ï¼‰
   - GPU 0: batch1æœ‰2ä¸ªæ ·æœ¬, batch2æœ‰4ä¸ªæ ·æœ¬
   - GPU 1: batch1æœ‰3ä¸ªæ ·æœ¬, batch2æœ‰3ä¸ªæ ·æœ¬
   - âœ… **å®Œå…¨æ²¡é—®é¢˜ï¼**

2. **ä¸åŒ GPU å¯ä»¥æœ‰ä¸åŒçš„ batch æ•°é‡**ï¼ˆiterations æ•°é‡ï¼‰
   - GPU 0: 25 ä¸ª batches
   - GPU 1: 23 ä¸ª batches
   - âœ… **åœ¨ PyTorch Lightning ä¸­æ˜¯å®‰å…¨çš„ï¼**
   - âš ï¸ **åœ¨çº¯ PyTorch DDP ä¸­å¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†**

3. **ä¸ºä»€ä¹ˆ Lightning å¯ä»¥å¤„ç†ä¸åŒçš„ batch æ•°é‡ï¼Ÿ**
   ```python
   # Lightning çš„è®­ç»ƒå¾ªç¯ä¸è¦æ±‚æ‰€æœ‰è¿›ç¨‹å®Œæˆç›¸åŒæ•°é‡çš„ iterations
   # è€Œæ˜¯é€šè¿‡ä»¥ä¸‹æœºåˆ¶ä¿è¯æ­£ç¡®æ€§ï¼š
   
   # 1. æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹å®Œæˆè‡ªå·±çš„ dataloader
   # 2. åœ¨ validation/test ä¹‹å‰é€šè¿‡ barrier åŒæ­¥
   # 3. æŒ‡æ ‡èšåˆæ—¶åªèšåˆå·²å®Œæˆçš„ batches
   # 4. Epoch ç»“æŸæ—¶è‡ªåŠ¨å¯¹é½
   ```

---

### 3. å½“å‰ä»£ç çš„ DDP çŠ¶æ€

#### âœ… `DynamicBatchSampler` å·²æ­£ç¡®é›†æˆ

```python
# pointsuite/data/datamodule_base.py
def _create_dataloader(self, dataset, shuffle=True, drop_last=False, use_sampler_weights=False):
    if self.use_dynamic_batch:
        # âœ… æ­£ç¡®ï¼šä½¿ç”¨ batch_samplerï¼ˆLightning ä¼šè‡ªåŠ¨åŒ…è£… DistributedSamplerï¼‰
        batch_sampler = DynamicBatchSampler(
            dataset=dataset,
            max_points=self.max_points,
            shuffle=(shuffle and base_sampler is None),
            drop_last=drop_last,
            sampler=base_sampler
        )
        
        return DataLoader(
            dataset,
            batch_sampler=batch_sampler,  # âœ… å…³é”®
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            # ...
        )
```

**PyTorch Lightning çš„è‡ªåŠ¨å¤„ç†**ï¼š
```python
# å½“ä½ è¿è¡Œ Trainer(strategy='ddp', devices=2) æ—¶ï¼š
# Lightning ä¼šè‡ªåŠ¨ï¼š

# 1. æ£€æµ‹åˆ° batch_sampler
trainer = pl.Trainer(strategy='ddp', devices=2)

# 2. åœ¨ DDP æ¨¡å¼ä¸‹ï¼ŒLightning ä¼šåŒ…è£…ä½ çš„ batch_sampler
# å†…éƒ¨ç±»ä¼¼äºï¼š
from torch.utils.data.distributed import DistributedSampler

# ä¼ªä»£ç ï¼ˆLightning å†…éƒ¨å®ç°ï¼‰
if trainer.world_size > 1 and batch_sampler is not None:
    # Lightning ä¸ä¼šç›´æ¥åŒ…è£… batch_sampler
    # ä½†ä¼šç¡®ä¿æ¯ä¸ª rank è·å¾—ä¸åŒçš„æ ·æœ¬
    # é€šè¿‡åœ¨ dataset æˆ– sampler å±‚é¢å¤„ç†
    pass
```

**é‡è¦æç¤º**ï¼š
```python
# âš ï¸ æ½œåœ¨é—®é¢˜ï¼šDynamicBatchSampler + DDP
# 
# å½“ä½¿ç”¨è‡ªå®šä¹‰ batch_sampler æ—¶ï¼ŒLightning æ— æ³•è‡ªåŠ¨åº”ç”¨ DistributedSampler
# éœ€è¦ç¡®ä¿ DynamicBatchSampler å†…éƒ¨å¤„ç†åˆ†å¸ƒå¼é‡‡æ ·

# è§£å†³æ–¹æ¡ˆ 1ï¼šåœ¨ DynamicBatchSampler ä¸­é›†æˆ DistributedSampler
class DynamicBatchSampler:
    def __init__(self, dataset, ..., rank=None, world_size=None):
        self.rank = rank or 0
        self.world_size = world_size or 1
    
    def __iter__(self):
        # æ ¹æ® rank å’Œ world_size åˆ†é…æ ·æœ¬
        indices = self._get_indices()
        # åªå¤„ç†å±äºå½“å‰ rank çš„æ ·æœ¬
        indices = indices[self.rank::self.world_size]
        # ...

# è§£å†³æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ replace_sampler_ddp=Falseï¼ˆLightningï¼‰
trainer = pl.Trainer(
    strategy='ddp',
    devices=2,
    replace_sampler_ddp=False  # å‘Šè¯‰ Lightning ä¸è¦æ›¿æ¢ sampler
)

# ç„¶åæ‰‹åŠ¨ï¿½ï¿½ï¿½ DataModule ä¸­å¤„ç† DDP
from pytorch_lightning.utilities import rank_zero_only
from torch.utils.data.distributed import DistributedSampler

def _create_dataloader(self, dataset, shuffle=True, ...):
    if self.use_dynamic_batch:
        # æ‰‹åŠ¨åˆ›å»º base_sampler with DistributedSampler
        if self.trainer and self.trainer.world_size > 1:
            base_indices = list(range(len(dataset)))
            dist_sampler = DistributedSampler(
                dataset,
                num_replicas=self.trainer.world_size,
                rank=self.trainer.global_rank,
                shuffle=shuffle
            )
            # å°† dist_sampler ä¼ é€’ç»™ DynamicBatchSampler
            batch_sampler = DynamicBatchSampler(
                dataset=dataset,
                max_points=self.max_points,
                sampler=dist_sampler,  # â† å…³é”®
                shuffle=False  # å·²ç»åœ¨ dist_sampler ä¸­å¤„ç†
            )
        else:
            # å• GPU æ¨¡å¼
            batch_sampler = DynamicBatchSampler(...)
```

---

### 4. æ¨èçš„ DDP é…ç½®

#### é…ç½® 1ï¼šDynamicBatchSamplerï¼ˆæ¨èï¼‰

```python
# config.yaml
data:
  batch_size: 4  # å½“ use_dynamic_batch=True æ—¶å¿½ç•¥
  use_dynamic_batch: true
  max_points: 500000
  num_workers: 4

trainer:
  strategy: ddp
  devices: 2
  accelerator: gpu
```

```python
# ä½¿ç”¨æ–¹å¼
datamodule = BinPklDataModule(
    data_root='path/to/data',
    use_dynamic_batch=True,
    max_points=500000,
    batch_size=4,  # è¢«å¿½ç•¥
)

trainer = pl.Trainer(
    strategy='ddp',
    devices=2,
    accelerator='gpu',
)

trainer.fit(model, datamodule)
```

**é¢„æœŸè¡Œä¸º**ï¼š
- GPU 0 å¯èƒ½å¤„ç† 25 ä¸ª batchesï¼ˆæ¯ä¸ª batch 2-5 ä¸ªæ ·æœ¬ï¼‰
- GPU 1 å¯èƒ½å¤„ç† 27 ä¸ª batchesï¼ˆæ¯ä¸ª batch 2-4 ä¸ªæ ·æœ¬ï¼‰
- âœ… å®Œå…¨æ­£å¸¸ï¼ŒæŒ‡æ ‡ä¼šæ­£ç¡®èšåˆ

---

#### é…ç½® 2ï¼šå›ºå®š batch_size + drop_last=Trueï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰

```python
# config.yaml
data:
  batch_size: 4
  use_dynamic_batch: false  # ä¸ä½¿ç”¨åŠ¨æ€ batch
  num_workers: 4

trainer:
  strategy: ddp
  devices: 2
```

**é¢„æœŸè¡Œä¸º**ï¼š
- GPU 0 å¤„ç† N ä¸ª batchesï¼ˆæ¯ä¸ª batch å›ºå®š 4 ä¸ªæ ·æœ¬ï¼‰
- GPU 1 å¤„ç† N ä¸ª batchesï¼ˆæ¯ä¸ª batch å›ºå®š 4 ä¸ªæ ·æœ¬ï¼‰
- âœ… ä¼ ç»Ÿæ–¹å¼ï¼Œç¨³å®š

---

### 5. æ½œåœ¨é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### é—®é¢˜ 1ï¼šDynamicBatchSampler åœ¨ DDP ä¸‹å¯èƒ½ä¸ä¼šè‡ªåŠ¨åˆ†å¸ƒ

**é—®é¢˜æè¿°**ï¼š
```python
# å½“å‰å®ç°å¯èƒ½å¯¼è‡´æ‰€æœ‰ GPU çœ‹åˆ°ç›¸åŒçš„æ ·æœ¬
# å› ä¸º DynamicBatchSampler æ²¡æœ‰æ„ŸçŸ¥åˆ° DDP ç¯å¢ƒ
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

**æ–¹æ¡ˆ Aï¼šä¿®æ”¹ DynamicBatchSampler æ”¯æŒåˆ†å¸ƒå¼ï¼ˆæ¨èï¼‰**

```python
# pointsuite/data/datasets/collate.py
class DynamicBatchSampler:
    def __init__(
        self, 
        dataset, 
        max_points=500000, 
        shuffle=True, 
        drop_last=False, 
        sampler=None,
        # âœ… æ–°å¢ DDP å‚æ•°
        num_replicas=None,
        rank=None,
        seed=0,
    ):
        self.dataset = dataset
        self.max_points = max_points
        self.shuffle = shuffle
        self.drop_last = drop_last
        self.sampler = sampler
        
        # âœ… DDP æ”¯æŒ
        if num_replicas is None:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                num_replicas = dist.get_world_size()
                rank = dist.get_rank()
            else:
                num_replicas = 1
                rank = 0
        
        self.num_replicas = num_replicas
        self.rank = rank
        self.seed = seed
        self.epoch = 0
        
        # é¢„å…ˆè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°
        self.num_points_list = self._get_num_points_list()
    
    def set_epoch(self, epoch):
        """è®¾ç½® epochï¼ˆç”¨äº DDP shuffleï¼‰"""
        self.epoch = epoch
    
    def __iter__(self):
        # ç”Ÿæˆç´¢å¼•åˆ—è¡¨
        if self.sampler is not None:
            # ä½¿ç”¨æä¾›çš„ sampler
            indices = list(self.sampler)
        elif self.shuffle:
            # âœ… ä½¿ç”¨ç¡®å®šæ€§éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆDDP å‹å¥½ï¼‰
            g = torch.Generator()
            g.manual_seed(self.seed + self.epoch)
            indices = torch.randperm(len(self.dataset), generator=g).tolist()
        else:
            indices = list(range(len(self.dataset)))
        
        # âœ… æ ¹æ® rank åˆ†é…æ ·æœ¬ï¼ˆç±»ä¼¼ DistributedSamplerï¼‰
        # ç¡®ä¿æ¯ä¸ª GPU è·å¾—ä¸åŒçš„æ ·æœ¬å­é›†
        indices = indices[self.rank:len(indices):self.num_replicas]
        
        # åŠ¨æ€ç”Ÿæˆ batch
        batch = []
        batch_points = 0
        
        for idx in indices:
            num_points = self.num_points_list[idx]
            
            if len(batch) == 0 or batch_points + num_points <= self.max_points:
                batch.append(idx)
                batch_points += num_points
            else:
                yield batch
                batch = [idx]
                batch_points = num_points
        
        # å¤„ç†æœ€åä¸€ä¸ª batch
        if len(batch) > 0 and not self.drop_last:
            yield batch
    
    def __len__(self):
        # âœ… æ¯ä¸ª GPU çš„é•¿åº¦ï¼ˆåŸºäºåˆ†é…ç»™è¯¥ GPU çš„æ ·æœ¬ï¼‰
        num_samples = len(self.dataset) // self.num_replicas
        if not self.drop_last and len(self.dataset) % self.num_replicas != 0:
            num_samples += 1
        
        # ä¼°ç®— batch æ•°é‡
        total_points = sum(self.num_points_list[self.rank:len(self.num_points_list):self.num_replicas])
        estimated_batches = max(1, (total_points + self.max_points - 1) // self.max_points)
        return estimated_batches
```

**ä¿®æ”¹ DataModule**ï¼š
```python
# pointsuite/data/datamodule_base.py
def _create_dataloader(self, dataset, shuffle=True, drop_last=False, use_sampler_weights=False):
    if self.use_dynamic_batch:
        # âœ… æ£€æµ‹ DDP ç¯å¢ƒ
        import torch.distributed as dist
        num_replicas = None
        rank = None
        if dist.is_available() and dist.is_initialized():
            num_replicas = dist.get_world_size()
            rank = dist.get_rank()
        
        # åˆ›å»º base_samplerï¼ˆå¦‚æœéœ€è¦ï¼‰
        base_sampler = None
        if use_sampler_weights and self.train_sampler_weights is not None:
            # âš ï¸ WeightedRandomSampler + DDP éœ€è¦ç‰¹æ®Šå¤„ç†
            # æš‚æ—¶ç¦ç”¨æˆ–è€…éœ€è¦è‡ªå®šä¹‰å®ç°
            pass
        
        # åˆ›å»º DynamicBatchSampler with DDP support
        batch_sampler = DynamicBatchSampler(
            dataset=dataset,
            max_points=self.max_points,
            shuffle=shuffle,
            drop_last=drop_last,
            sampler=base_sampler,
            num_replicas=num_replicas,  # âœ… ä¼ é€’ DDP å‚æ•°
            rank=rank,
            seed=42,  # å¯é…ç½®
        )
        
        return DataLoader(
            dataset,
            batch_sampler=batch_sampler,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers and self.num_workers > 0,
            prefetch_factor=self.prefetch_factor if self.num_workers > 0 else None,
        )
```

**Trainer é…ç½®**ï¼š
```python
# éœ€è¦åœ¨æ¯ä¸ª epoch è®¾ç½® epoch numberï¼ˆç”¨äº shuffleï¼‰
class YourTask(BaseTask):
    def on_train_epoch_start(self):
        # âœ… è®¾ç½® epochï¼ˆç¡®ä¿æ¯ä¸ª epoch çš„ shuffle ä¸åŒï¼‰
        if hasattr(self.trainer.train_dataloader.batch_sampler, 'set_epoch'):
            self.trainer.train_dataloader.batch_sampler.set_epoch(self.current_epoch)
```

---

**æ–¹æ¡ˆ Bï¼šä½¿ç”¨ Lightning çš„ replace_sampler_ddpï¼ˆç®€å•ä½†æœ‰é™åˆ¶ï¼‰**

```python
# åœ¨ DataModule ä¸­
class BinPklDataModule(DataModuleBase):
    def __init__(self, ..., **kwargs):
        super().__init__(...)
        # ä¸éœ€è¦ç‰¹æ®Šå¤„ç†
    
    # Lightning ä¼šè‡ªåŠ¨å¤„ç† DDP
    # ä½†å¯èƒ½æ— æ³•ä¸ DynamicBatchSampler å®Œç¾é…åˆ
```

```python
# åœ¨ Trainer ä¸­
trainer = pl.Trainer(
    strategy='ddp',
    devices=2,
    # âœ… è®© Lightning è‡ªåŠ¨å¤„ç†ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
    # replace_sampler_ddp=True  # é»˜è®¤å€¼
)

# âš ï¸ ä½†è¿™å¯èƒ½ä¸ä¼šæ­£ç¡®å¤„ç† DynamicBatchSampler
```

---

#### é—®é¢˜ 2ï¼šWeightedRandomSampler + DynamicBatchSampler + DDP

**é—®é¢˜æè¿°**ï¼š
```python
# ä¸‰è€…ç»„åˆä½¿ç”¨æ—¶çš„å¤æ‚æ€§ï¼š
# 1. WeightedRandomSampler ç”¨äºç±»åˆ«å¹³è¡¡
# 2. DynamicBatchSampler ç”¨äºç‚¹æ•°æ§åˆ¶
# 3. DDP éœ€è¦åˆ†å¸ƒå¼é‡‡æ ·

# å½“å‰ä»£ç ï¼š
base_sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)
batch_sampler = DynamicBatchSampler(dataset, sampler=base_sampler)

# âš ï¸ é—®é¢˜ï¼šbase_sampler æ²¡æœ‰æ„ŸçŸ¥ DDPï¼Œæ‰€æœ‰ GPU å¯èƒ½äº§ç”Ÿç›¸åŒçš„æ ·æœ¬
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šè‡ªå®šä¹‰ DistributedWeightedSampler
class DistributedWeightedSampler:
    """
    ç»“åˆ DistributedSampler å’Œ WeightedRandomSampler çš„é‡‡æ ·å™¨
    
    ç¡®ä¿ï¼š
    1. æ¯ä¸ª GPU çœ‹åˆ°ä¸åŒçš„æ ·æœ¬ï¼ˆDistributedï¼‰
    2. æ ·æœ¬æŒ‰æƒé‡é‡‡æ ·ï¼ˆWeightedï¼‰
    """
    def __init__(self, dataset, weights, num_samples, replacement=True,
                 num_replicas=None, rank=None, seed=0):
        # DistributedSampler å‚æ•°
        if num_replicas is None:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                num_replicas = dist.get_world_size()
                rank = dist.get_rank()
            else:
                num_replicas = 1
                rank = 0
        
        self.num_replicas = num_replicas
        self.rank = rank
        self.epoch = 0
        self.seed = seed
        
        # WeightedRandomSampler å‚æ•°
        self.weights = torch.as_tensor(weights, dtype=torch.double)
        self.num_samples = num_samples
        self.replacement = replacement
        
        # è®¡ç®—æ¯ä¸ª GPU çš„æ ·æœ¬æ•°
        self.num_samples_per_replica = self.num_samples // self.num_replicas
        self.total_size = self.num_samples_per_replica * self.num_replicas
    
    def set_epoch(self, epoch):
        self.epoch = epoch
    
    def __iter__(self):
        # ä½¿ç”¨ç¡®å®šæ€§éšæœºæ•°ï¼ˆåŸºäº epoch å’Œ rankï¼‰
        g = torch.Generator()
        g.manual_seed(self.seed + self.epoch)
        
        # ç”ŸæˆåŠ æƒæ ·æœ¬ç´¢å¼•
        indices = torch.multinomial(
            self.weights, 
            self.total_size, 
            replacement=self.replacement,
            generator=g
        ).tolist()
        
        # åˆ†é…ç»™å½“å‰ GPU
        indices = indices[self.rank:self.total_size:self.num_replicas]
        
        return iter(indices)
    
    def __len__(self):
        return self.num_samples_per_replica


# åœ¨ DataModule ä¸­ä½¿ç”¨
def _create_dataloader(self, dataset, shuffle=True, drop_last=False, use_sampler_weights=False):
    if self.use_dynamic_batch:
        import torch.distributed as dist
        num_replicas = None
        rank = None
        if dist.is_available() and dist.is_initialized():
            num_replicas = dist.get_world_size()
            rank = dist.get_rank()
        
        # åˆ›å»ºåˆ†å¸ƒå¼åŠ æƒé‡‡æ ·å™¨
        base_sampler = None
        if use_sampler_weights and self.train_sampler_weights is not None:
            base_sampler = DistributedWeightedSampler(
                dataset=dataset,
                weights=self.train_sampler_weights,
                num_samples=len(dataset),
                replacement=True,
                num_replicas=num_replicas,
                rank=rank,
                seed=42
            )
        
        # åˆ›å»º DynamicBatchSampler
        batch_sampler = DynamicBatchSampler(
            dataset=dataset,
            max_points=self.max_points,
            shuffle=(shuffle and base_sampler is None),
            drop_last=drop_last,
            sampler=base_sampler,
            num_replicas=num_replicas,
            rank=rank,
        )
        
        return DataLoader(...)
```

---

### 6. æµ‹è¯•ä¸éªŒè¯

#### æµ‹è¯•è„šæœ¬ï¼šéªŒè¯ DDP + DynamicBatchSampler

```python
# test/test_ddp_dynamic_batch.py
"""
æµ‹è¯• DynamicBatchSampler åœ¨ DDP ç¯å¢ƒä¸‹çš„æ­£ç¡®æ€§

éªŒè¯ï¼š
1. ä¸åŒ GPU çœ‹åˆ°ä¸åŒçš„æ ·æœ¬
2. æ‰€æœ‰æ ·æœ¬è¢«è¦†ç›–ï¼ˆæ²¡æœ‰é—æ¼ï¼‰
3. Batch å¤§å°æ­£ç¡®é™åˆ¶
4. æŒ‡æ ‡æ­£ç¡®èšåˆ
"""

import torch
import pytorch_lightning as pl
from pointsuite.data.datamodule_binpkl import BinPklDataModule
from pointsuite.tasks.semantic_segmentation import SemanticSegmentationTask

def test_ddp_different_samples():
    """æµ‹è¯•ä¸åŒ GPU æ˜¯å¦çœ‹åˆ°ä¸åŒæ ·æœ¬"""
    print("\n[æµ‹è¯•] DDP - ä¸åŒ GPU æ ·æœ¬åˆ†å¸ƒ")
    
    # åˆ›å»º DataModule
    datamodule = BinPklDataModule(
        data_root='path/to/data',
        use_dynamic_batch=True,
        max_points=500000,
        batch_size=4,
    )
    datamodule.setup('fit')
    
    # æ¨¡æ‹Ÿ DDPï¼ˆ2ä¸ªGPUï¼‰
    import torch.distributed as dist
    
    # GPU 0
    seen_samples_gpu0 = set()
    for batch in datamodule.train_dataloader():
        # è®°å½•çœ‹åˆ°çš„æ ·æœ¬ç´¢å¼•ï¼ˆéœ€è¦åœ¨ batch ä¸­æ·»åŠ ç´¢å¼•è¿½è¸ªï¼‰
        seen_samples_gpu0.update(batch['sample_idx'].tolist())
    
    # GPU 1
    seen_samples_gpu1 = set()
    for batch in datamodule.train_dataloader():
        seen_samples_gpu1.update(batch['sample_idx'].tolist())
    
    # éªŒè¯
    overlap = seen_samples_gpu0 & seen_samples_gpu1
    all_samples = seen_samples_gpu0 | seen_samples_gpu1
    
    print(f"GPU 0 æ ·æœ¬æ•°: {len(seen_samples_gpu0)}")
    print(f"GPU 1 æ ·æœ¬æ•°: {len(seen_samples_gpu1)}")
    print(f"é‡å æ ·æœ¬æ•°: {len(overlap)}")
    print(f"æ€»æ ·æœ¬æ•°: {len(all_samples)}")
    print(f"æ•°æ®é›†å¤§å°: {len(datamodule.train_dataset)}")
    
    # æ–­è¨€
    assert len(overlap) == 0, "ä¸åŒ GPU ä¸åº”è¯¥çœ‹åˆ°ç›¸åŒçš„æ ·æœ¬"
    assert len(all_samples) == len(datamodule.train_dataset), "æ‰€æœ‰æ ·æœ¬éƒ½åº”è¯¥è¢«è¦†ç›–"


def test_ddp_batch_sizes():
    """æµ‹è¯• DDP ä¸‹çš„ batch å¤§å°æ§åˆ¶"""
    print("\n[æµ‹è¯•] DDP - Batch å¤§å°æ§åˆ¶")
    
    datamodule = BinPklDataModule(
        data_root='path/to/data',
        use_dynamic_batch=True,
        max_points=500000,
    )
    datamodule.setup('fit')
    
    # è®°å½•æ¯ä¸ª batch çš„ç‚¹æ•°
    batch_points = []
    for batch in datamodule.train_dataloader():
        total_points = batch['coord'].shape[0]
        batch_points.append(total_points)
        
        # éªŒè¯ä¸è¶…è¿‡é™åˆ¶
        assert total_points <= 500000, f"Batch ç‚¹æ•° {total_points} è¶…è¿‡é™åˆ¶ 500000"
    
    print(f"Batch æ•°é‡: {len(batch_points)}")
    print(f"å¹³å‡ç‚¹æ•°: {sum(batch_points) / len(batch_points):.0f}")
    print(f"æœ€å°ç‚¹æ•°: {min(batch_points)}")
    print(f"æœ€å¤§ç‚¹æ•°: {max(batch_points)}")


def test_ddp_metrics_aggregation():
    """æµ‹è¯• DDP ä¸‹çš„æŒ‡æ ‡èšåˆ"""
    print("\n[æµ‹è¯•] DDP - æŒ‡æ ‡èšåˆ")
    
    # åˆ›å»ºç®€å•çš„è®­ç»ƒå¾ªç¯
    datamodule = BinPklDataModule(
        data_root='path/to/data',
        use_dynamic_batch=True,
        max_points=500000,
    )
    
    model = SemanticSegmentationTask(
        num_classes=10,
        learning_rate=0.001,
    )
    
    trainer = pl.Trainer(
        strategy='ddp',
        devices=2,
        accelerator='gpu',
        max_epochs=1,
        limit_train_batches=10,  # åªæµ‹è¯• 10 ä¸ª batches
        limit_val_batches=5,
    )
    
    # è®­ç»ƒ
    trainer.fit(model, datamodule)
    
    # è·å–æŒ‡æ ‡
    metrics = trainer.callback_metrics
    
    print("è®­ç»ƒæŒ‡æ ‡:")
    for key, value in metrics.items():
        if 'train' in key:
            print(f"  {key}: {value:.4f}")
    
    print("\néªŒè¯æŒ‡æ ‡:")
    for key, value in metrics.items():
        if 'val' in key:
            print(f"  {key}: {value:.4f}")


if __name__ == '__main__':
    # è¿è¡Œæµ‹è¯•
    # test_ddp_different_samples()
    # test_ddp_batch_sizes()
    test_ddp_metrics_aggregation()
```

**è¿è¡Œæµ‹è¯•**ï¼š
```bash
# å• GPU æµ‹è¯•
python test/test_ddp_dynamic_batch.py

# DDP æµ‹è¯•ï¼ˆ2ä¸ªGPUï¼‰
python -m torch.distributed.launch --nproc_per_node=2 test/test_ddp_dynamic_batch.py

# æˆ–ä½¿ç”¨ Lightning CLI
python main.py fit --trainer.strategy=ddp --trainer.devices=2 --config config.yaml
```

---

### 7. æœ€ç»ˆæ¨è

#### âœ… æ¨èæ–¹æ¡ˆï¼šä¿®æ”¹ DynamicBatchSampler æ”¯æŒ DDP

**æ­¥éª¤**ï¼š
1. ä¿®æ”¹ `DynamicBatchSampler` æ·»åŠ  DDP å‚æ•°ï¼ˆè§ä¸Šæ–‡æ–¹æ¡ˆ Aï¼‰
2. ä¿®æ”¹ `DataModuleBase._create_dataloader` ä¼ é€’ DDP å‚æ•°
3. ï¼ˆå¯é€‰ï¼‰å®ç° `DistributedWeightedSampler` æ”¯æŒåŠ æƒé‡‡æ · + DDP
4. åœ¨ Task ä¸­æ·»åŠ  `on_train_epoch_start` è®¾ç½® epoch
5. æµ‹è¯•éªŒè¯

**ä¼˜åŠ¿**ï¼š
- âœ… å®Œå…¨æ§åˆ¶é‡‡æ ·é€»è¾‘
- âœ… æ”¯æŒåŠ¨æ€ batch + åŠ æƒé‡‡æ · + DDP
- âœ… æ¯ä¸ª GPU ç‹¬ç«‹å†³å®š batch å¤§å°
- âœ… æŒ‡æ ‡æ­£ç¡®èšåˆ

**ä»£ç ä¿®æ”¹é‡**ï¼š
- `collate.py`: ~50 è¡Œï¼ˆä¿®æ”¹ DynamicBatchSamplerï¼‰
- `datamodule_base.py`: ~20 è¡Œï¼ˆä¼ é€’ DDP å‚æ•°ï¼‰
- `base_task.py`: ~5 è¡Œï¼ˆè®¾ç½® epochï¼‰
- ï¼ˆå¯é€‰ï¼‰`collate.py`: ~80 è¡Œï¼ˆæ·»åŠ  DistributedWeightedSamplerï¼‰

---

#### âš ï¸ å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ LimitedPointsCollateFn

**é€‚ç”¨åœºæ™¯**ï¼š
- å¿«é€ŸåŸå‹éªŒè¯
- ä¸éœ€è¦åŠ æƒé‡‡æ ·
- å¯ä»¥æ¥å—æ ·æœ¬åˆ†å¸ƒä¸å‡

**é…ç½®**ï¼š
```python
# config.yaml
data:
  batch_size: 4
  use_dynamic_batch: false
  num_workers: 4

# åœ¨ DataLoader ä¸­ä½¿ç”¨
from pointsuite.data.datasets.collate import LimitedPointsCollateFn

limited_collate = LimitedPointsCollateFn(max_points=500000, strategy='drop_largest')

dataloader = DataLoader(
    dataset,
    batch_size=4,
    shuffle=True,
    collate_fn=limited_collate,
    # Lightning ä¼šè‡ªåŠ¨å¤„ç† DistributedSampler
)
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç 
- âœ… Lightning è‡ªåŠ¨å¤„ç† DistributedSampler

**åŠ£åŠ¿**ï¼š
- âš ï¸ å¯èƒ½å¯¼è‡´ GPU é—´æ ·æœ¬åˆ†å¸ƒä¸å‡
- âš ï¸ æ— æ³•ä¸ WeightedRandomSampler å®Œç¾é…åˆ

---

## ğŸ“Š å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | DynamicBatchSampler<br/>(å½“å‰) | DynamicBatchSampler<br/>(ä¿®æ”¹å) | LimitedPointsCollateFn | å›ºå®š batch_size |
|------|------------------------------|-------------------------------|----------------------|----------------|
| **DDP å…¼å®¹æ€§** | âš ï¸ éœ€è¦ä¿®æ”¹ | âœ… å®Œå…¨å…¼å®¹ | âœ… å…¼å®¹ï¼ˆæœ‰é™åˆ¶ï¼‰ | âœ… å®Œå…¨å…¼å®¹ |
| **ä¸åŒ GPU batch æ•°ä¸åŒ** | âš ï¸ å¯èƒ½ç›¸åŒ | âœ… æ”¯æŒ | âœ… ç›¸åŒï¼ˆä½†æ ·æœ¬æ•°ä¸åŒï¼‰ | âŒ ç›¸åŒ |
| **ç‚¹æ•°æ§åˆ¶** | âœ… ç²¾ç¡® | âœ… ç²¾ç¡® | âœ… ç²¾ç¡® | âŒ æ— æ³•æ§åˆ¶ |
| **æ ·æœ¬è¦†ç›–** | âœ… å®Œæ•´ | âœ… å®Œæ•´ | âš ï¸ éƒ¨åˆ†ä¸¢å¼ƒ | âœ… å®Œæ•´ |
| **åŠ æƒé‡‡æ ·æ”¯æŒ** | âš ï¸ éœ€è¦ç‰¹æ®Šå¤„ç† | âœ… å®Œç¾æ”¯æŒ | âš ï¸ éš¾ä»¥ç»“åˆ | âœ… æ”¯æŒ |
| **å®ç°å¤æ‚åº¦** | - | â­â­â­ | â­ | â­ |
| **æ¨èç¨‹åº¦** | âŒ ä¸æ¨è | â­â­â­â­â­ | â­â­â­ | â­â­ |

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒç»“è®º

1. **åŠ¨æ€ batch æ–¹æ³•å®Œå…¨å…¼å®¹ DDP**
   - âœ… `DynamicBatchSampler` æ˜¯æœ€ä½³é€‰æ‹©ï¼ˆéœ€è¦ä¿®æ”¹ä»¥æ”¯æŒ DDPï¼‰
   - âœ… `LimitedPointsCollateFn` å¯ä»¥ä½¿ç”¨ï¼ˆä½†ä¸å¦‚å‰è€…ä¼˜é›…ï¼‰

2. **ä¸åŒæ˜¾å¡å¯ä»¥æœ‰ä¸åŒçš„ batch æ•°**
   - âœ… PyTorch Lightning ä¼šæ­£ç¡®å¤„ç†
   - âœ… æŒ‡æ ‡é€šè¿‡ torchmetrics æ­£ç¡®èšåˆ
   - âœ… ä¸ä¼šæ­»é”

3. **æ¨èå®ç°**
   ```python
   # ä¿®æ”¹ DynamicBatchSampler æ”¯æŒ DDPï¼ˆçº¦ 70 è¡Œä»£ç ï¼‰
   # ä¿®æ”¹ DataModuleBase ä¼ é€’ DDP å‚æ•°ï¼ˆçº¦ 20 è¡Œä»£ç ï¼‰
   # æ·»åŠ  epoch è®¾ç½®ï¼ˆçº¦ 5 è¡Œä»£ç ï¼‰
   
   # æ€»å·¥ä½œé‡ï¼š~95 è¡Œä»£ç ä¿®æ”¹ + æµ‹è¯•
   # æ”¶ç›Šï¼šå®Œç¾çš„ DDP + åŠ¨æ€ batch + åŠ æƒé‡‡æ ·æ”¯æŒ
   ```

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**å¦‚æœéœ€è¦ç«‹å³ä½¿ç”¨**ï¼š
- ä½¿ç”¨ `LimitedPointsCollateFn`ï¼ˆå·²ç» DDP å…¼å®¹ï¼‰
- æˆ–ä½¿ç”¨å›ºå®š `batch_size`ï¼ˆæœ€ç¨³å®šï¼‰

**å¦‚æœéœ€è¦æœ€ä¼˜æ–¹æ¡ˆ**ï¼š
- å®ç° `DynamicBatchSampler` çš„ DDP æ”¯æŒ
- å®ç° `DistributedWeightedSampler`ï¼ˆå¦‚æœéœ€è¦åŠ æƒé‡‡æ ·ï¼‰
- ç¼–å†™æµ‹è¯•éªŒè¯æ­£ç¡®æ€§

---

## ğŸ“ å¿«é€Ÿå‚è€ƒ

```python
# âœ… æ¨èé…ç½®ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰
datamodule = BinPklDataModule(
    data_root='path/to/data',
    use_dynamic_batch=True,  # ä½¿ç”¨ DynamicBatchSampler
    max_points=500000,
    train_sampler_weights=weights,  # å¯é€‰ï¼šåŠ æƒé‡‡æ ·
)

trainer = pl.Trainer(
    strategy='ddp',
    devices=2,
    accelerator='gpu',
)

# âœ… å½“å‰å¯ç”¨é…ç½®ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
datamodule = BinPklDataModule(
    data_root='path/to/data',
    use_dynamic_batch=False,  # ä½¿ç”¨å›ºå®š batch_size
    batch_size=4,
)

trainer = pl.Trainer(
    strategy='ddp',
    devices=2,
)
```

---

**æœ€ç»ˆç­”æ¡ˆ**ï¼š
1. **åŠ¨æ€ batch æ–¹æ³•èƒ½å…¼å®¹ DDP å—ï¼Ÿ** â†’ âœ… æ˜¯çš„ï¼Œä½†éœ€è¦ä¿®æ”¹ `DynamicBatchSampler` ä»¥æ”¯æŒ DDP
2. **èƒ½æ”¯æŒä¸åŒæ˜¾å¡ batch æ•°ä¸åŒå—ï¼Ÿ** â†’ âœ… å¯ä»¥ï¼ŒPyTorch Lightning ä¼šæ­£ç¡®å¤„ç†
