"""
ä»…è¿è¡Œé¢„æµ‹çš„è„šæœ¬ - ä½¿ç”¨å·²è®­ç»ƒçš„checkpoint

ç”¨æ³•ï¼š
    python predict_dales.py

éœ€è¦ä¿®æ”¹çš„é…ç½®ï¼š
    - CHECKPOINT_PATH: ä½ çš„checkpointè·¯å¾„
    - TEST_DATA: æµ‹è¯•æ•°æ®è·¯å¾„
    - OUTPUT_DIR: é¢„æµ‹ç»“æœè¾“å‡ºè·¯å¾„
"""

import os
import sys
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pointsuite.data import BinPklDataModule
from pointsuite.data.transforms import ToTensor, Collect, CenterShift
from pointsuite.tasks import SemanticSegmentationTask
from pointsuite.models import PointTransformerV2, SegHead
from pointsuite.utils.callbacks import SemanticPredictLasWriter, TextLoggingCallback


def main():
    # ========================================================================
    # é…ç½® - æ ¹æ®ä½ çš„å®é™…æƒ…å†µä¿®æ”¹
    # ========================================================================
    
    # ğŸ”¥ é‡è¦ï¼šä¿®æ”¹ä¸ºä½ å®é™…çš„è·¯å¾„
    CHECKPOINT_PATH = r"E:\code\PointSuite\outputs\dales\csv_logs\version_42\checkpoints\dales-epoch=09-mean_iou=0.8094.ckpt"  # ä¿®æ”¹è¿™é‡Œï¼
    TEST_DATA = r"E:\data\DALES\dales_las\bin\test"  # ä¿®æ”¹è¿™é‡Œï¼
    OUTPUT_DIR = r"E:\data\DALES\dales_las\bin\result"  # ä¿®æ”¹è¿™é‡Œï¼
    
    # Predict é…ç½®
    NUM_WORKERS = 0
    MAX_POINTS_INFERENCE = 300000  # æ¨ç†æ—¶ä½¿ç”¨æ›´å¤§batch
    
    pl.seed_everything(42)
    
    if torch.cuda.is_available():
        torch.set_float32_matmul_precision('high')
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    
    print("\n" + "=" * 80)
    print(f"DALES é¢„æµ‹ - ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹")
    print("=" * 80)
    print(f"Checkpoint: {CHECKPOINT_PATH}")
    print(f"æµ‹è¯•æ•°æ®: {TEST_DATA}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    # ========================================================================
    # 1. ä» Checkpoint åŠ è½½æ¨¡å‹ (è·å–é…ç½®ä¿¡æ¯)
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ä» Checkpoint åŠ è½½æ¨¡å‹...")
    print("=" * 80)
    
    # æ£€æŸ¥ checkpoint æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"\nâŒ é”™è¯¯: Checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {CHECKPOINT_PATH}")
        print("\nè¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹ï¼š")
        print("1. ç¡®è®¤è®­ç»ƒæ˜¯å¦å®Œæˆå¹¶ä¿å­˜äº†checkpoint")
        print("2. æŸ¥çœ‹ outputs/dales/checkpoints/ ç›®å½•")
        print("3. ä¿®æ”¹ CHECKPOINT_PATH ä¸ºå®é™…çš„æ–‡ä»¶è·¯å¾„")
        return
    
    # ä» checkpoint åŠ è½½æ¨¡å‹
    # æ³¨æ„ï¼šload_from_checkpoint éœ€è¦çŸ¥é“å¦‚ä½•å®ä¾‹åŒ– backbone å’Œ head
    # å¦‚æœ checkpoint ä¸­ä¿å­˜äº†è¿™äº›å‚æ•°ï¼ˆé€šè¿‡ save_hyperparametersï¼‰ï¼Œåˆ™ä¼šè‡ªåŠ¨åŠ è½½
    # ä½†å¦‚æœ backbone å’Œ head æ˜¯ä½œä¸ºå¯¹è±¡ä¼ å…¥ __init__ çš„ï¼ŒPL å¯èƒ½æ— æ³•è‡ªåŠ¨é‡å»ºå®ƒä»¬
    # å› æ­¤æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å®ä¾‹åŒ–å®ƒä»¬å¹¶ä¼ å…¥
    
    # 1. å…ˆå®ä¾‹åŒ– backbone å’Œ head (ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é…ç½®)
    # è¿™é‡Œæˆ‘ä»¬å‡è®¾ä½¿ç”¨ PointTransformerV2 å’Œ SegHeadï¼Œå‚æ•°éœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´
    # å¦‚æœä½ ä¸ç¡®å®šå‚æ•°ï¼Œå¯ä»¥æŸ¥çœ‹ hparams.yaml æˆ– checkpoint ä¸­çš„ hyper_parameters
    
    # ä¸ºäº†é€šç”¨æ€§ï¼Œæˆ‘ä»¬å°è¯•ç›´æ¥åŠ è½½ã€‚å¦‚æœå¤±è´¥ï¼Œè¯´æ˜éœ€è¦æ‰‹åŠ¨ä¼ å…¥ backbone/head
    try:
        task = SemanticSegmentationTask.load_from_checkpoint(
            CHECKPOINT_PATH,
            strict=False
        )
    except TypeError as e:
        print(f"\nâš ï¸  è‡ªåŠ¨åŠ è½½å¤±è´¥: {e}")
        print("å°è¯•æ‰‹åŠ¨å®ä¾‹åŒ– Backbone å’Œ Head...")
        
        # è¿™é‡Œéœ€è¦ç¡¬ç¼–ç è®­ç»ƒæ—¶çš„é…ç½®ï¼Œæˆ–è€…ä»é…ç½®æ–‡ä»¶è¯»å–
        # å‡è®¾æ˜¯ DALES çš„é…ç½®ï¼š
        backbone = PointTransformerV2(
            in_channels=4,  # coord(3) + intensity(1)
            num_classes=8,
            patch_embed_depth=2,
            enc_depths=[2, 2, 6, 2],
            dec_depths=[1, 1, 1, 1],
            enc_channels=[32, 64, 128, 256],
            dec_channels=[32, 64, 128, 256],
            num_heads=[2, 4, 8, 16],
            patch_embed_channels=32,
            grid_size=0.05,
            in_grid_size=0.02
        )
        
        head = SegHead(
            in_channels=32,
            num_classes=8,
            dropout=0.5
        )
        
        task = SemanticSegmentationTask.load_from_checkpoint(
            CHECKPOINT_PATH,
            backbone=backbone,
            head=head,
            strict=False
        )

    # ä»æ¨¡å‹ä¸­æå–é…ç½®ä¿¡æ¯
    class_mapping = task.hparams.get('class_mapping')
    class_names = task.hparams.get('class_names')
    
    # å°è¯•è·å–ç±»åˆ«æ•°é‡
    if hasattr(task.head, 'num_classes'):
        num_classes = task.head.num_classes
    elif class_mapping:
        num_classes = len(set(class_mapping.values()))
    else:
        num_classes = -1  # è®© Writer è‡ªåŠ¨æ¨æ–­
        
    # æ„å»ºåå‘æ˜ å°„ (ç”¨äºå°†é¢„æµ‹ç»“æœæ˜ å°„å›åŸå§‹æ ‡ç­¾)
    reverse_mapping = None
    if class_mapping:
        reverse_mapping = {v: k for k, v in class_mapping.items()}
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"  - è‡ªåŠ¨æå–ç±»åˆ«æ•°: {num_classes}")
    print(f"  - è‡ªåŠ¨æå– Class Mapping: {class_mapping is not None}")
    
    # ========================================================================
    # 2. æ•°æ®æ¨¡å— - Predict transformsï¼ˆä¸è¦æ•°æ®å¢å¼ºï¼ï¼‰
    # ========================================================================
    
    predict_transforms = [
        CenterShift(),
        ToTensor(),
        Collect(keys=['coord', 'feat'], feat_keys=['coord', 'intensity'])
    ]
    
    # é¢„æµ‹æ—¶ä¸éœ€è¦åŠ è½½ classificationï¼Œä¹Ÿä¸éœ€è¦ class_mapping
    # åªè¦æä¾› coord å’Œ intensity ç»™æ¨¡å‹å³å¯
    datamodule = BinPklDataModule(
        predict_data=TEST_DATA,
        assets=['coord', 'intensity'],  # ä»…åŠ è½½éœ€è¦çš„ç‰¹å¾ï¼Œä¸åŠ è½½æ ‡ç­¾
        batch_size=1,  # Predict æ—¶batch_sizeæ— å…³ç´§è¦
        num_workers=NUM_WORKERS,
        # class_mapping=None,  # é¢„æµ‹ä¸éœ€è¦æ˜ å°„è¾“å…¥æ ‡ç­¾
        # ignore_label=None,   # é¢„æµ‹ä¸éœ€è¦ ignore_label
        predict_loop=1,
        predict_transforms=predict_transforms,
        use_dynamic_batch_inference=True,
        max_points_inference=MAX_POINTS_INFERENCE,
        pin_memory=True,
    )
    
    # ========================================================================
    # 3. Trainer å’Œ Callbacks
    # ========================================================================
    
    callbacks = [
        SemanticPredictLasWriter(
            output_dir=OUTPUT_DIR, 
            num_classes=num_classes,
            save_logits=False, 
            reverse_class_mapping=reverse_mapping, # ä¼ å…¥ä»æ¨¡å‹æå–çš„åå‘æ˜ å°„
            auto_infer_reverse_mapping=False #æ—¢ç„¶å·²ç»ä¼ å…¥äº†ï¼Œå°±ä¸éœ€è¦è‡ªåŠ¨æ¨æ–­äº†
        ),
        TextLoggingCallback(log_interval=10),
    ]
    
    trainer = pl.Trainer(
        devices=1,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        precision="bf16-mixed", # ä½¿ç”¨ bf16-mixed åŠ é€Ÿé¢„æµ‹ (ä¸è®­ç»ƒä¸€è‡´)
        logger=False,  # Predict æ—¶ä¸éœ€è¦ logger
        callbacks=callbacks,
        enable_progress_bar=False,  # ä½¿ç”¨ TextLoggingCallback ä»£æ›¿
        enable_model_summary=False,
    )
    
    print(f"\nè®¾å¤‡: {trainer.accelerator}")
    print(f"ç²¾åº¦: {trainer.precision}")
    print(f"æ¨ç†é…ç½®: max_points={MAX_POINTS_INFERENCE/1000:.0f}K | workers={NUM_WORKERS}")
    
    # ========================================================================
    # 4. è¿è¡Œé¢„æµ‹
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("å¼€å§‹é¢„æµ‹")
    print("=" * 80)
    
    trainer.predict(task, datamodule)
    
    print("\n" + "=" * 80)
    print("é¢„æµ‹å®Œæˆï¼")
    print("=" * 80)
    print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print(f"\nè¯·ä½¿ç”¨ CloudCompare æˆ–å…¶ä»–å·¥å…·æŸ¥çœ‹ç”Ÿæˆçš„ .las æ–‡ä»¶")
    print("æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒæ˜¯å¦æ­£å¸¸ï¼ˆä¸åº”è¯¥99%æ˜¯ä¸€ä¸ªç±»åˆ«ï¼‰")


if __name__ == '__main__':
    main()
