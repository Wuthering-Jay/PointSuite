"""
ä»…è¿è¡Œé¢„æµ‹çš„è„šæœ¬ - ä½¿ç”¨å·²è®­ç»ƒçš„checkpoint

ç”¨æ³•ï¼š
    python predict_dales.py

éœ€è¦ä¿®æ”¹çš„é…ç½®ï¼š
    - CHECKPOINT_PATH: ä½ çš„checkpointè·¯å¾„
    - TEST_DATA: æµ‹è¯•æ•°æ®è·¯å¾„
    - OUTPUT_DIR: é¢„æµ‹ç»“æœè¾“å‡ºè·¯å¾„
"""

import os
import sys
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pointsuite.data import BinPklDataModule
from pointsuite.data.transforms import ToTensor, Collect
from pointsuite.tasks import SemanticSegmentationTask
from pointsuite.models import PointTransformerV2, SegHead
from pointsuite.utils.callbacks import SegmentationWriter
from pointsuite.utils.progress_bar import CustomProgressBar


def main():
    # ========================================================================
    # é…ç½® - æ ¹æ®ä½ çš„å®é™…æƒ…å†µä¿®æ”¹
    # ========================================================================
    
    # ğŸ”¥ é‡è¦ï¼šä¿®æ”¹ä¸ºä½ å®é™…çš„è·¯å¾„
    CHECKPOINT_PATH = r"outputs/dales/checkpoints/dales-epoch=XX-mean_iou=0.XXXX.ckpt"  # ä¿®æ”¹è¿™é‡Œï¼
    TEST_DATA = r"E:\data\DALES\dales_las\bin\test"  # ä¿®æ”¹è¿™é‡Œï¼
    OUTPUT_DIR = r"E:\data\DALES\dales_las\bin\result"  # ä¿®æ”¹è¿™é‡Œï¼
    
    CLASS_MAPPING = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7}
    CLASS_NAMES = ['åœ°é¢', 'æ¤è¢«', 'è½¦è¾†', 'å¡è½¦', 'ç”µçº¿', 'ç¯±ç¬†', 'æ†çŠ¶ç‰©', 'å»ºç­‘']
    NUM_CLASSES = 8
    IGNORE_LABEL = -1
    
    # Predict é…ç½®
    NUM_WORKERS = 4
    MAX_POINTS_INFERENCE = 300000  # æ¨ç†æ—¶ä½¿ç”¨æ›´å¤§batch
    
    pl.seed_everything(42)
    
    if torch.cuda.is_available():
        torch.set_float32_matmul_precision('high')
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    
    print("\n" + "=" * 80)
    print(f"DALES é¢„æµ‹ - ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹")
    print("=" * 80)
    print(f"Checkpoint: {CHECKPOINT_PATH}")
    print(f"æµ‹è¯•æ•°æ®: {TEST_DATA}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # ========================================================================
    # æ•°æ®æ¨¡å— - Predict transformsï¼ˆä¸è¦æ•°æ®å¢å¼ºï¼ï¼‰
    # ========================================================================
    
    predict_transforms = [
        ToTensor(),
        Collect(keys=['coord', 'feat'], feat_keys=['coord', 'intensity'])
    ]
    
    datamodule = BinPklDataModule(
        predict_data=TEST_DATA,
        assets=['coord', 'intensity', 'classification'],
        batch_size=1,  # Predict æ—¶batch_sizeæ— å…³ç´§è¦
        num_workers=NUM_WORKERS,
        class_mapping=CLASS_MAPPING,
        class_names=CLASS_NAMES,
        ignore_label=IGNORE_LABEL,
        predict_loop=1,
        predict_transforms=predict_transforms,
        use_dynamic_batch_inference=True,
        max_points_inference=MAX_POINTS_INFERENCE,
        pin_memory=True,
    )
    
    # ========================================================================
    # ä» Checkpoint åŠ è½½æ¨¡å‹
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("ä» Checkpoint åŠ è½½æ¨¡å‹...")
    print("=" * 80)
    
    # æ£€æŸ¥ checkpoint æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"\nâŒ é”™è¯¯: Checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {CHECKPOINT_PATH}")
        print("\nè¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹ï¼š")
        print("1. ç¡®è®¤è®­ç»ƒæ˜¯å¦å®Œæˆå¹¶ä¿å­˜äº†checkpoint")
        print("2. æŸ¥çœ‹ outputs/dales/checkpoints/ ç›®å½•")
        print("3. ä¿®æ”¹ CHECKPOINT_PATH ä¸ºå®é™…çš„æ–‡ä»¶è·¯å¾„")
        return
    
    # ä» checkpoint åŠ è½½æ¨¡å‹
    task = SemanticSegmentationTask.load_from_checkpoint(
        CHECKPOINT_PATH,
        strict=False  # å¦‚æœæ¨¡å‹ç»“æ„ç•¥æœ‰ä¸åŒï¼Œä½¿ç”¨ strict=False
    )
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"  - ç±»åˆ«æ•°: {NUM_CLASSES}")
    print(f"  - Class Mapping: {CLASS_MAPPING}")
    
    # ========================================================================
    # Trainer å’Œ Callbacks
    # ========================================================================
    
    callbacks = [
        SegmentationWriter(
            output_dir=OUTPUT_DIR, 
            save_logits=False, 
            auto_infer_reverse_mapping=True
        ),
        CustomProgressBar(refresh_rate=1),
    ]
    
    trainer = pl.Trainer(
        devices=1,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        precision="32-true",
        logger=False,  # Predict æ—¶ä¸éœ€è¦ logger
        callbacks=callbacks,
        enable_progress_bar=True,
        enable_model_summary=False,
    )
    
    print(f"\nè®¾å¤‡: {trainer.accelerator}")
    print(f"æ¨ç†é…ç½®: max_points={MAX_POINTS_INFERENCE/1000:.0f}K | workers={NUM_WORKERS}")
    
    # ========================================================================
    # è¿è¡Œé¢„æµ‹
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("å¼€å§‹é¢„æµ‹")
    print("=" * 80)
    
    trainer.predict(task, datamodule)
    
    print("\n" + "=" * 80)
    print("é¢„æµ‹å®Œæˆï¼")
    print("=" * 80)
    print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print(f"\nè¯·ä½¿ç”¨ CloudCompare æˆ–å…¶ä»–å·¥å…·æŸ¥çœ‹ç”Ÿæˆçš„ .las æ–‡ä»¶")
    print("æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒæ˜¯å¦æ­£å¸¸ï¼ˆä¸åº”è¯¥99%æ˜¯ä¸€ä¸ªç±»åˆ«ï¼‰")


if __name__ == '__main__':
    main()
