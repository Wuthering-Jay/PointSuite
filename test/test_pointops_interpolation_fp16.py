"""
æµ‹è¯• pointops.interpolation æ˜¯å¦æ”¯æŒ FP16
"""

import torch
import pointops

def test_interpolation_fp16():
    """æµ‹è¯• pointops.interpolation åœ¨ FP16 ä¸‹æ˜¯å¦å·¥ä½œ"""
    print("\n" + "="*80)
    print("æµ‹è¯• pointops.interpolation FP16 æ”¯æŒ")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    n1, n2 = 1000, 2000
    c = 64
    
    # æºç‚¹äº‘å’Œç›®æ ‡ç‚¹äº‘
    coord1 = torch.randn(n1, 3).cuda()
    coord2 = torch.randn(n2, 3).cuda()
    feat = torch.randn(n1, c).cuda()
    
    offset1 = torch.tensor([n1], dtype=torch.long).cuda()
    offset2 = torch.tensor([n2], dtype=torch.long).cuda()
    
    # æµ‹è¯• 1: FP32
    print("\n" + "-"*80)
    print("æµ‹è¯• 1: FP32 (åŸå§‹ç²¾åº¦)")
    print("-"*80)
    try:
        result_fp32 = pointops.interpolation(
            coord1.float(), coord2.float(), feat.float(), offset1, offset2
        )
        print(f"âœ… FP32 æˆåŠŸ")
        print(f"  è¾“å…¥ feat: {feat.float().shape}, dtype={feat.float().dtype}")
        print(f"  è¾“å‡º: {result_fp32.shape}, dtype={result_fp32.dtype}")
        print(f"  è¾“å‡ºèŒƒå›´: [{result_fp32.min().item():.4f}, {result_fp32.max().item():.4f}]")
    except Exception as e:
        print(f"âŒ FP32 å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯• 2: FP16 è¾“å…¥
    print("\n" + "-"*80)
    print("æµ‹è¯• 2: FP16 è¾“å…¥ (coord + feat éƒ½æ˜¯ FP16)")
    print("-"*80)
    try:
        result_fp16 = pointops.interpolation(
            coord1.half(), coord2.half(), feat.half(), offset1, offset2
        )
        print(f"âœ… FP16 è¾“å…¥æˆåŠŸ")
        print(f"  è¾“å…¥ feat: {feat.half().shape}, dtype={feat.half().dtype}")
        print(f"  è¾“å‡º: {result_fp16.shape}, dtype={result_fp16.dtype}")
        print(f"  è¾“å‡ºèŒƒå›´: [{result_fp16.min().item():.4f}, {result_fp16.max().item():.4f}]")
    except Exception as e:
        print(f"âŒ FP16 è¾“å…¥å¤±è´¥: {e}")
        print(f"   å¯èƒ½åŸå› : CUDA kernel ä¸æ”¯æŒ FP16")
        return False
    
    # æµ‹è¯• 3: FP16 + AMP
    print("\n" + "-"*80)
    print("æµ‹è¯• 3: åœ¨ AMP ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨")
    print("-"*80)
    try:
        with torch.cuda.amp.autocast(enabled=True):
            # è¾“å…¥æ˜¯ FP32ï¼Œautocast ä¼šè‡ªåŠ¨å¤„ç†
            result_amp = pointops.interpolation(
                coord1, coord2, feat, offset1, offset2
            )
        print(f"âœ… AMP è‡ªåŠ¨è½¬æ¢æˆåŠŸ")
        print(f"  è¾“å‡º: {result_amp.shape}, dtype={result_amp.dtype}")
        print(f"  è¾“å‡ºèŒƒå›´: [{result_amp.min().item():.4f}, {result_amp.max().item():.4f}]")
    except Exception as e:
        print(f"âŒ AMP å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯• 4: æ¢¯åº¦æµ‹è¯•
    print("\n" + "-"*80)
    print("æµ‹è¯• 4: FP16 åå‘ä¼ æ’­")
    print("-"*80)
    try:
        feat_grad = feat.half().clone().requires_grad_(True)
        result = pointops.interpolation(
            coord1.half(), coord2.half(), feat_grad, offset1, offset2
        )
        loss = result.sum()
        loss.backward()
        
        print(f"âœ… FP16 åå‘ä¼ æ’­æˆåŠŸ")
        print(f"  æ¢¯åº¦ shape: {feat_grad.grad.shape}, dtype={feat_grad.grad.dtype}")
        print(f"  æ¢¯åº¦èŒƒå›´: [{feat_grad.grad.min().item():.4f}, {feat_grad.grad.max().item():.4f}]")
        print(f"  æ˜¯å¦æœ‰ NaN: {torch.isnan(feat_grad.grad).any().item()}")
    except Exception as e:
        print(f"âŒ FP16 åå‘ä¼ æ’­å¤±è´¥: {e}")
        return False
    
    print("\n" + "="*80)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! pointops.interpolation å®Œå…¨æ”¯æŒ FP16")
    print("="*80)
    return True

if __name__ == "__main__":
    success = test_interpolation_fp16()
    if success:
        print("\nç»“è®º: pointops.interpolation åŸç”Ÿæ”¯æŒ FP16ï¼Œä¸éœ€è¦é¢å¤–çš„ç±»å‹è½¬æ¢!")
    else:
        print("\nç»“è®º: pointops.interpolation ä¸æ”¯æŒ FP16ï¼Œéœ€è¦è½¬æ¢åˆ° FP32")
