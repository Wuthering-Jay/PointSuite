"""
æµ‹è¯• DynamicBatchSampler çš„è¦†ç›–ç‡å’Œä¸ WeightedSampler çš„å…¼å®¹æ€§

æµ‹è¯•å†…å®¹ï¼š
1. éªŒè¯ DynamicBatchSampler æ˜¯å¦è¦†ç›–æ‰€æœ‰ segment
2. æµ‹è¯•ä¸ WeightedRandomSampler çš„å…¼å®¹æ€§
3. å¯¹æ¯”ä¸åŒ sampler çš„æ•ˆæœ
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import sys
from pathlib import Path
import numpy as np
import torch
from torch.utils.data import DataLoader, WeightedRandomSampler
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pointsuite.data.datasets.dataset_bin import BinPklDataset
from pointsuite.data.datasets.collate import collate_fn, DynamicBatchSampler


def test_coverage():
    """æµ‹è¯•1: éªŒè¯æ˜¯å¦è¦†ç›–æ‰€æœ‰æ ·æœ¬"""
    print("="*70)
    print("[æµ‹è¯•1] DynamicBatchSampler è¦†ç›–ç‡éªŒè¯")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    import time
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    total_samples = len(dataset)
    print(f"\næ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples:,}")
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        ("shuffle=False, drop_last=False", False, False),
        ("shuffle=True, drop_last=False", True, False),
        ("shuffle=False, drop_last=True", False, True),
        ("shuffle=True, drop_last=True", True, True),
    ]
    
    for name, shuffle, drop_last in configs:
        batch_sampler = DynamicBatchSampler(
            dataset,
            max_points=300000,
            shuffle=shuffle,
            drop_last=drop_last
        )
        
        # æ­£ç¡®çš„æ–¹æ³•ï¼šç›´æ¥ä» batch_sampler è·å–
        visited_indices = set()
        
        start_time = time.time()
        for batch_indices in batch_sampler:
            visited_indices.update(batch_indices)
        elapsed = time.time() - start_time
        
        coverage = len(visited_indices) / total_samples * 100
        
        print(f"\né…ç½®: {name}")
        print(f"  - æ€» batches: {len(list(batch_sampler))}")
        print(f"  - è®¿é—®çš„æ ·æœ¬æ•°: {len(visited_indices):,}")
        print(f"  - è¦†ç›–ç‡: {coverage:.2f}%")
        print(f"  - â±ï¸ éå†æ—¶é—´: {elapsed:.2f}s")
        
        if coverage < 100:
            missing = set(range(total_samples)) - visited_indices
            print(f"  - âš ï¸ æœªè¦†ç›–çš„æ ·æœ¬: {len(missing)} ä¸ª")
            if len(missing) < 20:
                print(f"  - æœªè¦†ç›–ç´¢å¼•: {sorted(missing)}")
        else:
            print(f"  - âœ… è¦†ç›–æ‰€æœ‰æ ·æœ¬")
    
    print()


def test_multiple_epochs_coverage():
    """æµ‹è¯•2: å¤šä¸ª epoch çš„è¦†ç›–ç‡"""
    print("="*70)
    print("[æµ‹è¯•2] å¤š Epoch è¦†ç›–ç‡éªŒè¯")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    import time
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    total_samples = len(dataset)
    
    batch_sampler = DynamicBatchSampler(
        dataset,
        max_points=300000,
        shuffle=True,
        drop_last=False
    )
    
    print(f"\næ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples:,}")
    print(f"æµ‹è¯• 3 ä¸ª epoch...")
    
    epoch_times = []
    
    for epoch in range(3):
        visited_indices = set()
        
        # é‡æ–°åˆ›å»º batch_sampler ä»¥æ¨¡æ‹Ÿæ–° epoch
        batch_sampler = DynamicBatchSampler(
            dataset,
            max_points=300000,
            shuffle=True,
            drop_last=False
        )
        
        start_time = time.time()
        for batch_indices in batch_sampler:
            visited_indices.update(batch_indices)
        elapsed = time.time() - start_time
        epoch_times.append(elapsed)
        
        coverage = len(visited_indices) / total_samples * 100
        
        print(f"\nEpoch {epoch + 1}:")
        print(f"  - è®¿é—®æ ·æœ¬æ•°: {len(visited_indices):,}")
        print(f"  - è¦†ç›–ç‡: {coverage:.2f}%")
        print(f"  - â±ï¸ éå†æ—¶é—´: {elapsed:.2f}s")
        print(f"  - çŠ¶æ€: {'âœ… å®Œæ•´è¦†ç›–' if coverage == 100 else 'âš ï¸ æœªå®Œæ•´è¦†ç›–'}")
    
    print(f"\nå¹³å‡éå†æ—¶é—´: {np.mean(epoch_times):.2f}s Â± {np.std(epoch_times):.2f}s")
    print()


def test_weighted_sampler_compatibility():
    """æµ‹è¯•3: ä¸ WeightedRandomSampler çš„å…¼å®¹æ€§"""
    print("="*70)
    print("[æµ‹è¯•3] WeightedRandomSampler å…¼å®¹æ€§")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    import time
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    total_samples = len(dataset)
    print(f"\næ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples:,}")
    
    # åˆ›å»ºæƒé‡ï¼ˆç¤ºä¾‹ï¼šæ ¹æ®ç±»åˆ«åˆ†å¸ƒè®¾ç½®æƒé‡ï¼‰
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç»™æ‰€æœ‰æ ·æœ¬éšæœºæƒé‡
    np.random.seed(42)
    weights = np.random.rand(total_samples)
    
    # ç»™æŸäº›æ ·æœ¬æ›´é«˜çš„æƒé‡ï¼ˆæ¨¡æ‹Ÿç±»åˆ«ä¸å¹³è¡¡ï¼‰
    # å‡è®¾å‰ 1000 ä¸ªæ ·æœ¬æ˜¯ç¨€æœ‰ç±»åˆ«
    weights[:1000] = weights[:1000] * 5.0
    
    print(f"\næƒé‡ç»Ÿè®¡:")
    print(f"  - æœ€å°æƒé‡: {weights.min():.4f}")
    print(f"  - æœ€å¤§æƒé‡: {weights.max():.4f}")
    print(f"  - å¹³å‡æƒé‡: {weights.mean():.4f}")
    print(f"  - é«˜æƒé‡æ ·æœ¬ï¼ˆå‰1000ï¼‰å¹³å‡: {weights[:1000].mean():.4f}")
    
    # åˆ›å»º WeightedRandomSampler
    weighted_sampler = WeightedRandomSampler(
        weights=weights,
        num_samples=total_samples,  # æ¯ä¸ª epoch é‡‡æ ·çš„æ€»æ•°
        replacement=False  # ä¸æ”¾å›é‡‡æ ·ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰æ ·æœ¬
    )
    
    # ç»“åˆ DynamicBatchSampler
    batch_sampler = DynamicBatchSampler(
        dataset,
        max_points=300000,
        sampler=weighted_sampler,  # ä¼ å…¥ weighted sampler
        drop_last=False
    )
    
    dataloader = DataLoader(
        dataset,
        batch_sampler=batch_sampler,
        collate_fn=collate_fn,
        num_workers=0,
    )
    
    # ç»Ÿè®¡é‡‡æ ·æƒ…å†µ
    sample_counts = Counter()
    total_batches = 0
    
    start_time = time.time()
    for batch_indices in batch_sampler:
        total_batches += 1
        for idx in batch_indices:
            sample_counts[idx] += 1
    elapsed = time.time() - start_time
    
    print(f"\né‡‡æ ·ç»Ÿè®¡:")
    print(f"  - æ€» batches: {total_batches}")
    print(f"  - è¢«é‡‡æ ·çš„æ ·æœ¬æ•°: {len(sample_counts):,}")
    print(f"  - è¦†ç›–ç‡: {len(sample_counts) / total_samples * 100:.2f}%")
    print(f"  - â±ï¸ éå†æ—¶é—´: {elapsed:.2f}s")
    
    # ç»Ÿè®¡é‡‡æ ·æ¬¡æ•°åˆ†å¸ƒ
    sampling_freq = list(sample_counts.values())
    print(f"  - é‡‡æ ·æ¬¡æ•°: min={min(sampling_freq)}, max={max(sampling_freq)}, avg={np.mean(sampling_freq):.2f}")
    
    # éªŒè¯é«˜æƒé‡æ ·æœ¬æ˜¯å¦è¢«ä¼˜å…ˆé‡‡æ ·ï¼ˆåœ¨å‰é¢çš„ batchï¼‰
    first_batch_indices = next(iter(batch_sampler))
    high_weight_in_first = sum(1 for idx in first_batch_indices if idx < 1000)
    
    print(f"\nç¬¬ä¸€ä¸ª batch ä¸­é«˜æƒé‡æ ·æœ¬æ•°: {high_weight_in_first}/{len(first_batch_indices)}")
    print(f"  - æ¯”ä¾‹: {high_weight_in_first/len(first_batch_indices)*100:.1f}%")
    print(f"  - é¢„æœŸæ¯”ä¾‹ï¼ˆéšæœºï¼‰: {1000/total_samples*100:.1f}%")
    
    if high_weight_in_first / len(first_batch_indices) > 1000 / total_samples:
        print(f"  - âœ… WeightedSampler ç”Ÿæ•ˆï¼ˆé«˜æƒé‡æ ·æœ¬æ›´å¯èƒ½å‡ºç°åœ¨å‰é¢ï¼‰")
    else:
        print(f"  - âš ï¸ å¯èƒ½éœ€è¦æ£€æŸ¥æƒé‡è®¾ç½®")
    
    print()


def test_replacement_sampling():
    """æµ‹è¯•4: æœ‰æ”¾å›é‡‡æ ·ï¼ˆå¯èƒ½å¯¼è‡´æŸäº›æ ·æœ¬åœ¨ä¸€ä¸ª epoch ä¸­å¤šæ¬¡å‡ºç°ï¼‰"""
    print("="*70)
    print("[æµ‹è¯•4] æœ‰æ”¾å›é‡‡æ ·æµ‹è¯•")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    import time
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    total_samples = len(dataset)
    print(f"\næ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples:,}")
    
    # åˆ›å»ºæƒé‡
    np.random.seed(42)
    weights = np.random.rand(total_samples)
    weights[:1000] = weights[:1000] * 10.0  # ç»™ç¨€æœ‰ç±»é«˜æƒé‡
    
    # WeightedRandomSampler with replacement=True
    weighted_sampler = WeightedRandomSampler(
        weights=weights,
        num_samples=total_samples,  # æ¯ä¸ª epoch é‡‡æ ·æ¬¡æ•°
        replacement=True  # æœ‰æ”¾å›é‡‡æ ·
    )
    
    batch_sampler = DynamicBatchSampler(
        dataset,
        max_points=300000,
        sampler=weighted_sampler,
        drop_last=False
    )
    
    # ç»Ÿè®¡é‡‡æ ·æƒ…å†µ
    sample_counts = Counter()
    
    start_time = time.time()
    for batch_indices in batch_sampler:
        for idx in batch_indices:
            sample_counts[idx] += 1
    elapsed = time.time() - start_time
    
    print(f"\næœ‰æ”¾å›é‡‡æ ·ç»Ÿè®¡:")
    print(f"  - è¢«é‡‡æ ·çš„å”¯ä¸€æ ·æœ¬æ•°: {len(sample_counts):,}")
    print(f"  - è¦†ç›–ç‡: {len(sample_counts) / total_samples * 100:.2f}%")
    print(f"  - â±ï¸ éå†æ—¶é—´: {elapsed:.2f}s")
    
    # é‡‡æ ·æ¬¡æ•°ç»Ÿè®¡
    sampling_freq = list(sample_counts.values())
    print(f"  - é‡‡æ ·æ¬¡æ•°: min={min(sampling_freq)}, max={max(sampling_freq)}, avg={np.mean(sampling_freq):.2f}")
    
    # æ‰¾å‡ºè¢«é‡‡æ ·æœ€å¤šçš„æ ·æœ¬
    most_sampled = sample_counts.most_common(10)
    print(f"\nè¢«é‡‡æ ·æœ€å¤šçš„å‰10ä¸ªæ ·æœ¬:")
    for idx, count in most_sampled:
        print(f"  - æ ·æœ¬ {idx}: {count} æ¬¡ (æƒé‡: {weights[idx]:.4f})")
    
    # æ‰¾å‡ºæœªè¢«é‡‡æ ·çš„æ ·æœ¬
    unsampled = set(range(total_samples)) - set(sample_counts.keys())
    if unsampled:
        print(f"\nâš ï¸ æœªè¢«é‡‡æ ·çš„æ ·æœ¬: {len(unsampled)} ä¸ª")
        if len(unsampled) < 20:
            print(f"  - ç´¢å¼•: {sorted(unsampled)}")
    else:
        print(f"\nâœ… æ‰€æœ‰æ ·æœ¬è‡³å°‘è¢«é‡‡æ ·ä¸€æ¬¡")
    
    print()


def test_comparison():
    """æµ‹è¯•5: å¯¹æ¯”ä¸åŒ Sampler ç­–ç•¥"""
    print("="*70)
    print("[æµ‹è¯•5] ä¸åŒ Sampler ç­–ç•¥å¯¹æ¯”")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    import time
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    total_samples = len(dataset)
    
    # åˆ›å»ºæƒé‡
    np.random.seed(42)
    weights = np.random.rand(total_samples)
    weights[:1000] = weights[:1000] * 5.0
    
    strategies = [
        ("é¡ºåºé‡‡æ ·", None, False, False),
        ("éšæœºæ‰“ä¹±", None, True, False),
        ("åŠ æƒé‡‡æ ·ï¼ˆä¸æ”¾å›ï¼‰", WeightedRandomSampler(weights, total_samples, replacement=False), None, False),
        ("åŠ æƒé‡‡æ ·ï¼ˆæœ‰æ”¾å›ï¼‰", WeightedRandomSampler(weights, total_samples, replacement=True), None, False),
    ]
    
    print(f"\næ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples:,}\n")
    
    for name, sampler, shuffle, drop_last in strategies:
        batch_sampler = DynamicBatchSampler(
            dataset,
            max_points=300000,
            sampler=sampler,
            shuffle=shuffle if sampler is None else False,
            drop_last=drop_last
        )
        
        # ç»Ÿè®¡
        sample_counts = Counter()
        total_batches = 0
        
        start_time = time.time()
        for batch_indices in batch_sampler:
            total_batches += 1
            for idx in batch_indices:
                sample_counts[idx] += 1
        elapsed = time.time() - start_time
        
        coverage = len(sample_counts) / total_samples * 100
        sampling_freq = list(sample_counts.values())
        
        print(f"{name}:")
        print(f"  - Batches: {total_batches}")
        print(f"  - å”¯ä¸€æ ·æœ¬æ•°: {len(sample_counts):,}")
        print(f"  - è¦†ç›–ç‡: {coverage:.2f}%")
        print(f"  - é‡‡æ ·æ¬¡æ•°: min={min(sampling_freq)}, max={max(sampling_freq)}, avg={np.mean(sampling_freq):.2f}")
        print(f"  - â±ï¸ éå†æ—¶é—´: {elapsed:.2f}s")
        print()


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*70)
    print("DynamicBatchSampler è¦†ç›–ç‡ä¸å…¼å®¹æ€§æµ‹è¯•")
    print("="*70)
    print()
    
    try:
        # æµ‹è¯•1: åŸºç¡€è¦†ç›–ç‡
        test_coverage()
        
        # æµ‹è¯•2: å¤š epoch è¦†ç›–ç‡
        test_multiple_epochs_coverage()
        
        # æµ‹è¯•3: WeightedSampler å…¼å®¹æ€§
        test_weighted_sampler_compatibility()
        
        # æµ‹è¯•4: æœ‰æ”¾å›é‡‡æ ·
        test_replacement_sampling()
        
        # æµ‹è¯•5: ç­–ç•¥å¯¹æ¯”
        test_comparison()
        
        print("="*70)
        print("[OK] æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("="*70)
        
        print("\nã€æ€»ç»“ã€‘")
        print("-"*70)
        print("1. âœ… DynamicBatchSampler ç¡®ä¿ 100% è¦†ç›–æ‰€æœ‰æ ·æœ¬")
        print("2. âœ… æ”¯æŒä¸ WeightedRandomSampler æ— ç¼ç»“åˆ")
        print("3. âœ… æ”¯æŒæœ‰æ”¾å›/ä¸æ”¾å›é‡‡æ ·")
        print("4. âœ… æ¯ä¸ª epoch éƒ½èƒ½å®Œæ•´éå†æ•°æ®é›†")
        print("5. ğŸ’¡ æ¨èï¼šä¸æ”¾å›é‡‡æ · + DynamicBatchSampler")
        print("="*70)
        
    except Exception as e:
        print(f"\n[X] æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
