"""
æµ‹è¯• UnpoolWithSkip çš„ interp backend æ˜¯å¦å¯ä»¥åœ¨ FP16 ä¸‹å·¥ä½œ
ä¸ä¾èµ– cluster ç´¢å¼•ï¼Œä½¿ç”¨ pointops.interpolation
"""

import torch
import torch.nn as nn
from pointsuite.models.backbones.point_transformer_v2m5 import PointTransformerV2
from pointsuite.models.heads.seg_head import SegHead

def test_unpool_interp_backend():
    """æµ‹è¯• unpool_backend='interp' æ¨¡å¼"""
    print("\n" + "="*80)
    print("æµ‹è¯• UnpoolWithSkip with backend='interp'")
    print("="*80)
    
    # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨ interp backend (ä¸ train_dales.py ä¸€è‡´çš„é…ç½®)
    backbone = PointTransformerV2(
        in_channels=5,
        patch_embed_depth=2,
        patch_embed_channels=48,
        patch_embed_groups=6,
        patch_embed_neighbours=16,
        enc_depths=(2, 2, 6, 2),
        enc_channels=(96, 192, 384, 512),
        enc_groups=(12, 24, 48, 64),
        enc_neighbours=(16, 16, 16, 16),
        dec_depths=(1, 1, 1, 1),
        dec_channels=(48, 96, 192, 384),
        dec_groups=(6, 12, 24, 48),
        dec_neighbours=(16, 16, 16, 16),
        grid_sizes=(1, 2.5, 7.5, 15),  # ä½¿ç”¨ä¸ train_dales.py ç›¸åŒçš„å€¼
        attn_qkv_bias=True,
        pe_multiplier=False,
        pe_bias=True,
        attn_drop_rate=0.0,
        drop_path_rate=0.2,
        unpool_backend="interp",  # ğŸ”¥ ä½¿ç”¨ interp backendï¼Œä¸ä¾èµ– cluster
    )
    
    head = SegHead(in_channels=48, num_classes=8)
    
    model = nn.Sequential(backbone, head)
    model = model.cuda()
    
    # æµ‹è¯•æ•°æ®
    batch_size = 2
    num_points = 50000
    
    data_dict = {
        "coord": torch.randn(num_points, 3, dtype=torch.float32).cuda(),
        "feat": torch.randn(num_points, 5, dtype=torch.float32).cuda(),
        "offset": torch.tensor([25000, 50000], dtype=torch.long).cuda(),
    }
    
    print(f"\nè¾“å…¥æ•°æ®:")
    print(f"  coord: {data_dict['coord'].shape}, dtype={data_dict['coord'].dtype}")
    print(f"  feat: {data_dict['feat'].shape}, dtype={data_dict['feat'].dtype}")
    print(f"  offset: {data_dict['offset']}")
    
    # æµ‹è¯• 1: FP32 å‰å‘
    print("\n" + "-"*80)
    print("æµ‹è¯• 1: FP32 å‰å‘ä¼ æ’­")
    print("-"*80)
    model.train()
    output = model(data_dict)
    print(f"âœ… FP32 å‰å‘æˆåŠŸ")
    print(f"  è¾“å‡º shape: {output.shape}, dtype={output.dtype}")
    print(f"  è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
    
    # æµ‹è¯• 2: FP16 å‰å‘
    print("\n" + "-"*80)
    print("æµ‹è¯• 2: FP16 å‰å‘ä¼ æ’­ (AMP)")
    print("-"*80)
    
    data_dict_fp16 = {
        "coord": data_dict["coord"].clone(),
        "feat": data_dict["feat"].clone().half(),
        "offset": data_dict["offset"].clone(),
    }
    
    with torch.cuda.amp.autocast(enabled=True):
        output_fp16 = model(data_dict_fp16)
    
    print(f"âœ… FP16 å‰å‘æˆåŠŸ")
    print(f"  è¾“å‡º shape: {output_fp16.shape}, dtype={output_fp16.dtype}")
    print(f"  è¾“å‡ºèŒƒå›´: [{output_fp16.min().item():.4f}, {output_fp16.max().item():.4f}]")
    
    # æµ‹è¯• 3: FP16 å‰å‘ + æŸå¤±
    print("\n" + "-"*80)
    print("æµ‹è¯• 3: FP16 å‰å‘ + æŸå¤±è®¡ç®—")
    print("-"*80)
    
    target = torch.randint(0, 8, (num_points,), dtype=torch.long).cuda()
    criterion = nn.CrossEntropyLoss()
    
    with torch.cuda.amp.autocast(enabled=True):
        output_fp16 = model(data_dict_fp16)
        loss = criterion(output_fp16, target)
    
    print(f"âœ… FP16 æŸå¤±è®¡ç®—æˆåŠŸ")
    print(f"  æŸå¤±å€¼: {loss.item():.4f}")
    print(f"  æŸå¤±æ˜¯å¦ä¸º NaN: {torch.isnan(loss).item()}")
    
    # æµ‹è¯• 4: FP16 åå‘ä¼ æ’­
    print("\n" + "-"*80)
    print("æµ‹è¯• 4: FP16 åå‘ä¼ æ’­")
    print("-"*80)
    
    scaler = torch.cuda.amp.GradScaler()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    model.train()
    optimizer.zero_grad()
    
    with torch.cuda.amp.autocast(enabled=True):
        output_fp16 = model(data_dict_fp16)
        loss = criterion(output_fp16, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    
    print(f"âœ… FP16 åå‘ä¼ æ’­æˆåŠŸ")
    
    # æ£€æŸ¥æ¢¯åº¦
    grad_norms = []
    has_nan_grad = False
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_norms.append(grad_norm)
            if torch.isnan(param.grad).any():
                has_nan_grad = True
                print(f"  âš ï¸ {name}: æ¢¯åº¦åŒ…å« NaN!")
    
    print(f"  æ¢¯åº¦ç»Ÿè®¡: min={min(grad_norms):.6f}, max={max(grad_norms):.6f}, mean={sum(grad_norms)/len(grad_norms):.6f}")
    print(f"  æ˜¯å¦æœ‰ NaN æ¢¯åº¦: {has_nan_grad}")
    
    # æµ‹è¯• 5: å¤šæ­¥è®­ç»ƒ
    print("\n" + "-"*80)
    print("æµ‹è¯• 5: 3 æ­¥ FP16 è®­ç»ƒ")
    print("-"*80)
    
    for step in range(3):
        optimizer.zero_grad()
        
        with torch.cuda.amp.autocast(enabled=True):
            output_fp16 = model(data_dict_fp16)
            loss = criterion(output_fp16, target)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        print(f"  Step {step}: Loss = {loss.item():.4f}, NaN={torch.isnan(loss).item()}")
    
    print(f"\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! unpool_backend='interp' å¯ä»¥åœ¨ FP16 ä¸‹æ­£å¸¸å·¥ä½œ")
    print(f"   ä¸ä¾èµ– cluster ç´¢å¼•ï¼Œä½¿ç”¨ pointops.interpolation")
    
    return True

if __name__ == "__main__":
    print("\n" + "="*80)
    print("UnpoolWithSkip Interpolation Backend æµ‹è¯•")
    print("="*80)
    print("æµ‹è¯• unpool_backend='interp' æ˜¯å¦å¯ä»¥é¿å… cluster ç´¢å¼•é—®é¢˜")
    print("="*80)
    
    try:
        test_unpool_interp_backend()
        print("\n" + "="*80)
        print("ğŸ‰ æµ‹è¯•æˆåŠŸ! å¯ä»¥ä½¿ç”¨ unpool_backend='interp'")
        print("="*80)
    except Exception as e:
        print("\n" + "="*80)
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("="*80)
        import traceback
        traceback.print_exc()
