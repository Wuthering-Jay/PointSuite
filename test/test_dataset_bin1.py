"""
æµ‹è¯• BinPklDataset1 å’?BinPklDataModule1 çš„åŠŸèƒ?

æµ‹è¯•å†…å®¹ï¼?
1. å…¨é‡æ¨¡å¼å’Œä½“ç´ æ¨¡å¼çš„åŸºæœ¬åŠŸèƒ½
2. train/val éšæœºé‡‡æ · vs test/predict æ¨¡è¿ç®—é‡‡æ ?
3. ç‚¹äº‘å…¨è¦†ç›–éªŒè¯?
4. åŠ¨æ€æ‰¹å¤„ç†å…¼å®¹æ€?
5. ç±»åˆ«æ˜ å°„å’Œç±»åˆ«æƒé‡?
6. é€Ÿåº¦æµ‹è¯•ï¼ˆå•æ ·æœ¬ã€å¤šæ ·æœ¬éšæœºã€åŠ¨æ€æ‰¹å¤„ç†ï¼?
"""

import os
import sys
import time
import numpy as np
import pickle
from pathlib import Path
from collections import Counter
from typing import Dict, List, Tuple, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


class Colors:
    """ANSI é¢œè‰²ä»£ç """
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    RESET = '\033[0m'


def format_number(num: int) -> str:
    return f"{num:,}"


def format_percent(value: float) -> str:
    return f"{value:.2f}%"


def format_time(seconds: float) -> str:
    if seconds < 0.001:
        return f"{seconds*1000000:.1f}Î¼s"
    elif seconds < 1:
        return f"{seconds*1000:.2f}ms"
    else:
        return f"{seconds:.3f}s"


# ============================================================================
# æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½æµ‹è¯•
# ============================================================================

def test_dataset_basic(pkl_path: str, mode: str = 'voxel'):
    """
    æµ‹è¯• BinPklDataset1 åŸºæœ¬åŠŸèƒ½
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  ğŸ“‹ æµ‹è¯•1: BinPklDataset1 åŸºæœ¬åŠŸèƒ½æµ‹è¯• (mode={mode}){Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    # æµ‹è¯•ä¸åŒ split
    for split in ['train', 'val', 'test']:
        print(f"\n  {Colors.BOLD}ğŸ“¦ Split: {split}{Colors.RESET}")
        
        dataset = BinPklDataset1(
            data_root=pkl_path,
            split=split,
            mode=mode,
            assets=['coord', 'intensity', 'class'],
            max_loops=5 if split in ['test', 'predict'] else None
        )
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ ·æœ¬æ•? {Colors.CYAN}{len(dataset)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} åŸå§‹æ•°æ®åˆ—è¡¨é•¿åº¦: {Colors.CYAN}{len(dataset.data_list)}{Colors.RESET}")
        
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ?
        sample = dataset[0]
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ ·æœ¬ 0 çš?keys: {list(sample.keys())}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} coord shape: {sample['coord'].shape}")
        
        if 'intensity' in sample:
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} intensity range: [{sample['intensity'].min():.3f}, {sample['intensity'].max():.3f}]")
        
        if 'class' in sample:
            unique_classes = np.unique(sample['class'])
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç±»åˆ«: {unique_classes}")
        
        if split == 'test' and 'indices' in sample:
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} indices shape: {sample['indices'].shape}")
            if 'loop_idx' in sample:
                print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} loop_idx: {sample['loop_idx']}")
        
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} {Colors.GREEN}âœ?é€šè¿‡{Colors.RESET}")
    
    return True


# ============================================================================
# æµ‹è¯•2: ä½“ç´ æ¨¡å¼å…¨è¦†ç›–æµ‹è¯?
# ============================================================================

def test_voxel_full_coverage(pkl_path: str, max_loops: Optional[int] = None):
    """
    æµ‹è¯•ä½“ç´ æ¨¡å¼ä¸?test split æ˜¯å¦è¦†ç›–æ‰€æœ‰ç‚¹
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  ğŸ”„ æµ‹è¯•2: ä½“ç´ æ¨¡å¼å…¨è¦†ç›–æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    max_loops_str = str(max_loops) if max_loops else "è‡ªåŠ¨"
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Max Loops: {Colors.CYAN}{max_loops_str}{Colors.RESET}")
    
    # åŠ è½½åŸå§‹ PKL è·å–çœŸå®ç‚¹æ•°
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    
    total_original_points = metadata['num_points']
    segments = metadata['segments']
    
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} åŸå§‹æ€»ç‚¹æ•? {Colors.CYAN}{format_number(total_original_points)}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Segments æ•? {Colors.CYAN}{len(segments)}{Colors.RESET}")
    
    # åˆ›å»º test æ•°æ®é›?
    dataset = BinPklDataset1(
        data_root=pkl_path,
        split='test',
        mode='grid',
        assets=['coord', 'class'],
        max_loops=max_loops
    )
    
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ•°æ®é›†æ ·æœ¬æ•°: {Colors.CYAN}{len(dataset)}{Colors.RESET}")
    
    # æ”¶é›†æ‰€æœ‰é‡‡æ ·çš„ç´¢å¼•
    all_sampled_indices = []
    segment_coverage = {}  # {segment_id: set of indices}
    
    print(f"\n  {Colors.BOLD}ğŸ“Š é‡‡æ ·è¦†ç›–åˆ†æ:{Colors.RESET}")
    
    for i in range(len(dataset.data_list)):
        sample_info = dataset.data_list[i]
        segment_id = sample_info['segment_id']
        
        # è·å–æ ·æœ¬
        sample = dataset._load_data(i)
        
        if 'indices' in sample:
            indices = sample['indices']
            all_sampled_indices.extend(indices.tolist())
            
            if segment_id not in segment_coverage:
                segment_coverage[segment_id] = set()
            segment_coverage[segment_id].update(indices.tolist())
    
    # ç»Ÿè®¡è¦†ç›–æƒ…å†µ
    all_sampled = np.array(all_sampled_indices)
    unique_sampled = np.unique(all_sampled)
    
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»é‡‡æ ·æ•°: {Colors.CYAN}{format_number(len(all_sampled))}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å”¯ä¸€é‡‡æ ·æ•? {Colors.CYAN}{format_number(len(unique_sampled))}{Colors.RESET}")
    
    coverage = len(unique_sampled) / total_original_points * 100
    if coverage >= 99.99:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è¦†ç›–ç? {Colors.GREEN}{format_percent(coverage)} âœ“{Colors.RESET}")
    else:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è¦†ç›–ç? {Colors.RED}{format_percent(coverage)}{Colors.RESET}")
    
    # é‡å¤é‡‡æ ·ç»Ÿè®¡
    repeat_total = len(all_sampled) - len(unique_sampled)
    sample_counter = Counter(all_sampled)
    
    print(f"\n  {Colors.BOLD}ğŸ” é‡å¤é‡‡æ ·ç»Ÿè®¡:{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} é‡å¤é‡‡æ ·æ¬¡æ•°: {Colors.YELLOW}{format_number(repeat_total)}{Colors.RESET}")
    
    if sample_counter:
        counts = list(sample_counter.values())
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡é‡‡æ ·æ¬¡æ•°: {Colors.YELLOW}{np.mean(counts):.2f}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æœ€å¤§é‡‡æ ·æ¬¡æ•? {Colors.YELLOW}{max(counts)}{Colors.RESET}")
        
        # åˆ†å¸ƒ
        count_dist = Counter(counts)
        print(f"\n  {Colors.BOLD}ğŸ“ˆ é‡‡æ ·æ¬¡æ•°åˆ†å¸ƒ:{Colors.RESET}")
        for cnt, num in sorted(count_dist.items())[:5]:
            pct = num / len(counts) * 100
            print(f"  {Colors.DIM}â”‚{Colors.RESET}   é‡‡æ · {cnt} æ¬? {format_number(num)} ç‚?({format_percent(pct)})")
    
    passed = coverage >= 99.99
    return {
        'coverage': coverage,
        'total_sampled': len(all_sampled),
        'unique_sampled': len(unique_sampled),
        'passed': passed
    }


# ============================================================================
# æµ‹è¯•3: åŠ¨æ€æ‰¹å¤„ç†å…¼å®¹æ€?
# ============================================================================

def test_dynamic_batch_compatibility(pkl_path: str):
    """
    æµ‹è¯•ä¸?DynamicBatchSampler çš„å…¼å®¹æ€?
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  ğŸ“¦ æµ‹è¯•3: åŠ¨æ€æ‰¹å¤„ç†å…¼å®¹æ€§{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    # æµ‹è¯• voxel æ¨¡å¼
    for split in ['train', 'test']:
        print(f"\n  {Colors.BOLD}ğŸ“¦ Split: {split}{Colors.RESET}")
        
        dataset = BinPklDataset1(
            data_root=pkl_path,
            split=split,
            mode='grid',
            max_loops=5 if split == 'test' else None
        )
        
        # è·å–æ ·æœ¬ç‚¹æ•°åˆ—è¡¨
        sample_num_points = dataset.get_sample_num_points()
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ ·æœ¬æ•? {Colors.CYAN}{len(sample_num_points)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç‚¹æ•°èŒƒå›´: [{min(sample_num_points):,}, {max(sample_num_points):,}]")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡ç‚¹æ•°: {np.mean(sample_num_points):,.1f}")
        
        # æ¨¡æ‹ŸåŠ¨æ€æ‰¹å¤„ç†
        max_points = 50000
        batches = []
        current_batch = []
        current_points = 0
        
        for i, num_points in enumerate(sample_num_points):
            if current_points + num_points > max_points and current_batch:
                batches.append(current_batch)
                current_batch = [i]
                current_points = num_points
            else:
                current_batch.append(i)
                current_points += num_points
        
        if current_batch:
            batches.append(current_batch)
        
        batch_sizes = [len(b) for b in batches]
        batch_points = [sum(sample_num_points[i] for i in b) for b in batches]
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ‰¹æ¬¡æ•?(max_points={max_points}): {Colors.CYAN}{len(batches)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ‰¹æ¬¡å¤§å°èŒƒå›´: [{min(batch_sizes)}, {max(batch_sizes)}]")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ‰¹æ¬¡ç‚¹æ•°èŒƒå›´: [{min(batch_points):,}, {max(batch_points):,}]")
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} {Colors.GREEN}âœ?é€šè¿‡{Colors.RESET}")
    
    return True


# ============================================================================
# æµ‹è¯•4: DataModule åŠŸèƒ½æµ‹è¯•
# ============================================================================

def test_datamodule(pkl_path: str):
    """
    æµ‹è¯• BinPklDataModule1 åŠŸèƒ½
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  ğŸ”§ æµ‹è¯•4: BinPklDataModule1 åŠŸèƒ½æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datamodule_bin1 import BinPklDataModule1
    
    # åˆ›å»º DataModule
    datamodule = BinPklDataModule1(
        train_data=pkl_path,
        val_data=pkl_path,
        test_data=pkl_path,
        batch_size=4,
        num_workers=0,  # æµ‹è¯•æ—¶ä½¿ç”?0
        mode='grid',
        max_loops=5,
        assets=['coord', 'intensity', 'class'],
    )
    
    # è®¾ç½®æ•°æ®é›?
    datamodule.setup('fit')
    datamodule.setup('test')
    
    print(f"\n  {Colors.BOLD}ğŸ“Š DataModule ä¿¡æ¯:{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è®­ç»ƒæ ·æœ¬æ•? {Colors.CYAN}{len(datamodule.train_dataset)}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} éªŒè¯æ ·æœ¬æ•? {Colors.CYAN}{len(datamodule.val_dataset)}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æµ‹è¯•æ ·æœ¬æ•? {Colors.CYAN}{len(datamodule.test_dataset)}{Colors.RESET}")
    
    # æµ‹è¯• DataLoader
    train_loader = datamodule.train_dataloader()
    val_loader = datamodule.val_dataloader()
    test_loader = datamodule.test_dataloader()
    
    print(f"\n  {Colors.BOLD}ğŸ“¦ DataLoader æµ‹è¯•:{Colors.RESET}")
    
    # æµ‹è¯•ä¸€ä¸?batch
    for name, loader in [('train', train_loader), ('val', val_loader), ('test', test_loader)]:
        batch = next(iter(loader))
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {name} batch:")
        print(f"  {Colors.DIM}â”‚{Colors.RESET}   - coord shape: {batch['coord'].shape}")
        if 'offset' in batch:
            print(f"  {Colors.DIM}â”‚{Colors.RESET}   - offset: {batch['offset']}")
    
    print(f"\n  {Colors.DIM}â””â”€{Colors.RESET} {Colors.GREEN}âœ?é€šè¿‡{Colors.RESET}")
    
    return True


# ============================================================================
# æµ‹è¯•5: ç±»åˆ«æ˜ å°„å’Œæƒé‡?
# ============================================================================

def test_class_mapping(pkl_path: str):
    """
    æµ‹è¯•ç±»åˆ«æ˜ å°„å’Œç±»åˆ«æƒé‡åŠŸèƒ?
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  ğŸ·ï¸?æµ‹è¯•5: ç±»åˆ«æ˜ å°„å’Œæƒé‡{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    # é¦–å…ˆè·å–åŸå§‹ç±»åˆ«åˆ†å¸ƒ
    dataset_orig = BinPklDataset1(
        data_root=pkl_path,
        split='train',
        mode='grid',
        assets=['coord', 'class'],
    )
    
    orig_dist = dataset_orig.get_class_distribution()
    print(f"\n  {Colors.BOLD}ğŸ“Š åŸå§‹ç±»åˆ«åˆ†å¸ƒ:{Colors.RESET}")
    for cls, count in sorted(orig_dist.items()):
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç±»åˆ« {cls}: {format_number(count)}")
    
    # æµ‹è¯•ç±»åˆ«æ˜ å°„
    # å‡è®¾æ˜ å°„ DALES ç±»åˆ«: 0->ignore, 1->0, 2->1, 3->2, 4->3, 5->4, 6->5, 7->6, 8->7
    class_mapping = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7}
    
    dataset_mapped = BinPklDataset1(
        data_root=pkl_path,
        split='train',
        mode='grid',
        assets=['coord', 'class'],
        class_mapping=class_mapping,
        ignore_label=-1
    )
    
    mapped_dist = dataset_mapped.get_class_distribution()
    print(f"\n  {Colors.BOLD}ğŸ“Š æ˜ å°„åç±»åˆ«åˆ†å¸?{Colors.RESET}")
    for cls, count in sorted(mapped_dist.items()):
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç±»åˆ« {cls}: {format_number(count)}")
    
    # æµ‹è¯•ç±»åˆ«æƒé‡
    weights = dataset_mapped.class_weights
    print(f"\n  {Colors.BOLD}âš–ï¸ ç±»åˆ«æƒé‡:{Colors.RESET}")
    if weights is not None:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æƒé‡ shape: {weights.shape}")
        for i, w in enumerate(weights):
            print(f"  {Colors.DIM}â”‚{Colors.RESET}   ç±»åˆ« {i}: {w:.4f}")
    
    print(f"\n  {Colors.DIM}â””â”€{Colors.RESET} {Colors.GREEN}âœ?é€šè¿‡{Colors.RESET}")
    
    return True


# ============================================================================
# æµ‹è¯•6: train å’?test æ¨¡å¼å¯¹æ¯”
# ============================================================================

def test_train_vs_test_sampling(pkl_path: str):
    """
    å¯¹æ¯” train å’?test æ¨¡å¼çš„é‡‡æ ·å·®å¼?
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  ğŸ”€ æµ‹è¯•6: Train vs Test é‡‡æ ·å¯¹æ¯”{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    # Train æ¨¡å¼
    dataset_train = BinPklDataset1(
        data_root=pkl_path,
        split='train',
        mode='grid',
        assets=['coord'],
    )
    
    # Test æ¨¡å¼
    dataset_test = BinPklDataset1(
        data_root=pkl_path,
        split='test',
        mode='grid',
        max_loops=None,  # è‡ªåŠ¨
        assets=['coord'],
    )
    
    print(f"\n  {Colors.BOLD}ğŸ“Š å¯¹æ¯”:{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Train æ ·æœ¬æ•? {Colors.CYAN}{len(dataset_train)}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Test æ ·æœ¬æ•? {Colors.CYAN}{len(dataset_test)}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ¯”ä¾‹: {Colors.YELLOW}{len(dataset_test) / len(dataset_train):.2f}x{Colors.RESET}")
    
    # æ£€æŸ?train çš„éšæœºæ€?
    print(f"\n  {Colors.BOLD}ğŸ² Train éšæœºæ€§éªŒè¯?{Colors.RESET}")
    sample1 = dataset_train[0]
    sample2 = dataset_train[0]  # å†æ¬¡è·å–åŒä¸€ä¸ªæ ·æœ?
    
    # æ£€æŸ¥åæ ‡æ˜¯å¦ä¸åŒï¼ˆéšæœºé‡‡æ ·ï¼?
    coords_same = np.allclose(sample1['coord'], sample2['coord'])
    if not coords_same:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ä¸¤æ¬¡é‡‡æ ·ç»“æœä¸åŒ: {Colors.GREEN}âœ?(éšæœºé‡‡æ ·æ­£å¸¸){Colors.RESET}")
    else:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ä¸¤æ¬¡é‡‡æ ·ç»“æœç›¸åŒ: {Colors.YELLOW}! (å¯èƒ½æ˜¯ç¼“å­˜æˆ–ç¡®å®šæ€§é‡‡æ ?{Colors.RESET}")
    
    # æ£€æŸ?test çš„ç¡®å®šæ€?
    print(f"\n  {Colors.BOLD}ğŸ”’ Test ç¡®å®šæ€§éªŒè¯?{Colors.RESET}")
    # Test æ¨¡å¼ä¸‹åŒä¸€ç´¢å¼•åº”è¯¥è¿”å›ç›¸åŒç»“æœ
    test_sample1 = dataset_test._load_data(0)
    test_sample2 = dataset_test._load_data(0)
    
    if 'indices' in test_sample1 and 'indices' in test_sample2:
        indices_same = np.array_equal(test_sample1['indices'], test_sample2['indices'])
        if indices_same:
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ä¸¤æ¬¡é‡‡æ ·ç´¢å¼•ç›¸åŒ: {Colors.GREEN}âœ?(æ¨¡è¿ç®—ç¡®å®šæ€§æ­£å¸?{Colors.RESET}")
        else:
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ä¸¤æ¬¡é‡‡æ ·ç´¢å¼•ä¸åŒ: {Colors.RED}âœ?(åº”è¯¥ç›¸åŒ){Colors.RESET}")
    
    print(f"\n  {Colors.DIM}â””â”€{Colors.RESET} {Colors.GREEN}âœ?é€šè¿‡{Colors.RESET}")
    
    return True


# ============================================================================
# æµ‹è¯•7: é€Ÿåº¦æµ‹è¯•
# ============================================================================

def test_speed_single_sample(pkl_path: str, n_iterations: int = 100):
    """
    å•æ ·æœ¬é‡‡æ ·é€Ÿåº¦æµ‹è¯•
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  âš?æµ‹è¯•7a: å•æ ·æœ¬é‡‡æ ·é€Ÿåº¦æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    results = {}
    
    for mode in ['voxel', 'full']:
        for split in ['train', 'test']:
            print(f"\n  {Colors.BOLD}ğŸ“Š Mode={mode}, Split={split}{Colors.RESET}")
            
            dataset = BinPklDataset1(
                data_root=pkl_path,
                split=split,
                mode=mode,
                assets=['coord', 'intensity', 'class'],
                max_loops=5 if split == 'test' else None
            )
            
            # é¢„çƒ­ (JIT ç¼–è¯‘)
            _ = dataset[0]
            _ = dataset[0]
            
            # è®¡æ—¶
            times = []
            for _ in range(n_iterations):
                idx = np.random.randint(0, len(dataset))
                t0 = time.perf_counter()
                sample = dataset[idx]
                t1 = time.perf_counter()
                times.append(t1 - t0)
            
            times = np.array(times)
            avg_time = np.mean(times)
            std_time = np.std(times)
            min_time = np.min(times)
            max_time = np.max(times)
            
            # è·å–æ ·æœ¬ç‚¹æ•°ä¿¡æ¯
            sample_points = [dataset.data_list[i].get('num_voxels', dataset.data_list[i]['num_points']) 
                            for i in range(min(10, len(dataset.data_list)))]
            avg_points = np.mean(sample_points)
            
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è¿­ä»£æ¬¡æ•°: {n_iterations}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡æ—¶é—´: {Colors.CYAN}{format_time(avg_time)}{Colors.RESET} Â± {format_time(std_time)}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æœ€å°?æœ€å¤? {format_time(min_time)} / {format_time(max_time)}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡ç‚¹æ•°: {avg_points:,.0f}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ååé‡? {Colors.GREEN}{1/avg_time:.1f} samples/s{Colors.RESET}")
            
            key = f"{mode}_{split}"
            results[key] = {
                'avg_time': avg_time,
                'std_time': std_time,
                'throughput': 1/avg_time
            }
    
    return results


def test_speed_random_access(pkl_path: str, n_iterations: int = 500):
    """
    å¤šæ–‡ä»¶éšæœºè®¿é—®é€Ÿåº¦æµ‹è¯•ï¼ˆæ¨¡æ‹?DataLoader è¡Œä¸ºï¼?
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  âš?æµ‹è¯•7b: éšæœºè®¿é—®é€Ÿåº¦æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin import BinPklDataset
    
    # åˆ›å»º train å’?test æ•°æ®é›?
    dataset_train = BinPklDataset(
        data_root=pkl_path,
        split='train',
        mode='grid',
        assets=['coord', 'intensity', 'class'],
    )
    
    dataset_test = BinPklDataset1(
        data_root=pkl_path,
        split='test',
        mode='grid',
        max_loops=5,
        assets=['coord', 'intensity', 'class'],
    )
    
    # é¢„çƒ­
    _ = dataset_train[0]
    _ = dataset_test[0]
    
    results = {}
    
    for name, dataset in [('train_voxel', dataset_train), ('test_voxel', dataset_test)]:
        print(f"\n  {Colors.BOLD}ğŸ“Š {name}{Colors.RESET}")
        
        # ç”Ÿæˆéšæœºç´¢å¼•åºåˆ—
        indices = np.random.randint(0, len(dataset), size=n_iterations)
        
        # è®¡æ—¶
        t0 = time.perf_counter()
        total_points = 0
        for idx in indices:
            sample = dataset[idx]
            total_points += len(sample['coord'])
        t1 = time.perf_counter()
        
        total_time = t1 - t0
        avg_time = total_time / n_iterations
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è¿­ä»£æ¬¡æ•°: {n_iterations}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»æ—¶é—? {Colors.CYAN}{format_time(total_time)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡æ—¶é—´: {Colors.CYAN}{format_time(avg_time)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»ç‚¹æ•? {format_number(total_points)}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ååé‡? {Colors.GREEN}{1/avg_time:.1f} samples/s{Colors.RESET}")
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} ç‚¹ååé‡: {Colors.GREEN}{total_points/total_time/1e6:.2f} M points/s{Colors.RESET}")
        
        results[name] = {
            'total_time': total_time,
            'avg_time': avg_time,
            'throughput': 1/avg_time,
            'points_per_sec': total_points/total_time
        }
    
    return results


def test_speed_dataloader(pkl_path: str, n_batches: int = 50):
    """
    DataLoader é€Ÿåº¦æµ‹è¯•ï¼ˆåŒ…æ‹¬åŠ¨æ€æ‰¹å¤„ç†ï¼?
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  âš?æµ‹è¯•7c: DataLoader é€Ÿåº¦æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datamodule_bin1 import BinPklDataModule1
    from torch.utils.data import DataLoader
    
    results = {}
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        {'name': 'fixed_batch', 'use_dynamic_batch': False, 'batch_size': 4},
        {'name': 'dynamic_batch_50k', 'use_dynamic_batch': True, 'max_points': 50000},
        {'name': 'dynamic_batch_100k', 'use_dynamic_batch': True, 'max_points': 100000},
    ]
    
    for config in configs:
        print(f"\n  {Colors.BOLD}ğŸ“Š {config['name']}{Colors.RESET}")
        
        datamodule = BinPklDataModule1(
            train_data=pkl_path,
            mode='grid',
            assets=['coord', 'intensity', 'class'],
            num_workers=0,  # å•çº¿ç¨‹æµ‹è¯•ä»¥å‡†ç¡®æµ‹é‡é‡‡æ ·æ—¶é—´
            use_dynamic_batch=config.get('use_dynamic_batch', False),
            batch_size=config.get('batch_size', 8),
            max_points=config.get('max_points', 100000),
        )
        
        datamodule.setup('fit')
        train_loader = datamodule.train_dataloader()
        
        # é¢„çƒ­
        for i, batch in enumerate(train_loader):
            if i >= 2:
                break
        
        # è®¡æ—¶
        t0 = time.perf_counter()
        total_points = 0
        batch_count = 0
        batch_sizes = []
        
        for batch in train_loader:
            total_points += batch['coord'].shape[0]
            batch_sizes.append(batch['coord'].shape[0])
            batch_count += 1
            if batch_count >= n_batches:
                break
        
        t1 = time.perf_counter()
        
        total_time = t1 - t0
        avg_batch_time = total_time / batch_count
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ‰¹æ¬¡æ•? {batch_count}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»æ—¶é—? {Colors.CYAN}{format_time(total_time)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡æ‰¹æ¬¡æ—¶é—´: {Colors.CYAN}{format_time(avg_batch_time)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ‰¹æ¬¡å¤§å°èŒƒå›´: [{min(batch_sizes):,}, {max(batch_sizes):,}]")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡æ‰¹æ¬¡ç‚¹æ•°: {np.mean(batch_sizes):,.0f}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ‰¹æ¬¡ååé‡? {Colors.GREEN}{batch_count/total_time:.1f} batches/s{Colors.RESET}")
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} ç‚¹ååé‡: {Colors.GREEN}{total_points/total_time/1e6:.2f} M points/s{Colors.RESET}")
        
        results[config['name']] = {
            'total_time': total_time,
            'avg_batch_time': avg_batch_time,
            'batch_throughput': batch_count/total_time,
            'points_per_sec': total_points/total_time
        }
    
    return results


def test_speed_comparison(pkl_path: str):
    """
    é€Ÿåº¦å¯¹æ¯”æµ‹è¯•ï¼šNumba vs çº?Pythonï¼ˆå¦‚æœå¯ç”¨ï¼‰
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  âš?æµ‹è¯•7d: é‡‡æ ·å‡½æ•°æ€§èƒ½åˆ†æ{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    # åŠ è½½æ•°æ®
    dataset = BinPklDataset1(
        data_root=pkl_path,
        split='train',
        mode='grid',
        assets=['coord'],
    )
    
    # è·å–ä¸€ä¸?segment è¿›è¡Œæµ‹è¯•
    metadata = dataset._get_metadata(dataset.data_list[0]['pkl_path'])
    segment_info = metadata['segments'][0]
    mmap_data = dataset._get_mmap(
        dataset.data_list[0]['bin_path'], 
        metadata['dtype']
    )
    
    n_voxels = len(segment_info['voxel_counts'])
    n_points = segment_info['num_points']
    
    print(f"\n  {Colors.BOLD}ğŸ“Š æµ‹è¯•æ ·æœ¬ä¿¡æ¯:{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ä½“ç´ æ•? {n_voxels:,}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»ç‚¹æ•? {n_points:,}")
    
    # é¢„çƒ­ Numba
    _ = dataset._voxel_random_sample(segment_info, mmap_data)
    _ = dataset._voxel_modulo_sample(segment_info, mmap_data, 0, 1)
    
    # æµ‹è¯•éšæœºé‡‡æ ·
    n_iterations = 1000
    
    print(f"\n  {Colors.BOLD}ğŸ² éšæœºé‡‡æ ·æµ‹è¯• ({n_iterations} æ¬?:{Colors.RESET}")
    
    times = []
    for _ in range(n_iterations):
        t0 = time.perf_counter()
        _ = dataset._voxel_random_sample(segment_info, mmap_data)
        t1 = time.perf_counter()
        times.append(t1 - t0)
    
    avg_time = np.mean(times)
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡æ—¶é—´: {Colors.CYAN}{format_time(avg_time)}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ¯ä½“ç´ è€—æ—¶: {Colors.CYAN}{avg_time/n_voxels*1e9:.1f} ns{Colors.RESET}")
    print(f"  {Colors.DIM}â””â”€{Colors.RESET} ååé‡? {Colors.GREEN}{n_voxels/avg_time/1e6:.2f} M voxels/s{Colors.RESET}")
    
    # æµ‹è¯•æ¨¡è¿ç®—é‡‡æ ?
    print(f"\n  {Colors.BOLD}ğŸ”„ æ¨¡è¿ç®—é‡‡æ ·æµ‹è¯?({n_iterations} æ¬?:{Colors.RESET}")
    
    times = []
    for _ in range(n_iterations):
        t0 = time.perf_counter()
        _ = dataset._voxel_modulo_sample(segment_info, mmap_data, 0, 1)
        t1 = time.perf_counter()
        times.append(t1 - t0)
    
    avg_time = np.mean(times)
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡æ—¶é—´: {Colors.CYAN}{format_time(avg_time)}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ¯ä½“ç´ è€—æ—¶: {Colors.CYAN}{avg_time/n_voxels*1e9:.1f} ns{Colors.RESET}")
    print(f"  {Colors.DIM}â””â”€{Colors.RESET} ååé‡? {Colors.GREEN}{n_voxels/avg_time/1e6:.2f} M voxels/s{Colors.RESET}")
    
    return True


def test_speed_multi_workers(pkl_path: str, n_batches: int = 50):
    """
    æµ‹è¯•ä¸åŒ num_workers å¯?DataLoader é€Ÿåº¦çš„å½±å“?
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  âš?æµ‹è¯•7e: å¤?Workers é€Ÿåº¦æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datamodule_bin1 import BinPklDataModule1
    import multiprocessing
    
    max_workers = min(multiprocessing.cpu_count(), 8)
    worker_counts = [0, 1, 2, 4] + ([max_workers] if max_workers > 4 else [])
    worker_counts = sorted(set(worker_counts))
    
    print(f"\n  {Colors.DIM}CPU æ ¸å¿ƒæ•? {multiprocessing.cpu_count()}{Colors.RESET}")
    print(f"  {Colors.DIM}æµ‹è¯• workers: {worker_counts}{Colors.RESET}")
    
    results = {}
    
    for num_workers in worker_counts:
        print(f"\n  {Colors.BOLD}ğŸ“Š num_workers={num_workers}{Colors.RESET}")
        
        try:
            datamodule = BinPklDataModule1(
                train_data=pkl_path,
                mode='grid',
                assets=['coord', 'class'],
                num_workers=num_workers,
                use_dynamic_batch=True,
                max_points=80000,
                prefetch_factor=2 if num_workers > 0 else None,
                persistent_workers=num_workers > 0,
            )
            
            datamodule.setup('fit')
            train_loader = datamodule.train_dataloader()
            
            # é¢„çƒ­
            warmup_count = min(5, n_batches // 2)
            for i, batch in enumerate(train_loader):
                if i >= warmup_count:
                    break
            
            # è®¡æ—¶
            t0 = time.perf_counter()
            total_points = 0
            batch_count = 0
            
            for batch in train_loader:
                total_points += batch['coord'].shape[0]
                batch_count += 1
                if batch_count >= n_batches:
                    break
            
            t1 = time.perf_counter()
            
            total_time = t1 - t0
            points_per_sec = total_points / total_time
            
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ‰¹æ¬¡æ•? {batch_count}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»æ—¶é—? {Colors.CYAN}{format_time(total_time)}{Colors.RESET}")
            print(f"  {Colors.DIM}â””â”€{Colors.RESET} ååé‡? {Colors.GREEN}{points_per_sec/1e6:.2f} M points/s{Colors.RESET}")
            
            results[f'workers_{num_workers}'] = {
                'total_time': total_time,
                'points_per_sec': points_per_sec,
                'batch_count': batch_count,
            }
            
        except Exception as e:
            print(f"  {Colors.RED}â?å¤±è´¥: {e}{Colors.RESET}")
            results[f'workers_{num_workers}'] = {'error': str(e)}
    
    # å¯¹æ¯”åˆ†æ
    print(f"\n  {Colors.BOLD}ğŸ“ˆ Workers æ€§èƒ½å¯¹æ¯”:{Colors.RESET}")
    base_throughput = None
    for key, val in results.items():
        if 'points_per_sec' in val:
            throughput = val['points_per_sec']
            if base_throughput is None:
                base_throughput = throughput
                speedup = "åŸºå‡†"
            else:
                speedup = f"{throughput/base_throughput:.2f}x"
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {key}: {throughput/1e6:.2f} M pts/s ({speedup})")
    
    return results


def test_speed_multi_files(data_dir: str, n_iterations: int = 100):
    """
    æµ‹è¯•å¤šæ–‡ä»¶ï¼ˆå…¨ç›®å½•ï¼‰è¯»å–é€Ÿåº¦
    """
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  âš?æµ‹è¯•7f: å¤šæ–‡ä»¶é€Ÿåº¦æµ‹è¯•{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    from pointsuite.data.datasets.dataset_bin1 import BinPklDataset1
    
    data_path = Path(data_dir)
    pkl_files = list(data_path.glob('*.pkl'))
    
    print(f"\n  {Colors.DIM}æ•°æ®ç›®å½•: {data_dir}{Colors.RESET}")
    print(f"  {Colors.DIM}pkl æ–‡ä»¶æ•? {len(pkl_files)}{Colors.RESET}")
    
    if len(pkl_files) == 0:
        print(f"  {Colors.RED}æœªæ‰¾åˆ?pkl æ–‡ä»¶{Colors.RESET}")
        return {}
    
    results = {}
    
    # æµ‹è¯•ä¸åŒæ¨¡å¼å’?split ç»„åˆ
    test_configs = [
        {'mode': 'voxel', 'split': 'train', 'name': 'voxel_train'},
        {'mode': 'voxel', 'split': 'test', 'name': 'voxel_test'},
        {'mode': 'full', 'split': 'train', 'name': 'full_train'},
    ]
    
    for config in test_configs:
        print(f"\n  {Colors.BOLD}ğŸ“Š {config['name']} (å…¨ç›®å½?{len(pkl_files)} ä¸ªæ–‡ä»?{Colors.RESET}")
        
        try:
            t_load_start = time.perf_counter()
            dataset = BinPklDataset1(
                data_root=data_dir,
                split=config['split'],
                mode=config['mode'],
                assets=['coord', 'class'],
            )
            t_load_end = time.perf_counter()
            
            n_samples = len(dataset)
            load_time = t_load_end - t_load_start
            
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} åŠ è½½æ—¶é—´: {Colors.CYAN}{format_time(load_time)}{Colors.RESET}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»æ ·æœ¬æ•°: {Colors.CYAN}{n_samples:,}{Colors.RESET}")
            
            if n_samples == 0:
                print(f"  {Colors.RED}â””â”€ æ— æ ·æœ¬å¯æµ‹è¯•{Colors.RESET}")
                continue
            
            # é¢„çƒ­
            _ = dataset[0]
            
            # éšæœºè®¿é—®æµ‹è¯•
            actual_iterations = min(n_iterations, n_samples)
            indices = np.random.randint(0, n_samples, size=actual_iterations)
            
            t0 = time.perf_counter()
            total_points = 0
            
            for idx in indices:
                sample = dataset[idx]
                total_points += len(sample['coord'])
            
            t1 = time.perf_counter()
            
            total_time = t1 - t0
            avg_time = total_time / actual_iterations
            
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} éšæœºè®¿é—® {actual_iterations} æ¬? {Colors.CYAN}{format_time(total_time)}{Colors.RESET}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} å¹³å‡æ—¶é—´: {Colors.CYAN}{format_time(avg_time)}{Colors.RESET}")
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ååé‡? {Colors.GREEN}{1/avg_time:.1f} samples/s{Colors.RESET}")
            print(f"  {Colors.DIM}â””â”€{Colors.RESET} ç‚¹ååé‡: {Colors.GREEN}{total_points/total_time/1e6:.2f} M points/s{Colors.RESET}")
            
            results[config['name']] = {
                'n_files': len(pkl_files),
                'n_samples': n_samples,
                'load_time': load_time,
                'total_time': total_time,
                'avg_time': avg_time,
                'throughput': 1/avg_time,
                'points_per_sec': total_points/total_time,
            }
            
        except Exception as e:
            print(f"  {Colors.RED}â?å¤±è´¥: {e}{Colors.RESET}")
            import traceback
            traceback.print_exc()
            results[config['name']] = {'error': str(e)}
    
    # DataLoader æµ‹è¯•ï¼ˆå…¨ç›®å½•ï¼?
    print(f"\n  {Colors.BOLD}ğŸ“Š DataLoader å…¨ç›®å½•æµ‹è¯•{Colors.RESET}")
    
    try:
        from pointsuite.data.datamodule_bin1 import BinPklDataModule1
        
        datamodule = BinPklDataModule1(
            train_data=data_dir,
            mode='grid',
            assets=['coord', 'class'],
            num_workers=0,
            use_dynamic_batch=True,
            max_points=80000,
        )
        
        datamodule.setup('fit')
        train_loader = datamodule.train_dataloader()
        n_total_samples = len(train_loader.dataset)
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»æ ·æœ¬æ•°: {n_total_samples:,}")
        
        # éå†æ•´ä¸ªæ•°æ®é›†ä¸€æ¬?
        t0 = time.perf_counter()
        total_points = 0
        batch_count = 0
        
        for batch in train_loader:
            total_points += batch['coord'].shape[0]
            batch_count += 1
        
        t1 = time.perf_counter()
        
        total_time = t1 - t0
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»æ‰¹æ¬¡æ•°: {batch_count}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} éå†æ—¶é—´: {Colors.CYAN}{format_time(total_time)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»ç‚¹æ•? {Colors.CYAN}{total_points:,}{Colors.RESET}")
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} ååé‡? {Colors.GREEN}{total_points/total_time/1e6:.2f} M points/s{Colors.RESET}")
        
        results['dataloader_full'] = {
            'n_samples': n_total_samples,
            'batch_count': batch_count,
            'total_time': total_time,
            'total_points': total_points,
            'points_per_sec': total_points/total_time,
        }
        
    except Exception as e:
        print(f"  {Colors.RED}â?DataLoader æµ‹è¯•å¤±è´¥: {e}{Colors.RESET}")
        import traceback
        traceback.print_exc()
    
    return results


def run_speed_tests(pkl_path: str, n_iterations: int = 100, n_batches: int = 50, 
                    test_multi_workers: bool = True, test_multi_files: bool = True):
    """è¿è¡Œæ‰€æœ‰é€Ÿåº¦æµ‹è¯•"""
    print(f"\n{Colors.BOLD}{'#'*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.HEADER}  âš?é€Ÿåº¦æµ‹è¯•å¥—ä»¶{Colors.RESET}")
    print(f"{Colors.BOLD}{'#'*70}{Colors.RESET}")
    print(f"  æµ‹è¯•æ–‡ä»¶: {pkl_path}")
    
    results = {}
    
    try:
        # é‡‡æ ·å‡½æ•°æ€§èƒ½
        results['sampling_perf'] = test_speed_comparison(pkl_path)
        
        # å•æ ·æœ¬æµ‹è¯?
        results['single_sample'] = test_speed_single_sample(pkl_path, n_iterations=n_iterations)
        
        # éšæœºè®¿é—®æµ‹è¯•
        results['random_access'] = test_speed_random_access(pkl_path, n_iterations=n_iterations * 5)
        
        # DataLoader æµ‹è¯•
        results['dataloader'] = test_speed_dataloader(pkl_path, n_batches=n_batches)
        
        # å¤?Workers æµ‹è¯•
        if test_multi_workers:
            results['multi_workers'] = test_speed_multi_workers(pkl_path, n_batches=n_batches)
        
        # å¤šæ–‡ä»¶æµ‹è¯?
        if test_multi_files:
            data_dir = Path(pkl_path).parent
            results['multi_files'] = test_speed_multi_files(str(data_dir), n_iterations=n_iterations)
        
    except Exception as e:
        print(f"\n{Colors.RED}â?é€Ÿåº¦æµ‹è¯•å¤±è´¥: {e}{Colors.RESET}")
        import traceback
        traceback.print_exc()
    
    # æ±‡æ€?
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.GREEN}  ğŸ“‹ é€Ÿåº¦æµ‹è¯•æ±‡æ€»{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    if 'single_sample' in results:
        print(f"\n  {Colors.BOLD}å•æ ·æœ¬é‡‡æ ?{Colors.RESET}")
        for key, val in results['single_sample'].items():
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {key}: {format_time(val['avg_time'])} ({val['throughput']:.1f} samples/s)")
    
    if 'dataloader' in results:
        print(f"\n  {Colors.BOLD}DataLoader ååé‡?(num_workers=0):{Colors.RESET}")
        for key, val in results['dataloader'].items():
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {key}: {val['points_per_sec']/1e6:.2f} M points/s")
    
    if 'multi_workers' in results:
        print(f"\n  {Colors.BOLD}å¤?Workers ååé‡?{Colors.RESET}")
        for key, val in results['multi_workers'].items():
            if 'points_per_sec' in val:
                print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {key}: {val['points_per_sec']/1e6:.2f} M points/s")
    
    if 'multi_files' in results:
        print(f"\n  {Colors.BOLD}å¤šæ–‡ä»?(å…¨ç›®å½? ååé‡?{Colors.RESET}")
        for key, val in results['multi_files'].items():
            if 'points_per_sec' in val:
                print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {key}: {val['points_per_sec']/1e6:.2f} M points/s")
    
    print()
    return results


# ============================================================================
# ä¸»æµ‹è¯•å…¥å?
# ============================================================================

def run_all_tests(pkl_path: str):
    """è¿è¡Œæ‰€æœ‰æµ‹è¯?""
    print(f"\n{Colors.BOLD}{'#'*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.HEADER}  ğŸ§ª BinPklDataset1 & DataModule1 æµ‹è¯•å¥—ä»¶{Colors.RESET}")
    print(f"{Colors.BOLD}{'#'*70}{Colors.RESET}")
    print(f"  æµ‹è¯•æ–‡ä»¶: {pkl_path}")
    
    results = {}
    
    try:
        # æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½
        results['basic_voxel'] = test_dataset_basic(pkl_path, mode='grid')
        results['basic_full'] = test_dataset_basic(pkl_path, mode='full')
        
        # æµ‹è¯•2: å…¨è¦†ç›?
        results['coverage'] = test_voxel_full_coverage(pkl_path, max_loops=None)
        results['coverage_limited'] = test_voxel_full_coverage(pkl_path, max_loops=5)
        
        # æµ‹è¯•3: åŠ¨æ€æ‰¹å¤„ç†
        results['dynamic_batch'] = test_dynamic_batch_compatibility(pkl_path)
        
        # æµ‹è¯•4: DataModule
        results['datamodule'] = test_datamodule(pkl_path)
        
        # æµ‹è¯•5: ç±»åˆ«æ˜ å°„
        results['class_mapping'] = test_class_mapping(pkl_path)
        
        # æµ‹è¯•6: Train vs Test
        results['train_vs_test'] = test_train_vs_test_sampling(pkl_path)
        
        # æµ‹è¯•7: é€Ÿåº¦æµ‹è¯•
        results['speed'] = run_speed_tests(pkl_path)
        
    except Exception as e:
        print(f"\n{Colors.RED}â?æµ‹è¯•å¤±è´¥: {e}{Colors.RESET}")
        import traceback
        traceback.print_exc()
        return results
    
    # æ±‡æ€?
    print(f"\n{Colors.BOLD}{'='*70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.GREEN}  ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»{Colors.RESET}")
    print(f"{Colors.BOLD}{'='*70}{Colors.RESET}")
    
    all_passed = True
    for name, result in results.items():
        if name == 'speed':
            continue  # é€Ÿåº¦æµ‹è¯•ä¸å‚ä¸?pass/fail åˆ¤æ–­
        if isinstance(result, dict):
            passed = result.get('passed', True)
        else:
            passed = result
        status = f"{Colors.GREEN}âœ?PASS{Colors.RESET}" if passed else f"{Colors.RED}âœ?FAIL{Colors.RESET}"
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {name}: {status}")
        all_passed = all_passed and passed
    
    print(f"\n  {Colors.BOLD}æœ€ç»ˆç»“æ? ", end="")
    if all_passed:
        print(f"{Colors.GREEN}æ‰€æœ‰æµ‹è¯•é€šè¿‡ âœ“{Colors.RESET}")
    else:
        print(f"{Colors.RED}éƒ¨åˆ†æµ‹è¯•å¤±è´¥{Colors.RESET}")
    print()
    
    return results


# ============================================================================
# å‘½ä»¤è¡Œå…¥å?
# ============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯• BinPklDataset1 å’?DataModule1')
    parser.add_argument('--pkl', type=str, required=False,
                        help='PKL æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--test', type=str, default='all',
                        choices=['all', 'basic', 'coverage', 'batch', 'datamodule', 'class', 'compare', 'speed'],
                        help='è¿è¡Œçš„æµ‹è¯?)
    parser.add_argument('--speed-iterations', type=int, default=100,
                        help='é€Ÿåº¦æµ‹è¯•çš„è¿­ä»£æ¬¡æ•?)
    parser.add_argument('--speed-batches', type=int, default=50,
                        help='DataLoaderé€Ÿåº¦æµ‹è¯•çš„æ‰¹æ¬¡æ•°')
    
    args = parser.parse_args()
    
    # é»˜è®¤æµ‹è¯•è·¯å¾„
    if args.pkl:
        pkl_path = args.pkl
    else:
        default_path = r"E:\data\DALES\dales_las\bin\train_logical\5080_54435.pkl"
        if Path(default_path).exists():
            pkl_path = default_path
        else:
            print(f"{Colors.RED}è¯·æŒ‡å®?--pkl å‚æ•°{Colors.RESET}")
            sys.exit(1)
    
    if not Path(pkl_path).exists():
        print(f"{Colors.RED}æ–‡ä»¶ä¸å­˜åœ? {pkl_path}{Colors.RESET}")
        sys.exit(1)
    
    if args.test == 'all':
        run_all_tests(pkl_path)
    elif args.test == 'basic':
        test_dataset_basic(pkl_path, 'voxel')
        test_dataset_basic(pkl_path, 'full')
    elif args.test == 'coverage':
        test_voxel_full_coverage(pkl_path)
    elif args.test == 'batch':
        test_dynamic_batch_compatibility(pkl_path)
    elif args.test == 'datamodule':
        test_datamodule(pkl_path)
    elif args.test == 'class':
        test_class_mapping(pkl_path)
    elif args.test == 'compare':
        test_train_vs_test_sampling(pkl_path)
    elif args.test == 'speed':
        run_speed_tests(pkl_path, args.speed_iterations, args.speed_batches)
