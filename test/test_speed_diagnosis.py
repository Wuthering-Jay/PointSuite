"""
è¯Šæ–­æ•°æ®é›†è®¿é—®é€Ÿåº¦ç“¶é¢ˆ

é—®é¢˜ï¼šä¸€ä¸‡å¤šä¸ªæ ·æœ¬éœ€è¦æ•°åˆ†é’Ÿè®¿é—®ï¼Œè¿™æ˜æ˜¾ä¸æ­£å¸¸
å¯èƒ½åŸå› ï¼š
1. æ¯æ¬¡éƒ½ä»ç£ç›˜åŠ è½½ pkl å…ƒæ•°æ®
2. æ²¡æœ‰ä½¿ç”¨ cache_data
3. é¢‘ç¹çš„ memmap æ‰“å¼€/å…³é—­
4. ä¸å¿…è¦çš„æ•°æ®è½¬æ¢
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import sys
from pathlib import Path
import time
import numpy as np
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pointsuite.data.datasets.dataset_bin import BinPklDataset


def test_init_speed():
    """æµ‹è¯•1: åˆå§‹åŒ–é€Ÿåº¦"""
    print("="*70)
    print("[æµ‹è¯•1] æ•°æ®é›†åˆå§‹åŒ–é€Ÿåº¦")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    start = time.time()
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    init_time = time.time() - start
    
    print(f"\nåˆå§‹åŒ–æ—¶é—´: {init_time:.2f} ç§’")
    print(f"æ ·æœ¬æ•°: {len(dataset):,}")
    print(f"å¹³å‡æ¯ä¸ªæ ·æœ¬: {init_time/len(dataset)*1000:.2f} ms")
    print()


def test_single_sample_access():
    """æµ‹è¯•2: å•ä¸ªæ ·æœ¬è®¿é—®é€Ÿåº¦"""
    print("="*70)
    print("[æµ‹è¯•2] å•ä¸ªæ ·æœ¬è®¿é—®é€Ÿåº¦")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    # æµ‹è¯•è®¿é—®å‰ 100 ä¸ªæ ·æœ¬
    print(f"\næµ‹è¯•è®¿é—®å‰ 100 ä¸ªæ ·æœ¬...")
    
    times = []
    for i in range(100):
        start = time.time()
        sample = dataset[i]
        elapsed = time.time() - start
        times.append(elapsed)
    
    print(f"\nå‰ 100 ä¸ªæ ·æœ¬è®¿é—®æ—¶é—´ç»Ÿè®¡:")
    print(f"  - æ€»æ—¶é—´: {sum(times):.2f} ç§’")
    print(f"  - å¹³å‡: {np.mean(times)*1000:.2f} ms")
    print(f"  - ä¸­ä½æ•°: {np.median(times)*1000:.2f} ms")
    print(f"  - æœ€å°: {np.min(times)*1000:.2f} ms")
    print(f"  - æœ€å¤§: {np.max(times)*1000:.2f} ms")
    print(f"  - æ ‡å‡†å·®: {np.std(times)*1000:.2f} ms")
    
    # é¢„ä¼°å…¨éƒ¨æ ·æœ¬æ—¶é—´
    total_samples = len(dataset)
    estimated_time = np.mean(times) * total_samples
    print(f"\né¢„ä¼°éå†æ‰€æœ‰ {total_samples:,} ä¸ªæ ·æœ¬éœ€è¦: {estimated_time:.2f} ç§’ ({estimated_time/60:.2f} åˆ†é’Ÿ)")
    print()


def test_repeated_access():
    """æµ‹è¯•3: é‡å¤è®¿é—®åŒä¸€æ ·æœ¬"""
    print("="*70)
    print("[æµ‹è¯•3] é‡å¤è®¿é—®åŒä¸€æ ·æœ¬ï¼ˆæµ‹è¯•ç¼“å­˜æ•ˆæœï¼‰")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    print(f"\né‡å¤è®¿é—®æ ·æœ¬ 0ï¼Œå…± 10 æ¬¡...")
    
    times = []
    for i in range(10):
        start = time.time()
        sample = dataset[0]
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"  ç¬¬ {i+1} æ¬¡: {elapsed*1000:.2f} ms")
    
    print(f"\né‡å¤è®¿é—®æ—¶é—´åˆ†æ:")
    print(f"  - é¦–æ¬¡è®¿é—®: {times[0]*1000:.2f} ms")
    print(f"  - åç»­å¹³å‡: {np.mean(times[1:])*1000:.2f} ms")
    print(f"  - åŠ é€Ÿæ¯”: {times[0]/np.mean(times[1:]):.2f}x")
    
    if times[0] > np.mean(times[1:]) * 1.5:
        print(f"  - âœ… æœ‰ç¼“å­˜æœºåˆ¶ï¼ˆé¦–æ¬¡æ…¢ï¼Œåç»­å¿«ï¼‰")
    else:
        print(f"  - âš ï¸ æ— æ˜æ˜¾ç¼“å­˜æ•ˆæœï¼ˆæ¯æ¬¡éƒ½ä»ç£ç›˜è¯»ï¼‰")
    print()


def test_with_cache():
    """æµ‹è¯•4: ä½¿ç”¨ cache_data=True"""
    print("="*70)
    print("[æµ‹è¯•4] å¯ç”¨ cache_data=True çš„æ•ˆæœ")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    # æµ‹è¯•å°‘é‡æ ·æœ¬
    print("\nåˆ›å»ºæ•°æ®é›†ï¼ˆcache_data=Trueï¼Œå°æ•°æ®é›†ï¼‰...")
    
    start = time.time()
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=True
    )
    init_time = time.time() - start
    print(f"åˆå§‹åŒ–æ—¶é—´: {init_time:.2f} ç§’")
    
    # é¦–æ¬¡éå†ï¼ˆä¼šè§¦å‘ç¼“å­˜ï¼‰
    print(f"\né¦–æ¬¡éå†å‰ 100 ä¸ªæ ·æœ¬ï¼ˆè§¦å‘ç¼“å­˜ï¼‰...")
    start = time.time()
    for i in range(min(100, len(dataset))):
        sample = dataset[i]
    first_pass = time.time() - start
    print(f"é¦–æ¬¡éå†æ—¶é—´: {first_pass:.2f} ç§’")
    print(f"å¹³å‡æ¯æ ·æœ¬: {first_pass/100*1000:.2f} ms")
    
    # ç¬¬äºŒæ¬¡éå†ï¼ˆä»ç¼“å­˜è¯»å–ï¼‰
    print(f"\nç¬¬äºŒæ¬¡éå†å‰ 100 ä¸ªæ ·æœ¬ï¼ˆä»ç¼“å­˜è¯»å–ï¼‰...")
    start = time.time()
    for i in range(min(100, len(dataset))):
        sample = dataset[i]
    second_pass = time.time() - start
    print(f"ç¬¬äºŒæ¬¡éå†æ—¶é—´: {second_pass:.2f} ç§’")
    print(f"å¹³å‡æ¯æ ·æœ¬: {second_pass/100*1000:.2f} ms")
    
    # åŠ é€Ÿæ¯”
    if second_pass > 0:
        speedup = first_pass / second_pass
        print(f"\nç¼“å­˜åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if speedup > 2:
            print(f"  - âœ… ç¼“å­˜æ˜¾è‘—åŠ é€Ÿï¼")
        else:
            print(f"  - âš ï¸ ç¼“å­˜åŠ é€Ÿä¸æ˜æ˜¾")
    else:
        print(f"\nç¼“å­˜åŠ é€Ÿæ¯”: âˆ (ç¬¬äºŒæ¬¡å‡ ä¹ç¬æ—¶)")
        print(f"  - âœ… ç¼“å­˜æåº¦æ˜¾è‘—ï¼")
    print()


def test_batch_sampler_speed():
    """æµ‹è¯•5: DynamicBatchSampler çš„æ€§èƒ½ç“¶é¢ˆ"""
    print("="*70)
    print("[æµ‹è¯•5] DynamicBatchSampler æ€§èƒ½åˆ†æ")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    from pointsuite.data.datasets.collate import DynamicBatchSampler
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    print(f"\næ•°æ®é›†å¤§å°: {len(dataset):,} ä¸ªæ ·æœ¬")
    
    # åˆ›å»º batch sampler
    print("\nåˆ›å»º DynamicBatchSampler...")
    start = time.time()
    batch_sampler = DynamicBatchSampler(
        dataset,
        max_points=300000,
        shuffle=False,  # é¡ºåºè®¿é—®ï¼Œä¾¿äºæµ‹è¯•
        drop_last=False
    )
    sampler_init_time = time.time() - start
    print(f"Sampler åˆå§‹åŒ–æ—¶é—´: {sampler_init_time:.2f} ç§’")
    
    # éå†æ‰€æœ‰ batch indices
    print(f"\néå†æ‰€æœ‰ batch indicesï¼ˆä¸åŠ è½½æ•°æ®ï¼‰...")
    start = time.time()
    batch_count = 0
    sample_count = 0
    
    for batch_indices in batch_sampler:
        batch_count += 1
        sample_count += len(batch_indices)
        
        if batch_count % 500 == 0:
            elapsed = time.time() - start
            print(f"  å·²å¤„ç† {batch_count} batches, {sample_count:,} æ ·æœ¬, ç”¨æ—¶ {elapsed:.2f}s")
    
    total_time = time.time() - start
    
    print(f"\néå†ç»Ÿè®¡:")
    print(f"  - æ€» batches: {batch_count:,}")
    print(f"  - æ€»æ ·æœ¬: {sample_count:,}")
    print(f"  - æ€»æ—¶é—´: {total_time:.2f} ç§’")
    print(f"  - æ¯ batch å¹³å‡: {total_time/batch_count*1000:.2f} ms")
    print(f"  - æ¯æ ·æœ¬å¹³å‡: {total_time/sample_count*1000:.2f} ms")
    
    # å¯¹æ¯”ï¼šç›´æ¥éå†ç´¢å¼•
    print(f"\nå¯¹æ¯”ï¼šç›´æ¥éå†ç´¢å¼•...")
    start = time.time()
    for i in range(len(dataset)):
        pass
    direct_time = time.time() - start
    print(f"ç›´æ¥éå†æ—¶é—´: {direct_time:.4f} ç§’")
    
    overhead = (total_time - direct_time) / total_time * 100
    print(f"\nDynamicBatchSampler é¢å¤–å¼€é”€: {overhead:.1f}%")
    print()


def profile_load_data():
    """æµ‹è¯•6: è¯¦ç»†åˆ†æ _load_data æ–¹æ³•"""
    print("="*70)
    print("[æµ‹è¯•6] è¯¦ç»†åˆ†æ _load_data æ–¹æ³•")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    import pickle
    
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    print(f"\nåˆ†æå•ä¸ªæ ·æœ¬åŠ è½½è¿‡ç¨‹...")
    
    idx = 0
    sample_info = dataset.data_list[idx]
    
    # æ­¥éª¤1: è·å–è·¯å¾„
    start = time.time()
    bin_path = Path(sample_info['bin_path'])
    pkl_path = Path(sample_info['pkl_path'])
    segment_id = sample_info['segment_id']
    t1 = time.time() - start
    print(f"\n1. è·å–è·¯å¾„: {t1*1000:.4f} ms")
    
    # æ­¥éª¤2: åŠ è½½ pkl å…ƒæ•°æ®
    start = time.time()
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    t2 = time.time() - start
    print(f"2. åŠ è½½ pkl å…ƒæ•°æ®: {t2*1000:.2f} ms âš ï¸")
    
    # æ­¥éª¤3: æŸ¥æ‰¾ segment info
    start = time.time()
    segment_info = None
    for seg in metadata['segments']:
        if seg['segment_id'] == segment_id:
            segment_info = seg
            break
    t3 = time.time() - start
    print(f"3. æŸ¥æ‰¾ segment info: {t3*1000:.2f} ms")
    
    # æ­¥éª¤4: åˆ›å»º memmap
    start = time.time()
    point_data = np.memmap(bin_path, dtype=metadata['dtype'], mode='r')
    t4 = time.time() - start
    print(f"4. åˆ›å»º memmap: {t4*1000:.2f} ms")
    
    # æ­¥éª¤5: ç´¢å¼•æ•°æ®
    start = time.time()
    indices = segment_info['indices']
    segment_points = point_data[indices]
    t5 = time.time() - start
    print(f"5. ç´¢å¼•æ•°æ®: {t5*1000:.2f} ms")
    
    # æ­¥éª¤6: æå–ç‰¹å¾
    start = time.time()
    coord = np.stack([
        segment_points['X'],
        segment_points['Y'],
        segment_points['Z']
    ], axis=1).astype(np.float32)
    
    intensity = segment_points['intensity'].astype(np.float32)
    intensity = intensity / 65535.0
    intensity = intensity[:, np.newaxis]
    
    classification = segment_points['classification'].astype(np.int64)
    
    feature = np.concatenate([coord, intensity], axis=1)
    t6 = time.time() - start
    print(f"6. æå–ç‰¹å¾: {t6*1000:.2f} ms")
    
    # æ€»ç»“
    total = t1 + t2 + t3 + t4 + t5 + t6
    print(f"\næ€»è®¡: {total*1000:.2f} ms")
    print(f"\næ€§èƒ½å æ¯”:")
    print(f"  - è·å–è·¯å¾„: {t1/total*100:.1f}%")
    print(f"  - åŠ è½½ pkl: {t2/total*100:.1f}% âš ï¸ ä¸»è¦ç“¶é¢ˆ")
    print(f"  - æŸ¥æ‰¾ segment: {t3/total*100:.1f}%")
    print(f"  - åˆ›å»º memmap: {t4/total*100:.1f}%")
    print(f"  - ç´¢å¼•æ•°æ®: {t5/total*100:.1f}%")
    print(f"  - æå–ç‰¹å¾: {t6/total*100:.1f}%")
    
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    if t2 > total * 0.3:
        print(f"  - âš ï¸ pkl åŠ è½½å  {t2/total*100:.1f}%ï¼Œå»ºè®®ç¼“å­˜ metadataï¼")
    if t5 > total * 0.2:
        print(f"  - âš ï¸ æ•°æ®ç´¢å¼•å  {t5/total*100:.1f}%ï¼Œè€ƒè™‘é¢„å¤„ç†æˆ–ä¼˜åŒ–ç´¢å¼•æ–¹å¼")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("="*70)
    print("æ•°æ®é›†è®¿é—®é€Ÿåº¦è¯Šæ–­")
    print("="*70)
    print()
    
    # æµ‹è¯•1: åˆå§‹åŒ–é€Ÿåº¦
    test_init_speed()
    
    # æµ‹è¯•2: å•æ ·æœ¬è®¿é—®
    test_single_sample_access()
    
    # æµ‹è¯•3: é‡å¤è®¿é—®
    test_repeated_access()
    
    # æµ‹è¯•4: cache_data
    test_with_cache()
    
    # æµ‹è¯•5: BatchSampler æ€§èƒ½
    test_batch_sampler_speed()
    
    # æµ‹è¯•6: è¯¦ç»†åˆ†æ
    profile_load_data()
    
    print("="*70)
    print("[å®Œæˆ] è¯Šæ–­å®Œæˆ")
    print("="*70)


if __name__ == '__main__':
    main()
