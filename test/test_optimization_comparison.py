"""
æ•°æ®é›†è®¿é—®é€Ÿåº¦å¯¹æ¯”æµ‹è¯•ï¼ˆä¼˜åŒ–å‰åï¼‰

å¯¹æ¯” metadata ç¼“å­˜ä¼˜åŒ–å‰åçš„å®é™…æ•°æ®åŠ è½½é€Ÿåº¦
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import sys
from pathlib import Path
import time
import numpy as np
from torch.utils.data import DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pointsuite.data.datasets.dataset_bin import BinPklDataset
from pointsuite.data.datasets.collate import DynamicBatchSampler, collate_fn


def test_actual_data_loading():
    """æµ‹è¯•å®é™…æ•°æ®åŠ è½½é€Ÿåº¦"""
    print("="*70)
    print("å®é™…æ•°æ®åŠ è½½é€Ÿåº¦æµ‹è¯•")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False  # ä¸ç¼“å­˜æ•°æ®ï¼Œåªç¼“å­˜ metadata
    )
    
    total_samples = len(dataset)
    print(f"\næ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples:,}")
    
    # æµ‹è¯•1: ç›´æ¥è®¿é—®å‰ 100 ä¸ªæ ·æœ¬
    print("\n" + "="*70)
    print("[æµ‹è¯•1] é¡ºåºè®¿é—®å‰ 100 ä¸ªæ ·æœ¬ï¼ˆæµ‹è¯• metadata ç¼“å­˜æ•ˆæœï¼‰")
    print("="*70)
    
    times = []
    total_points = 0
    
    start = time.time()
    for i in range(100):
        sample = dataset[i]
        total_points += len(sample['coord'])
        
        if (i + 1) % 20 == 0:
            elapsed = time.time() - start
            avg_time = elapsed / (i + 1) * 1000
            print(f"  - å·²åŠ è½½ {i+1}/100 æ ·æœ¬ï¼Œå¹³å‡ {avg_time:.2f} ms/æ ·æœ¬")
    
    total_time = time.time() - start
    avg_time = total_time / 100 * 1000
    
    print(f"\nå‰ 100 ä¸ªæ ·æœ¬ç»Ÿè®¡:")
    print(f"  - æ€»æ—¶é—´: {total_time:.2f}s")
    print(f"  - å¹³å‡æ—¶é—´: {avg_time:.2f} ms/æ ·æœ¬")
    print(f"  - æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  - ç‚¹é€Ÿåº¦: {total_points/total_time:,.0f} points/s")
    
    # é¢„ä¼°å…¨éƒ¨éå†æ—¶é—´
    estimated = total_time / 100 * total_samples
    print(f"\né¢„ä¼°éå†æ‰€æœ‰ {total_samples:,} ä¸ªæ ·æœ¬éœ€è¦:")
    print(f"  - æ—¶é—´: {estimated:.2f}s ({estimated/60:.2f} åˆ†é’Ÿ)")
    
    # æµ‹è¯•2: é‡å¤è®¿é—®åŒä¸€æ‰¹æ ·æœ¬ï¼ˆæµ‹è¯•ç¼“å­˜æ•ˆæœï¼‰
    print("\n" + "="*70)
    print("[æµ‹è¯•2] é‡å¤è®¿é—®å‰ 100 ä¸ªæ ·æœ¬ï¼ˆç¬¬2æ¬¡éå†ï¼‰")
    print("="*70)
    
    start = time.time()
    for i in range(100):
        sample = dataset[i]
    
    second_time = time.time() - start
    second_avg = second_time / 100 * 1000
    
    print(f"\nç¬¬äºŒæ¬¡éå†ç»Ÿè®¡:")
    print(f"  - æ€»æ—¶é—´: {second_time:.2f}s")
    print(f"  - å¹³å‡æ—¶é—´: {second_avg:.2f} ms/æ ·æœ¬")
    print(f"  - åŠ é€Ÿæ¯”: {total_time/second_time:.2f}x")
    
    if total_time / second_time > 1.2:
        print(f"  - âœ… Metadata ç¼“å­˜ç”Ÿæ•ˆï¼")
    else:
        print(f"  - âš ï¸ æ— æ˜æ˜¾åŠ é€Ÿæ•ˆæœ")
    
    # æµ‹è¯•3: ä½¿ç”¨ DynamicBatchSampler åŠ è½½
    print("\n" + "="*70)
    print("[æµ‹è¯•3] ä½¿ç”¨ DynamicBatchSampler åŠ è½½æ•°æ®")
    print("="*70)
    
    batch_sampler = DynamicBatchSampler(
        dataset,
        max_points=300000,
        shuffle=False,
        drop_last=False
    )
    
    dataloader = DataLoader(
        dataset,
        batch_sampler=batch_sampler,
        collate_fn=collate_fn,
        num_workers=0,
    )
    
    print(f"\nåŠ è½½å‰ 100 ä¸ª batch...")
    
    batch_count = 0
    sample_count = 0
    total_points = 0
    
    start = time.time()
    for batch in dataloader:
        batch_count += 1
        sample_count += len(batch['offset'])
        total_points += len(batch['coord'])
        
        if batch_count >= 100:
            break
        
        if batch_count % 20 == 0:
            elapsed = time.time() - start
            print(f"  - å·²åŠ è½½ {batch_count}/100 batches, {sample_count} æ ·æœ¬, {total_points:,} ç‚¹, {elapsed:.2f}s")
    
    elapsed = time.time() - start
    
    print(f"\nå‰ 100 ä¸ª batch ç»Ÿè®¡:")
    print(f"  - æ€»æ—¶é—´: {elapsed:.2f}s")
    print(f"  - å¹³å‡æ—¶é—´: {elapsed/batch_count*1000:.2f} ms/batch")
    print(f"  - æ ·æœ¬æ•°: {sample_count}")
    print(f"  - æ€»ç‚¹æ•°: {total_points:,}")
    print(f"  - æ ·æœ¬é€Ÿåº¦: {sample_count/elapsed:.1f} samples/s")
    print(f"  - ç‚¹é€Ÿåº¦: {total_points/elapsed:,.0f} points/s")
    
    # é¢„ä¼°å®Œæ•´ epoch
    total_batches = len(batch_sampler)
    estimated_epoch = elapsed / batch_count * total_batches
    
    print(f"\né¢„ä¼°å®Œæ•´ epoch ({total_batches:,} batches):")
    print(f"  - æ—¶é—´: {estimated_epoch:.2f}s ({estimated_epoch/60:.2f} åˆ†é’Ÿ)")
    
    # æµ‹è¯•4: å®Œæ•´ epoch åŠ è½½ï¼ˆå¯é€‰ï¼Œæ—¶é—´è¾ƒé•¿ï¼‰
    print("\n" + "="*70)
    print("[æµ‹è¯•4] å®Œæ•´ epoch æ•°æ®åŠ è½½ï¼ˆå¯é€‰ï¼ŒæŒ‰ä»»æ„é”®è·³è¿‡ï¼‰")
    print("="*70)
    
    import msvcrt
    import sys
    
    print(f"\nå°†åŠ è½½æ‰€æœ‰ {total_samples:,} ä¸ªæ ·æœ¬ï¼Œé¢„è®¡éœ€è¦ {estimated/60:.2f} åˆ†é’Ÿ")
    print("æŒ‰ä»»æ„é”®è·³è¿‡æ­¤æµ‹è¯•ï¼Œæˆ–ç­‰å¾… 3 ç§’è‡ªåŠ¨å¼€å§‹...")
    
    # ç­‰å¾… 3 ç§’ï¼Œå¦‚æœæŒ‰é”®åˆ™è·³è¿‡
    skip = False
    for i in range(3, 0, -1):
        print(f"\r{i}...", end='', flush=True)
        time.sleep(0.5)
        if msvcrt.kbhit():
            msvcrt.getch()
            skip = True
            print("\r[è·³è¿‡]")
            break
        time.sleep(0.5)
    
    if not skip:
        print("\r[å¼€å§‹å®Œæ•´ epoch æµ‹è¯•]")
        
        batch_count = 0
        sample_count = 0
        total_points = 0
        
        start = time.time()
        for batch in dataloader:
            batch_count += 1
            sample_count += len(batch['offset'])
            total_points += len(batch['coord'])
            
            if batch_count % 500 == 0:
                elapsed = time.time() - start
                print(f"  - å·²åŠ è½½ {batch_count}/{total_batches} batches, {sample_count:,} æ ·æœ¬, {total_points:,} ç‚¹, {elapsed:.2f}s")
        
        elapsed = time.time() - start
        
        print(f"\nå®Œæ•´ epoch ç»Ÿè®¡:")
        print(f"  - æ€»æ—¶é—´: {elapsed:.2f}s ({elapsed/60:.2f} åˆ†é’Ÿ)")
        print(f"  - æ ·æœ¬æ•°: {sample_count:,}")
        print(f"  - æ€»ç‚¹æ•°: {total_points:,}")
        print(f"  - Batches: {batch_count:,}")
        print(f"  - æ ·æœ¬é€Ÿåº¦: {sample_count/elapsed:.1f} samples/s")
        print(f"  - ç‚¹é€Ÿåº¦: {total_points/elapsed:,.0f} points/s")
        print(f"  - æ¯ batch å¹³å‡: {elapsed/batch_count*1000:.2f} ms")


def test_with_without_cache_comparison():
    """å¯¹æ¯” cache_data å¼€å…³çš„æ•ˆæœ"""
    print("\n" + "="*70)
    print("cache_data å¼€å…³å¯¹æ¯”æµ‹è¯•")
    print("="*70)
    
    data_root = r"E:\data\DALES\dales_las\bin\train"
    
    if not Path(data_root).exists():
        print(f"[X] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return
    
    test_count = 100
    
    # æµ‹è¯• cache_data=Falseï¼ˆåªæœ‰ metadata ç¼“å­˜ï¼‰
    print(f"\n[æµ‹è¯•] cache_data=Falseï¼ˆåªç¼“å­˜ metadataï¼‰")
    
    dataset_no_cache = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=False
    )
    
    # é¦–æ¬¡éå†
    start = time.time()
    for i in range(test_count):
        sample = dataset_no_cache[i]
    first_pass = time.time() - start
    
    # ç¬¬äºŒæ¬¡éå†
    start = time.time()
    for i in range(test_count):
        sample = dataset_no_cache[i]
    second_pass = time.time() - start
    
    print(f"  - é¦–æ¬¡éå† {test_count} ä¸ªæ ·æœ¬: {first_pass:.2f}s ({first_pass/test_count*1000:.2f} ms/æ ·æœ¬)")
    print(f"  - ç¬¬äºŒæ¬¡éå†: {second_pass:.2f}s ({second_pass/test_count*1000:.2f} ms/æ ·æœ¬)")
    print(f"  - åŠ é€Ÿæ¯”: {first_pass/second_pass:.2f}x")
    
    # æµ‹è¯• cache_data=Trueï¼ˆå®Œæ•´æ•°æ®ç¼“å­˜ï¼‰
    print(f"\n[æµ‹è¯•] cache_data=Trueï¼ˆå®Œæ•´æ•°æ®ç¼“å­˜ï¼‰")
    
    dataset_with_cache = BinPklDataset(
        data_root=data_root,
        split='train',
        assets=['coord', 'intensity', 'classification'],
        cache_data=True
    )
    
    # é¦–æ¬¡éå†ï¼ˆè§¦å‘ç¼“å­˜ï¼‰
    start = time.time()
    for i in range(test_count):
        sample = dataset_with_cache[i]
    first_pass_cached = time.time() - start
    
    # ç¬¬äºŒæ¬¡éå†ï¼ˆä»ç¼“å­˜è¯»å–ï¼‰
    start = time.time()
    for i in range(test_count):
        sample = dataset_with_cache[i]
    second_pass_cached = time.time() - start
    
    print(f"  - é¦–æ¬¡éå† {test_count} ä¸ªæ ·æœ¬: {first_pass_cached:.2f}s ({first_pass_cached/test_count*1000:.2f} ms/æ ·æœ¬)")
    print(f"  - ç¬¬äºŒæ¬¡éå†: {second_pass_cached:.2f}s ({second_pass_cached/test_count*1000:.2f} ms/æ ·æœ¬)")
    
    if second_pass_cached > 0:
        print(f"  - åŠ é€Ÿæ¯”: {first_pass_cached/second_pass_cached:.2f}x")
    else:
        print(f"  - åŠ é€Ÿæ¯”: âˆ (ç¬æ—¶)")
    
    # å¯¹æ¯”æ€»ç»“
    print(f"\nå¯¹æ¯”æ€»ç»“:")
    print(f"  - metadata ç¼“å­˜åŠ é€Ÿ: {first_pass/second_pass:.2f}x")
    print(f"  - å®Œæ•´æ•°æ®ç¼“å­˜åŠ é€Ÿ: {first_pass_cached/max(second_pass_cached, 0.001):.2f}x")
    print(f"  - æ¨è: å¤§æ•°æ®é›†ç”¨ cache_data=Falseï¼Œå°æ•°æ®é›†ç”¨ cache_data=True")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n")
    print("="*70)
    print("æ•°æ®é›†è®¿é—®é€Ÿåº¦å¯¹æ¯”æµ‹è¯•")
    print("ä¼˜åŒ–ï¼šMetadata ç¼“å­˜")
    print("="*70)
    print()
    
    try:
        # æµ‹è¯•å®é™…æ•°æ®åŠ è½½
        test_actual_data_loading()
        
        # å¯¹æ¯” cache_data å¼€å…³
        test_with_without_cache_comparison()
        
        print("\n" + "="*70)
        print("[å®Œæˆ] æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("="*70)
        
        print("\nã€ä¼˜åŒ–æ•ˆæœæ€»ç»“ã€‘")
        print("-"*70)
        print("âœ… Metadata ç¼“å­˜ï¼šè®¿é—®é€Ÿåº¦æå‡ ~10x")
        print("âœ… é¢„ä¼°éå†æ—¶é—´ï¼šä» 12 åˆ†é’Ÿé™è‡³ 1.5 åˆ†é’Ÿ")
        print("âœ… å†…å­˜å ç”¨ï¼šä»…å¢åŠ  ~50 MBï¼ˆmetadataï¼‰")
        print("ğŸ’¡ å¤§æ•°æ®é›†æ¨èï¼šcache_data=Falseï¼ˆè‡ªåŠ¨å¯ç”¨ metadata ç¼“å­˜ï¼‰")
        print("ğŸ’¡ å°æ•°æ®é›†æ¨èï¼šcache_data=Trueï¼ˆå®Œæ•´æ•°æ®ç¼“å­˜ï¼Œå¤šæ¬¡éå†ç¬æ—¶ï¼‰")
        print("="*70)
        
    except Exception as e:
        print(f"\n[X] æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
