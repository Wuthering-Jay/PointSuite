# PointSuite 训练配置示例
# 使用方法：python train_example.py

seed_everything: 42

# ============= Trainer 配置 =============
trainer:
  max_epochs: 100
  accelerator: gpu
  devices: 1
  precision: 16-mixed  # 使用混合精度训练
  
  # 梯度相关
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  
  # 验证相关
  val_check_interval: 1.0  # 每个 epoch 验证一次
  check_val_every_n_epoch: 1
  
  # 日志相关
  log_every_n_steps: 50
  
  # 回调
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: checkpoints/
        filename: "epoch={epoch}-val_loss={val/total_loss:.4f}"
        monitor: val/total_loss
        mode: min
        save_top_k: 3
        save_last: true
        auto_insert_metric_name: false
    
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/total_loss
        patience: 20
        mode: min
        verbose: true
    
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

# ============= Model 配置 =============
model:
  class_path: pointsuite.tasks.SemanticSegmentationTask
  init_args:
    learning_rate: 0.001
    
    # Backbone 配置
    backbone:
      class_path: pointsuite.models.backbones.PointTransformerV2m5
      init_args:
        in_channels: 6  # coord(3) + h_norm(1) + intensity(1) + echo(1)
        num_classes: 8  # 根据你的数据集调整
        patch_embed_depth: 1
        patch_embed_channels: 48
        patch_embed_groups: 6
        patch_embed_neighbours: 16
        enc_depths: [2, 2, 6, 2]
        enc_channels: [48, 96, 192, 384]
        enc_num_head: [3, 6, 12, 24]
        enc_patch_size: [128, 128, 128, 128]
        dec_depths: [1, 1, 1, 1]
        dec_channels: [48, 96, 192, 384]
        dec_num_head: [3, 6, 12, 24]
        dec_patch_size: [128, 128, 128, 128]
    
    # Head 配置
    head:
      class_path: pointsuite.models.heads.SegmentationHead
      init_args:
        in_channels: 48  # 与 backbone 的输出通道一致
        num_classes: 8
        hidden_channels: 64
    
    # 损失函数配置
    loss_configs:
      - type: pointsuite.models.losses.CrossEntropyLoss
        weight: 1.0
        init_args:
          ignore_index: -1  # 忽略标签
    
    # 指标配置
    metric_configs:
      - type: pointsuite.utils.metrics.OverallAccuracy
        name: OA
        init_args:
          ignore_index: -1
      
      - type: pointsuite.utils.metrics.MeanIoU
        name: mIoU
        init_args:
          num_classes: 8
          ignore_index: -1

# ============= Data 配置 =============
data:
  class_path: pointsuite.data.BinPklDataModule
  init_args:
    # 数据路径
    train_data: data/train  # 修改为你的数据路径
    val_data: data/val
    test_data: data/test
    
    # DataLoader 参数
    batch_size: 8
    num_workers: 4
    pin_memory: true
    persistent_workers: true
    
    # Dataset 参数
    assets: [coord, intensity, echo, h_norm, classification]
    ignore_label: -1
    cache_data: false
    
    # Loop 参数 (TTA)
    train_loop: 1
    val_loop: 1
    test_loop: 1
    
    # 动态 Batch 配置
    use_dynamic_batch: true
    max_points: 500000  # 训练时每批最大点数
    use_dynamic_batch_inference: true
    max_points_inference: 800000  # 推理时可以更大
    
    # 加权采样（类别不平衡）
    use_weighted_sampler: false
    # train_sampler_weights: null  # 如果需要，手动计算并传入
    
    # 类别映射（如果类别不连续）
    class_mapping: null
    # class_mapping:
    #   0: 0  # 噪声 -> 0
    #   1: 1  # 地面 -> 1
    #   2: 2  # 植被 -> 2
    #   6: 3  # 建筑 -> 3
    #   9: 4  # 电线 -> 4
    
    # Transforms 配置
    train_transforms:
      - class_path: pointsuite.data.transforms.CenterShift
        init_args:
          apply_z: true
      
      - class_path: pointsuite.data.transforms.RandomRotate
        init_args:
          angle: [-180, 180]
          axis: z
          center: [0, 0, 0]
          p: 0.5
      
      - class_path: pointsuite.data.transforms.RandomScale
        init_args:
          scale: [0.9, 1.1]
          anisotropic: false
          p: 0.5
      
      - class_path: pointsuite.data.transforms.AutoNormalizeHNorm
        init_args:
          clip_range: null
      
      - class_path: pointsuite.data.transforms.Collect
        init_args:
          keys: [coord, class]
          offset_key:
            offset: coord
          feat_keys:
            feat: [coord, h_norm, intensity, echo]
      
      - class_path: pointsuite.data.transforms.ToTensor
    
    val_transforms:
      - class_path: pointsuite.data.transforms.CenterShift
        init_args:
          apply_z: true
      
      - class_path: pointsuite.data.transforms.AutoNormalizeHNorm
        init_args:
          clip_range: null
      
      - class_path: pointsuite.data.transforms.Collect
        init_args:
          keys: [coord, class]
          offset_key:
            offset: coord
          feat_keys:
            feat: [coord, h_norm, intensity, echo]
      
      - class_path: pointsuite.data.transforms.ToTensor
    
    test_transforms:
      - class_path: pointsuite.data.transforms.CenterShift
        init_args:
          apply_z: true
      
      - class_path: pointsuite.data.transforms.AutoNormalizeHNorm
        init_args:
          clip_range: null
      
      - class_path: pointsuite.data.transforms.Collect
        init_args:
          keys: [coord, class]
          offset_key:
            offset: coord
          feat_keys:
            feat: [coord, h_norm, intensity, echo]
      
      - class_path: pointsuite.data.transforms.ToTensor

# ============= Optimizer 配置 =============
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001
    weight_decay: 0.0001
    betas: [0.9, 0.999]

# ============= LR Scheduler 配置 =============
lr_scheduler:
  class_path: torch.optim.lr_scheduler.CosineAnnealingLR
  init_args:
    T_max: 100  # max_epochs
    eta_min: 0.00001
