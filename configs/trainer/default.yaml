# ════════════════════════════════════════════════════════════════════════════
# 默认训练器配置 (Default Trainer Configuration)
# ════════════════════════════════════════════════════════════════════════════
#
# PyTorch Lightning Trainer 配置
# 此文件定义训练过程相关的所有配置，可在 experiment yaml 中覆盖任意参数
#
# 配置结构:
#   - training: 训练循环参数 (epochs, precision, gradient)
#   - logging: 日志输出参数
#   - callbacks: 回调函数配置
#   - optimizer: 优化器配置
#   - lr_scheduler: 学习率调度器配置
#   - misc: 其他杂项配置
# ════════════════════════════════════════════════════════════════════════════

# ════════════════════════════════════════════════════════════════════════════
# 训练配置 (Training Configuration)
# ════════════════════════════════════════════════════════════════════════════
training:
  # 最大训练轮数
  max_epochs: 100
  
  # 设备数量: GPU/TPU 数量
  devices: 1
  
  # 加速器类型:
  #   - auto: 自动检测 (推荐)
  #   - gpu: 强制使用 GPU
  #   - cpu: 强制使用 CPU
  #   - tpu: 使用 TPU
  accelerator: auto
  
  # 训练精度:
  #   - 32: 全精度 FP32
  #   - 16-mixed: 混合精度 (FP16 + FP32)，显著减少显存占用
  #   - bf16-mixed: BF16 混合精度 (需要 Ampere 架构以上 GPU)
  precision: 16-mixed
  
  # ─────────────────────────────────────────────────────────────
  # 梯度配置
  # ─────────────────────────────────────────────────────────────
  # 梯度累积步数: 等效增大 batch_size
  # 实际 batch_size = batch_size * accumulate_grad_batches
  accumulate_grad_batches: 2
  
  # 梯度裁剪值: 防止梯度爆炸
  # null 表示不裁剪
  gradient_clip_val: 1.0
  
  # 梯度裁剪算法:
  #   - norm: 按 L2 范数裁剪 (推荐)
  #   - value: 按绝对值裁剪
  gradient_clip_algorithm: norm
  
  # ─────────────────────────────────────────────────────────────
  # 验证配置
  # ─────────────────────────────────────────────────────────────
  # 训练前验证步数: 用于检查验证流程是否正常
  num_sanity_val_steps: 2
  
  # 验证频率: 每 N 个 epoch 验证一次
  check_val_every_n_epoch: 1
  
  # 验证检查间隔: 
  #   - 1.0: 每个 epoch 结束后验证
  #   - 0.5: 每半个 epoch 验证一次
  #   - 100: 每 100 个 batch 验证一次 (整数)
  val_check_interval: 1.0
  
  # 限制训练批次数:
  #   - null: 使用全部数据
  #   - 0.1: 使用 10% 数据 (用于调试)
  #   - 100: 只使用 100 个批次
  limit_train_batches: null

# ════════════════════════════════════════════════════════════════════════════
# 日志配置 (Logging Configuration)
# ════════════════════════════════════════════════════════════════════════════
logging:
  # 日志记录间隔: 每 N 步记录一次
  log_every_n_steps: 10
  
  # 是否显示进度条:
  #   - true: 显示 tqdm 进度条
  #   - false: 不显示 (使用自定义文本日志)
  enable_progress_bar: false
  
  # 是否显示模型摘要:
  #   - true: 训练开始时打印模型结构和参数量
  enable_model_summary: true
  
  # 文本日志回调配置 (自定义的控制台日志)
  text_logging:
    enabled: true         # 是否启用文本日志
    log_interval: 10      # 每 N 个 batch 打印一次训练信息

# ════════════════════════════════════════════════════════════════════════════
# 回调配置 (Callbacks Configuration)
# ════════════════════════════════════════════════════════════════════════════
# 回调函数在训练的不同阶段自动执行特定逻辑
callbacks:
  # ─────────────────────────────────────────────────────────────
  # 模型检查点 (Model Checkpoint)
  # ─────────────────────────────────────────────────────────────
  # 自动保存最佳模型和定期检查点
  model_checkpoint:
    # 监控指标: 根据此指标选择最佳模型
    monitor: mean_iou
    # 监控模式: max (越大越好) / min (越小越好)
    mode: max
    # 保存最佳 K 个模型
    save_top_k: 3
    # 是否保存最后一个 epoch 的模型
    save_last: true
    # 文件名格式: 支持 {epoch}, {step}, {metric_name} 等占位符
    filename: '{epoch:02d}-{mean_iou:.4f}'
    # 是否打印保存信息
    verbose: true
    
  # ─────────────────────────────────────────────────────────────
  # 早停 (Early Stopping)
  # ─────────────────────────────────────────────────────────────
  # 当验证指标不再提升时提前停止训练
  early_stopping:
    enabled: true         # 是否启用早停
    monitor: mean_iou     # 监控指标
    patience: 20          # 容忍 N 个 epoch 无提升
    mode: max             # 监控模式
    verbose: true         # 是否打印早停信息
    
  # ─────────────────────────────────────────────────────────────
  # 自动显存清理 (Auto Empty Cache)
  # ─────────────────────────────────────────────────────────────
  # 检测训练变慢并自动清理 GPU 显存
  auto_empty_cache:
    enabled: true
    # 相对变慢阈值: 当前 step 耗时超过平均的 N 倍时触发
    slowdown_threshold: 3.0
    # 绝对耗时阈值: 当前 step 耗时超过 N 秒时触发
    absolute_threshold: 1.5
    # 清理间隔: 每 N 步强制清理一次 (0 表示不强制)
    clear_interval: 0
    # 热身步数: 前 N 步不触发清理
    warmup_steps: 10
    # 是否打印清理信息
    verbose: true
    
  # ─────────────────────────────────────────────────────────────
  # 预测结果写入器 (Predict Writer) - 默认配置
  # ─────────────────────────────────────────────────────────────
  # 将预测结果保存为文件，通常在 experiment yaml 中覆盖
  # predict_writer:
  #   enabled: false

# ════════════════════════════════════════════════════════════════════════════
# 优化器配置 (Optimizer Configuration)
# ════════════════════════════════════════════════════════════════════════════
optimizer:
  # 优化器类路径
  class_path: AdamW
  init_args:
    # 学习率: 最重要的超参数之一
    lr: 0.001
    # 权重衰减: L2 正则化系数，防止过拟合
    weight_decay: 0.0001
    # 可选参数:
    # betas: [0.9, 0.999]    # Adam 的动量参数
    # eps: 1e-8              # 数值稳定性

# ════════════════════════════════════════════════════════════════════════════
# 学习率调度器配置 (LR Scheduler Configuration)
# ════════════════════════════════════════════════════════════════════════════
lr_scheduler:
  # 调度器类路径
  class_path: CosineAnnealingLR
  init_args:
    # T_max: 余弦周期长度
    #   - null: 自动设置为 total_steps (一个完整余弦周期)
    #   - 整数: 手动指定周期长度
    T_max: null
    # 最小学习率: 余弦退火的下限
    eta_min: 0.000001
    
  # 调度间隔: step (每步更新) / epoch (每轮更新)
  interval: step
  
  # 调度频率: 每 N 个 interval 更新一次
  frequency: 1

# ════════════════════════════════════════════════════════════════════════════
# 其他配置 (Miscellaneous Configuration)
# ════════════════════════════════════════════════════════════════════════════
misc:
  # 确定性模式: 
  #   - true: 保证完全可复现，但可能降低性能
  #   - false: 允许非确定性操作，性能更好
  deterministic: false
  
  # cuDNN 基准测试:
  #   - true: 自动选择最优卷积算法，首次运行较慢但后续更快
  #   - false: 使用默认算法
  benchmark: true
