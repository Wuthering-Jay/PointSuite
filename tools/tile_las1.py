import os
import numpy as np
import laspy
import pickle
import time
import multiprocessing
import math
from pathlib import Path
from typing import Union, List, Tuple, Optional, Dict, Any
from tqdm import tqdm
from numba import jit, prange
from sklearn.neighbors import KDTree

# ============================================================================
# ç¾åŒ–è¾“å‡ºè¾…åŠ©ç±»
# ============================================================================

class Colors:
    """ANSI é¢œè‰²ä»£ç """
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    RESET = '\033[0m'

def format_size(size_bytes: float) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} TB"

def format_time(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´"""
    if seconds < 1:
        return f"{seconds*1000:.0f}ms"
    elif seconds < 60:
        return f"{seconds:.2f}s"
    else:
        return f"{seconds/60:.2f}min"

def format_number(num: int) -> str:
    """æ ¼å¼åŒ–å¤§æ•°å­—ï¼ˆåƒåˆ†ä½åˆ†éš”ï¼‰"""
    return f"{num:,}"

# ============================================================================
# Numba åŠ é€Ÿå‡½æ•° (å“ˆå¸Œä¸åæ ‡è®¡ç®—)
# ============================================================================

@jit(nopython=True, parallel=True)
def compute_grid_coord_numba(coord, grid_size):
    """è®¡ç®—ç½‘æ ¼åæ ‡"""
    n = coord.shape[0]
    grid_coord = np.empty_like(coord, dtype=np.int64)
    for i in prange(n):
        for j in range(3):
            grid_coord[i, j] = np.floor(coord[i, j] / grid_size)
    return grid_coord

@jit(nopython=True, parallel=True)
def ravel_hash_vec_numba(arr, arr_min, arr_max):
    """è®¡ç®—ç©ºé—´å“ˆå¸Œå€¼"""
    n = arr.shape[0]
    d = arr.shape[1]
    keys = np.zeros(n, dtype=np.uint64)
    
    # å½’ä¸€åŒ–å¹¶è½¬æ¢ä¸º uint64
    arr_normalized = np.empty_like(arr, dtype=np.uint64)
    for i in prange(n):
        for j in range(d):
            arr_normalized[i, j] = np.uint64(arr[i, j] - arr_min[j])
    
    # è®¡ç®—æ¯ä¸€ç»´åº¦çš„è·¨åº¦
    arr_max_plus_one = np.empty(d, dtype=np.uint64)
    for j in range(d):
        arr_max_plus_one[j] = np.uint64(arr_max[j] - arr_min[j] + 1)
    
    # Fortran style flatten
    for i in prange(n):
        key = np.uint64(0)
        for j in range(d - 1):
            key += arr_normalized[i, j]
            key *= arr_max_plus_one[j + 1]
        key += arr_normalized[i, d - 1]
        keys[i] = key
    
    return keys

# ============================================================================
# æ ¸å¿ƒå¤„ç†ç±»
# ============================================================================

class LASProcessorLogicalIndex:
    def __init__(self,
                 input_path: Union[str, Path],
                 output_dir: Union[str, Path] = None,
                 window_size: Tuple[float, float] = (50.0, 50.0),
                 overlap: bool = False,
                 grid_size: float = 0.5,      # ä»…ç”¨äºç”Ÿæˆé€»è¾‘ç´¢å¼•ï¼Œä¸è¿›è¡Œç‰©ç†é™é‡‡æ ·
                 min_points: int = 1000,
                 max_points: int = 5000,      # é€šå¸¸ä¸å†éœ€è¦å¼ºåˆ¶åˆ‡åˆ†ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰å®Œç¾çš„batchæ§åˆ¶
                 ground_class: Optional[int] = 2):
        
        self.input_path = Path(input_path)
        self.output_dir = Path(output_dir) if output_dir else self.input_path.parent
        self.window_size = window_size
        self.overlap = overlap
        # overlap_ratio = 0.5 if overlap else 0.0
        self.grid_size = grid_size
        self.min_points = min_points
        self.max_points = max_points
        self.ground_class = ground_class
        
        # è®¡ç®—æ­¥é•¿ (Stride)
        # self.stride = (
        #     window_size[0] * (1 - overlap_ratio),
        #     window_size[1] * (1 - overlap_ratio)
        # )
        
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True)
            
        self.las_files = self._find_las_files()

    def _find_las_files(self) -> List[Path]:
        """
        æŸ¥æ‰¾è¾“å…¥è·¯å¾„ä¸‹çš„æ‰€æœ‰ LAS/LAZ æ–‡ä»¶
        """
        if self.input_path.is_file():
            return [self.input_path]
        elif self.input_path.is_dir():
            return sorted(list(self.input_path.glob('*.las')) + list(self.input_path.glob('*.laz')))
        else:
            raise ValueError(f"Invalid path: {self.input_path}")

    def process_all_files(self, n_workers=None):
        """
        å¤„ç†æ‰€æœ‰ LAS/LAZ æ–‡ä»¶
        """

        if n_workers is None:
            n_workers = max(1, multiprocessing.cpu_count() - 1)

        start_time = time.time()

        # ç¾åŒ–çš„æ ‡é¢˜è¾“å‡º
        print(f"\n{Colors.BOLD}{'â•'*70}{Colors.RESET}")
        print(f"{Colors.BOLD}{Colors.CYAN}  ğŸš€ LAS é€»è¾‘ç´¢å¼•åˆ†å—å¤„ç†å™¨ (Logical Index Tiling){Colors.RESET}")
        print(f"{Colors.BOLD}{'â•'*70}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ€»æ–‡ä»¶æ•°: {Colors.GREEN}{len(self.las_files)}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} CPU æ ¸å¿ƒ: {Colors.GREEN}{n_workers}{Colors.RESET}")
        grid_size_str = f"{self.grid_size}m" if self.grid_size is not None else "è·³è¿‡ä½“ç´ åŒ–"
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç½‘æ ¼å¤§å°: {Colors.YELLOW}{grid_size_str}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} çª—å£å¤§å°: {Colors.YELLOW}{self.window_size}m{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} é‡å æ¨¡å¼: {Colors.GREEN if self.overlap else Colors.DIM}{'æ˜¯' if self.overlap else 'å¦'}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç‚¹æ•°èŒƒå›´: {Colors.YELLOW}{self.min_points} ~ {self.max_points or 'æ— é™åˆ¶'}{Colors.RESET}")
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} åœ°é¢ç±»åˆ«: {Colors.YELLOW}{self.ground_class or 'æœªæŒ‡å®š'}{Colors.RESET}")
        print(f"{Colors.BOLD}{'â”€'*70}{Colors.RESET}\n")
        
        # é¡ºåºå¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼Œä½†æ–‡ä»¶å†…éƒ¨å¹¶è¡Œå¤„ç†segments
        for idx, las_file in enumerate(self.las_files, 1):
            try:
                self.process_file(las_file, n_workers=n_workers, file_idx=idx, total_files=len(self.las_files))
            except Exception as e:
                print(f"\n{Colors.RED}[ERROR] {las_file.name}: {e}{Colors.RESET}")
                import traceback
                traceback.print_exc()

        elapsed = time.time() - start_time
        
        # ç¾åŒ–çš„å®Œæˆè¾“å‡º
        print(f"\n{Colors.BOLD}{'â•'*70}{Colors.RESET}")
        print(f"{Colors.BOLD}{Colors.GREEN}  âœ… å¤„ç†å®Œæˆ!{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} â±ï¸  æ€»è€—æ—¶: {Colors.CYAN}{format_time(elapsed)}{Colors.RESET}")
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} ğŸ“„ å¹³å‡æ¯æ–‡ä»¶: {Colors.CYAN}{format_time(elapsed/len(self.las_files))}{Colors.RESET}")
        print(f"{Colors.BOLD}{'â•'*70}{Colors.RESET}\n")

    def process_file(self, las_file: Path, n_workers=None, file_idx=1, total_files=1):

        print(f"{Colors.BOLD}{'â”€'*70}{Colors.RESET}")
        print(f"{Colors.BOLD}{Colors.BLUE}  ğŸ“„ [{file_idx}/{total_files}] {las_file.name}{Colors.RESET}")
        print(f"{Colors.BOLD}{'â”€'*70}{Colors.RESET}")
        file_start = time.time()

        # 1. è¯»å–æ•°æ®
        t0 = time.time()
        with laspy.open(las_file) as fh:
            las_data = fh.read()
        t1 = time.time()
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ğŸ“– è¯»å–LAS: {Colors.GREEN}{format_time(t1-t0)}{Colors.RESET} â†’ {Colors.CYAN}{format_number(len(las_data.points))}{Colors.RESET} ç‚¹")
            
        # è·å–åæ ‡ (laspy é»˜è®¤è¿”å› float64)
        t0 = time.time()
        points = np.vstack((las_data.x, las_data.y, las_data.z)).transpose()
        t1 = time.time()
        
        # 2. æ»‘åŠ¨çª—å£åˆ‡å— (è·å–ç´¢å¼•åˆ—è¡¨)
        t0 = time.time()
        result = self.segment_point_cloud(points, n_workers=n_workers)
        segments_indices, seg1_count, seg2_count = result
        t1 = time.time()
        
        # æ˜¾ç¤ºåˆ†å—ä¿¡æ¯
        if self.overlap and seg1_count is not None:
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ğŸ”² åˆ†å—å¤„ç†: {Colors.GREEN}{format_time(t1-t0)}{Colors.RESET} â†’ {Colors.CYAN}{len(segments_indices)}{Colors.RESET} å— ({seg1_count} + {seg2_count})")
        else:
            print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ğŸ”² åˆ†å—å¤„ç†: {Colors.GREEN}{format_time(t1-t0)}{Colors.RESET} â†’ {Colors.CYAN}{len(segments_indices)}{Colors.RESET} å—")
        
        # 3. å¤„ç†å¹¶ä¿å­˜
        t0 = time.time()
        self._save_bin_pkl(las_file, las_data, segments_indices)
        t1 = time.time()
        
        # æ€»è€—æ—¶
        total_time = time.time() - file_start
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} â±ï¸  æ–‡ä»¶æ€»è€—æ—¶: {Colors.BOLD}{Colors.GREEN}{format_time(total_time)}{Colors.RESET}")

    def segment_point_cloud(self, points: np.ndarray, n_workers: int = 4) -> List[np.ndarray]:
        """
        Segment point cloud into tiles based on window size.
        
        Args:
            points: Point cloud array (N, 3)
            n_workers: Number of parallel workers
            
        Returns:
            List of segment indices
        """
        import time
        
        if not self.overlap:
            # æ­£å¸¸æ¨¡å¼ï¼šå•æ¬¡ç½‘æ ¼åˆ†å‰²
            t0 = time.time()
            segments = self._grid_segmentation(points, offset_x=0, offset_y=0, n_workers=n_workers, show_details=False)
            return segments, None, None
        else:
            # Overlapæ¨¡å¼ï¼šä¸¤æ¬¡ç½‘æ ¼åˆ†å‰²ï¼ˆåç§»åŠä¸ªçª—å£ï¼‰
            x_size, y_size = self.window_size
            
            # ç¬¬ä¸€æ¬¡åˆ†å‰²ï¼šæ­£å¸¸ç½‘æ ¼
            t0 = time.time()
            segments1 = self._grid_segmentation(points, offset_x=0, offset_y=0, n_workers=n_workers, show_details=False)
            
            # ç¬¬äºŒæ¬¡åˆ†å‰²ï¼šåç§»åŠä¸ªçª—å£
            segments2 = self._grid_segmentation(points, offset_x=x_size/2, offset_y=y_size/2, n_workers=n_workers, show_details=False)
            
            # åˆå¹¶ä¸¤æ¬¡åˆ†å‰²ç»“æœ
            all_segments = segments1 + segments2
            
            return all_segments, len(segments1), len(segments2)
        
    def _grid_segmentation(self, points: np.ndarray, offset_x: float = 0, offset_y: float = 0, n_workers: int = 4, show_details: bool = False) -> List[np.ndarray]:
        """
        Perform grid-based segmentation with optional offset.
        
        Args:
            points: Point cloud array (N, 3)
            offset_x: X offset for grid origin
            offset_y: Y offset for grid origin
            n_workers: Number of parallel workers
            show_details: Whether to print detailed progress
            
        Returns:
            List of segment indices
        """
        import time
        
       # 1. çª—å£åˆ†ç»„ (ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨ argsort ä»£æ›¿ where å¾ªç¯)
        t0 = time.time()
        x_size, y_size = self.window_size
        
        # è®¡ç®—åŸç‚¹
        min_x, min_y = np.min(points[:, 0]), np.min(points[:, 1])
        origin_x = min_x - offset_x
        origin_y = min_y - offset_y
        
        # è®¡ç®—çª—å£ç´¢å¼•
        # ä¼˜åŒ–ï¼šç›´æ¥è®¡ç®— long å‹ç´¢å¼•ï¼Œä¸è®¡ç®— num_windowsï¼Œé¿å…æº¢å‡ºé£é™©
        x_bins = ((points[:, 0] - origin_x) / x_size).astype(np.int64)
        y_bins = ((points[:, 1] - origin_y) / y_size).astype(np.int64)
        
        # ä½¿ç”¨ cantor pairing æˆ–ç±»ä¼¼çš„ hash æ–¹å¼ç»„åˆäºŒç»´ç´¢å¼•ï¼Œæˆ–è€…ç®€å•çš„å­—ç¬¦ä¸²ç»„åˆï¼ˆæ…¢ï¼‰
        # è¿™é‡Œä¸ºäº†é€Ÿåº¦ï¼Œå‡è®¾ y_bins èŒƒå›´ä¸ä¼šå¤ªå¤§ï¼Œä½¿ç”¨å¤§æ•°ä¹˜æ³•ç»„åˆ
        # å‡è®¾ y æ–¹å‘ä¸ä¼šè¶…è¿‡ 1,000,000 ä¸ª grid
        y_multiplier = 1000000
        window_ids = x_bins * y_multiplier + y_bins
        
        # ğŸš€ æ ¸å¿ƒä¼˜åŒ–ï¼šä½¿ç”¨ argsort ä¸€æ¬¡æ€§åˆ†ç»„ï¼Œé¿å… N æ¬¡å…¨é‡æ‰«æ
        sort_idx = np.argsort(window_ids)
        sorted_window_ids = window_ids[sort_idx]
        
        # æ‰¾åˆ°åˆ‡åˆ†ç‚¹
        unique_ids, split_indices = np.unique(sorted_window_ids, return_index=True)
        # split_indices[0] æ˜¯ 0ï¼Œæˆ‘ä»¬éœ€è¦çš„åˆ‡åˆ†ç‚¹æ˜¯ split_indices[1:]
        # np.split ä¼šè¿”å›åˆ—è¡¨
        segments = np.split(sort_idx, split_indices[1:])
        
        # 2. Miné˜ˆå€¼å¤„ç†ï¼ˆä¼˜å…ˆå¤„ç†ï¼Œåˆå¹¶è¾¹ç•Œä¸Šç‚¹å°‘çš„æ— æ•ˆçª—å£ï¼‰
        if self.min_points is not None:
            before_count = len(segments)
            segments = self.apply_min_threshold(points, segments, min_threshold=self.min_points)
        
        # 3. Maxé˜ˆå€¼å¤„ç†ï¼ˆæœ€åå¤„ç†ï¼‰
        if self.max_points is not None:
            before_count = len(segments)
            segments = self.apply_max_threshold(points, segments, n_workers=n_workers)
            
        return segments
    
    def apply_max_threshold(self, points: np.ndarray, segments: List[np.ndarray], n_workers: int = 4) -> List[np.ndarray]:
        """
        Apply max_points threshold to segments, subdividing large segments.
        
        Args:
            points: Point cloud array
            segments: List of segment indices
            n_workers: Number of parallel workers
            
        Returns:
            List of processed segment indices
        """
        large_segment_indices = [i for i, segment in enumerate(segments) if len(segment) > self.max_points]
        
        if not large_segment_indices:
            return segments
        
        result_segments = [segment for i, segment in enumerate(segments) if i not in large_segment_indices]
        large_segments = [segments[i] for i in large_segment_indices]
        
        def process_segment(segment):
            if len(segment) <= self.max_points:
                return [segment]
            
            segment_points = points[segment]
            ranges = np.ptp(segment_points[:, :2], axis=0)
            split_dim = np.argmax(ranges[:2])
            sorted_indices = np.argsort(segment_points[:, split_dim])
            
            mid = len(sorted_indices) // 2
            left_half = segment[sorted_indices[:mid]]
            right_half = segment[sorted_indices[mid:]]
            
            result = []
            result.extend(process_segment(left_half))
            result.extend(process_segment(right_half))
            return result
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # ä½¿ç”¨ä¼ å…¥çš„n_workerså‚æ•°
        max_workers = min(n_workers, len(large_segments)) if len(large_segments) > 0 else 1
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_segment, segment) for segment in large_segments]
            for future in as_completed(futures):
                result_segments.extend(future.result())
        
        return result_segments
    
    def _process_single_segment(self, args) -> dict:
        """
        å¤„ç†å•ä¸ª segment çš„ä½“ç´ åŒ–ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰
        """
        i, indices, lx, ly, lz, l_class, bin_name, pkl_name = args
        
        # 1. æå–å½“å‰å—åæ ‡ (Float64)
        seg_points = np.column_stack((lx[indices], ly[indices], lz[indices]))
        
        # 2. å±€éƒ¨åæ ‡å½’ä¸€åŒ–
        local_min = seg_points.min(0)
        local_points = (seg_points - local_min).astype(np.float64)
        
        # 3. ä½“ç´ åŒ–å¤„ç† (å¦‚æœ grid_size ä¸º None åˆ™è·³è¿‡)
        if self.grid_size is not None:
            # è®¡ç®— Grid Hash (ä½¿ç”¨çº¯ NumPy å‘é‡åŒ–ï¼Œé¿å… Numba JIT å¼€é”€)
            grid_coord = np.floor(local_points / self.grid_size).astype(np.int64)
            
            # ç¡®ä¿éè´Ÿä¸”ç´§å‡‘
            if len(grid_coord) > 0:
                grid_min = grid_coord.min(0)
                grid_coord -= grid_min
                arr_max = grid_coord.max(0)
                
                # å‘é‡åŒ– ravel hash (Fortran style)
                multipliers = np.cumprod(np.concatenate([[1], arr_max[1:] + 1])).astype(np.uint64)
                keys = (grid_coord.astype(np.uint64) * multipliers).sum(axis=1)
            else:
                keys = np.zeros(0, dtype=np.uint64)
            
            # ç”Ÿæˆé€»è¾‘æ’åºç´¢å¼•
            sort_ptr = np.argsort(keys, kind='mergesort').astype(np.int32)
            keys_sorted = keys[sort_ptr]
            
            # è®¡ç®—ä½“ç´ ç»Ÿè®¡
            _, voxel_counts = np.unique(keys_sorted, return_counts=True)
        else:
            # è·³è¿‡ä½“ç´ åŒ–ï¼šä¿æŒåŸå§‹é¡ºåº
            sort_ptr = np.arange(len(indices), dtype=np.int32)
            voxel_counts = np.array([len(indices)], dtype=np.int64)  # å•ä¸ª"ä½“ç´ "åŒ…å«æ‰€æœ‰ç‚¹
        
        # 4. ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        label_counts = {}
        unique_labels = []
        if l_class is not None:
            seg_labels = l_class[indices]
            unique_labels, u_counts = np.unique(seg_labels, return_counts=True)
            label_counts = {int(k): int(v) for k, v in zip(unique_labels, u_counts)}
        
        # 5. è®¡ç®—è¾¹ç•Œæ¡†
        bounds = {
            'x_min': float(seg_points[:, 0].min()),
            'x_max': float(seg_points[:, 0].max()),
            'y_min': float(seg_points[:, 1].min()),
            'y_max': float(seg_points[:, 1].max()),
            'z_min': float(seg_points[:, 2].min()),
            'z_max': float(seg_points[:, 2].max())
        }

        return {
            'segment_id': i,
            'indices': indices,
            'num_points': len(indices),
            'sort_idx': sort_ptr,
            'voxel_counts': voxel_counts,
            'num_voxels': len(voxel_counts),
            'max_voxel_density': voxel_counts.max() if len(voxel_counts) > 0 else 0,
            'local_min': local_min,
            'label_counts': label_counts,
            'unique_labels': unique_labels,
            'bounds': bounds,
            'bin_path': bin_name,
            'pkl_path': pkl_name
        }

    def _process_segments_parallel(self, segments, lx, ly, lz, l_class, bin_path, pkl_path, n_workers=None):
        """
        å¹¶è¡Œå¤„ç†æ‰€æœ‰ segments çš„ä½“ç´ åŒ–
        """
        from concurrent.futures import ThreadPoolExecutor
        
        if n_workers is None:
            n_workers = min(8, max(1, multiprocessing.cpu_count() - 1))
        
        bin_name = str(bin_path.name)
        pkl_name = str(pkl_path.name)
        
        # å‡†å¤‡å‚æ•°
        args_list = [
            (i, indices, lx, ly, lz, l_class, bin_name, pkl_name)
            for i, indices in enumerate(segments)
        ]
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        segments_info = []
        with ThreadPoolExecutor(max_workers=n_workers) as executor:
            results = list(executor.map(self._process_single_segment, args_list))
            segments_info = sorted(results, key=lambda x: x['segment_id'])
        
        return segments_info

    def apply_min_threshold(self, points: np.ndarray, segments: List[np.ndarray], 
                           min_threshold: Optional[int] = None) -> List[np.ndarray]:
        """
        Apply min_points threshold using KD-Tree.
        
        Args:
            points: Point cloud array
            segments: List of segment indices
            min_threshold: Minimum points threshold (if None, use self.min_points)
        
        Returns:
            List of processed segment indices
        """
        if len(segments) <= 1:
            return segments
        
        # ä½¿ç”¨ä¼ å…¥çš„min_thresholdï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨self.min_points
        effective_min = min_threshold if min_threshold is not None else self.min_points
        
        centroids = np.array([np.mean(points[segment][:, :2], axis=0) for segment in segments])
        small_segments = [i for i, segment in enumerate(segments) if len(segment) < effective_min]
        
        if not small_segments:
            return segments
        
        valid_indices = [i for i in range(len(segments)) if i not in small_segments]
        if not valid_indices:
            return segments
        
        valid_centroids = centroids[valid_indices]
        kdtree = KDTree(valid_centroids)
        
        small_segments.sort(key=lambda i: len(segments[i]))
        
        for small_idx in small_segments:
            if small_idx >= len(segments):
                continue
            
            _, nearest_idx = kdtree.query([centroids[small_idx]], k=1)
            nearest_idx = valid_indices[nearest_idx[0][0]]
            
            if nearest_idx != small_idx and nearest_idx < len(segments):
                segments[nearest_idx] = np.concatenate([segments[nearest_idx], segments[small_idx]])
                segments[small_idx] = np.array([], dtype=int)
        
        return [segment for segment in segments if len(segment) > 0]


    def _save_bin_pkl(self, las_file, las_data, segments):
        base_name = las_file.stem
        bin_path = self.output_dir / f"{base_name}.bin"
        pkl_path = self.output_dir / f"{base_name}.pkl"
        
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ğŸ’¾ ä¿å­˜æ–‡ä»¶...")
        t0 = time.time()

        # --- A. æ”¶é›†å­—æ®µ ---
        
        # 1. æ ¸å¿ƒå­—æ®µ: å¼ºåˆ¶ float64 ä¿è¯ç²¾åº¦
        core_fields = ['X', 'Y', 'Z']
        dtype_list = [('X', np.float64), ('Y', np.float64), ('Z', np.float64)]
        
        # åˆå§‹æ•°æ®å­—å…¸ (laspy.x å·²ç»æ˜¯ float64)
        data_dict = {
            'X': np.array(las_data.x, dtype=np.float64), 
            'Y': np.array(las_data.y, dtype=np.float64), 
            'Z': np.array(las_data.z, dtype=np.float64)
        }
        
        # 2. æ‰©å±•çš„å¯é€‰å­—æ®µåˆ—è¡¨
        optional_fields = [
            'intensity', 'return_number', 'number_of_returns', 
            'classification', 'scan_angle_rank', 'user_data', 
            'point_source_id', 'gps_time', 
            'red', 'green', 'blue', 'nir', 'edge_of_flight_line'
        ]
        
        # 3. åŠ¨æ€æ”¶é›†å­˜åœ¨çš„å­—æ®µ
        # æ³¨æ„: laspy å±æ€§æ˜¯å°å†™çš„ï¼Œä½†æˆ‘ä»¬ä¿å­˜çš„ key ç”¨æ ‡å‡†å(é€šå¸¸å¤§å†™æˆ–é©¼å³°ï¼Œä½†è¿™é‡Œä¿æŒå°å†™å±æ€§åå¯¹åº”çš„åŸå§‹å)
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œé™¤äº†XYZï¼Œå…¶ä»–å­—æ®µæˆ‘ä»¬ä½¿ç”¨å°å†™æˆ– laspy å±æ€§å
        has_classification = False
        fields_to_save = list(core_fields) # å…ˆåŠ å…¥æ ¸å¿ƒå­—æ®µ
        
        for field in optional_fields:
            # laspy å±æ€§é€šå¸¸æ˜¯å°å†™çš„
            field_lower = field.lower()
            
            if hasattr(las_data, field_lower):
                arr = getattr(las_data, field_lower)
                # ä½¿ç”¨å­—æ®µåŸåä½œä¸º key (å¦‚ 'red', 'intensity')
                # æ³¨æ„ï¼šXYZ æˆ‘ä»¬ç”¨å¤§å†™ï¼Œå…¶ä»–é€šå¸¸ç”¨å°å†™
                # è¿™é‡Œæˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨ field (åˆ—è¡¨ä¸­çš„åå­—) ä½œä¸º key
                data_dict[field] = arr
                dtype_list.append((field, arr.dtype))
                fields_to_save.append(field)
                
                if field_lower == 'classification':
                    has_classification = True
        
        # 4. å…œåº•å¤„ç†ï¼šå¦‚æœæ²¡æœ‰ classificationï¼Œè¡¥å…¨ä¸º 0
        if not has_classification:
            print(f"  {Colors.DIM}â”‚{Colors.RESET}  {Colors.YELLOW}âš ï¸  æ—  classification å­—æ®µï¼Œè¡¥å…¨ä¸º 0{Colors.RESET}")
            data_dict['classification'] = np.zeros(len(las_data.points), dtype=np.uint8)
            dtype_list.append(('classification', np.uint8))
            fields_to_save.append('classification')
            
        # 5. ğŸ”¥ ç”Ÿæˆ is_ground å­—æ®µ ğŸ”¥
        # åŸºäº classification ç”Ÿæˆï¼Œé¿å… Dataset é‡å¤è®¡ç®—
        if self.ground_class is not None:
            # ç¡®ä¿ä½¿ç”¨åˆšæ‰ï¼ˆå¯èƒ½è¡¥å…¨çš„ï¼‰classification æ•°æ®
            cls_data = data_dict['classification']
            is_ground = (cls_data == self.ground_class).astype(np.uint8)
            
            data_dict['is_ground'] = is_ground
            dtype_list.append(('is_ground', np.uint8))
            fields_to_save.append('is_ground')

        # 6. åˆ›å»ºç»“æ„åŒ–æ•°ç»„å¹¶ä¿å­˜
        struct_arr = np.zeros(len(las_data.points), dtype=dtype_list)
        for field in fields_to_save:
            struct_arr[field] = data_dict[field]
            
        struct_arr.tofile(bin_path)
        
        t1 = time.time()
        bin_size = bin_path.stat().st_size
        print(f"  {Colors.DIM}â”‚{Colors.RESET}  ğŸ“ BIN: {Colors.GREEN}{format_time(t1-t0)}{Colors.RESET} â†’ {Colors.CYAN}{format_size(bin_size)}{Colors.RESET}")

        # --- B. ç”Ÿæˆ PKL (é€»è¾‘ç´¢å¼•å…ƒæ•°æ® & å…³é”®å¤´æ–‡ä»¶ä¿¡æ¯) ---
        t0 = time.time()
        
        # é¢„å–åæ ‡ä»¥åŠ é€Ÿå¾ªç¯ (å¼•ç”¨ä¸Šé¢å·²ç»è½¬å¥½çš„ float64 æ•°ç»„)
        lx, ly, lz = data_dict['X'], data_dict['Y'], data_dict['Z']
        l_class = data_dict.get('classification', None)
        
        # ğŸš€ å¹¶è¡Œå¤„ç†æ‰€æœ‰ segments çš„ä½“ç´ åŒ–
        segments_info = self._process_segments_parallel(
            segments, lx, ly, lz, l_class, bin_path, pkl_path
        )
            
        # 8. ğŸ”¥ æ”¶é›†å®Œæ•´çš„ LAS Header å’Œ VLRs ä¿¡æ¯ ğŸ”¥
        # æ”¶é›†å®Œæ•´çš„LASå¤´æ–‡ä»¶ä¿¡æ¯
        header_info = {
            'version': f"{las_data.header.version.major}.{las_data.header.version.minor}",
            'point_format': las_data.header.point_format.id,
            'point_count': las_data.header.point_count,
            'x_scale': las_data.header.x_scale,
            'y_scale': las_data.header.y_scale,
            'z_scale': las_data.header.z_scale,
            'x_offset': las_data.header.x_offset,
            'y_offset': las_data.header.y_offset,
            'z_offset': las_data.header.z_offset,
            'x_min': las_data.header.x_min,
            'x_max': las_data.header.x_max,
            'y_min': las_data.header.y_min,
            'y_max': las_data.header.y_max,
            'z_min': las_data.header.z_min,
            'z_max': las_data.header.z_max,
        }
        
        # ä¿å­˜å…¶ä»–å¤´æ–‡ä»¶å±æ€§
        if hasattr(las_data.header, 'system_identifier'):
            header_info['system_identifier'] = las_data.header.system_identifier
        if hasattr(las_data.header, 'generating_software'):
            header_info['generating_software'] = las_data.header.generating_software
        if hasattr(las_data.header, 'creation_date'):
            header_info['creation_date'] = str(las_data.header.creation_date)
        if hasattr(las_data.header, 'global_encoding'):
            try:
                # global_encodingå¯èƒ½æ˜¯å¯¹è±¡ï¼Œéœ€è¦è½¬æ¢
                ge = las_data.header.global_encoding
                if hasattr(ge, 'value'):
                    header_info['global_encoding'] = int(ge.value)
                else:
                    header_info['global_encoding'] = int(ge)
            except:
                pass
        
        # ä¿å­˜åæ ‡ç³»ä¿¡æ¯ï¼ˆVLRs - Variable Length Recordsï¼‰
        vlrs_info = []
        if hasattr(las_data.header, 'vlrs'):
            for vlr in las_data.header.vlrs:
                vlr_dict = {
                    'user_id': vlr.user_id,
                    'record_id': vlr.record_id,
                    'description': vlr.description,
                }
                # ä¿å­˜VLRæ•°æ®ï¼ˆäºŒè¿›åˆ¶ï¼‰
                if hasattr(vlr, 'record_data'):
                    vlr_dict['record_data'] = bytes(vlr.record_data)
                vlrs_info.append(vlr_dict)
        header_info['vlrs'] = vlrs_info
        
        # ä¿å­˜CRSä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(las_data, 'crs'):
            try:
                header_info['crs'] = str(las_data.crs)
            except:
                header_info['crs'] = None
        
        # 9. ç»Ÿè®¡å…¨å±€ç±»åˆ«åˆ†å¸ƒ (Global Label Counts)
        global_label_counts = {}
        if 'classification' in data_dict:
            unique_labels, u_counts = np.unique(data_dict['classification'], return_counts=True)
            global_label_counts = {int(k): int(v) for k, v in zip(unique_labels, u_counts)}

        # ä¿å­˜ PKL
        metadata = {
            'las_file': las_file.name,
            'num_points': len(las_data.points),
            'num_segments': len(segments_info),
            'fields': fields_to_save,
            'dtype': dtype_list,
            'window_size': self.window_size,
            'overlap': self.overlap,
            'min_points': self.min_points,
            'max_points': self.max_points,
            'segments': segments_info,
            'grid_size': self.grid_size, # è®°å½•ç”Ÿæˆç´¢å¼•æ—¶çš„ grid size
            'header_info': header_info,  # ğŸ”¥ è¡¥å›å¤´æ–‡ä»¶ä¿¡æ¯
            'label_counts': global_label_counts # ğŸ”¥ è¡¥å›å…¨å±€ç±»åˆ«ç»Ÿè®¡
        }
        
        with open(pkl_path, 'wb') as f:
            pickle.dump(metadata, f)
        
        t1 = time.time()
        pkl_size = pkl_path.stat().st_size
        print(f"  {Colors.DIM}â”‚{Colors.RESET}  ğŸ“¦ PKL: {Colors.GREEN}{format_time(t1-t0)}{Colors.RESET} â†’ {Colors.CYAN}{format_size(pkl_size)}{Colors.RESET} ({len(segments_info)} å—)")
        print(f"  {Colors.DIM}â”‚{Colors.RESET}  ğŸ“‹ å­—æ®µ: {Colors.CYAN}{', '.join(fields_to_save)}{Colors.RESET}")

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    processor = LASProcessorLogicalIndex(
        input_path=r"E:\data\DALES\dales_las\train",
        output_dir=r"E:\data\DALES\dales_las\bin\train_logical",
        window_size=(50.0, 50.0),
        overlap=False, 
        grid_size=None,     # ç»Ÿä¸€ Grid Size
        min_points=5000,
        max_points=None,
        ground_class=None
    )
    processor.process_all_files()