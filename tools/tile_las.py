import os

# é™åˆ¶çº¿ç¨‹æ•°ï¼Œé¿å…å°ä»»åŠ¡çš„çº¿ç¨‹è°ƒåº¦å¼€é”€ï¼ˆç»´æŒä¹‹å‰çš„ä¼˜åŒ–ï¼‰
os.environ['NUMBA_NUM_THREADS'] = '8'
os.environ['MKL_NUM_THREADS'] = '8'
os.environ['OMP_NUM_THREADS'] = '8'

import numpy as np
import laspy
import pickle
from pathlib import Path
from typing import Union, List, Tuple, Optional, Dict, Any
from sklearn.neighbors import KDTree
from tqdm import tqdm
from collections import defaultdict
from numba import jit, prange


@jit(nopython=True, parallel=True)
def ravel_hash_vec_numba(arr, arr_min, arr_max):
    """
    Ravel hash function accelerated with numba.
    
    Args:
        arr: Input coordinates array (N, 3)
        arr_min: Minimum coordinates for each dimension
        arr_max: Maximum coordinates for each dimension
        
    Returns:
        Hash keys for each point
    """
    n = arr.shape[0]
    d = arr.shape[1]
    keys = np.zeros(n, dtype=np.uint64)
    
    # Normalize coordinates
    arr_normalized = np.empty_like(arr, dtype=np.uint64)
    for i in prange(n):
        for j in range(d):
            arr_normalized[i, j] = np.uint64(arr[i, j] - arr_min[j])
    
    # Calculate max + 1 for each dimension
    arr_max_plus_one = np.empty(d, dtype=np.uint64)
    for j in range(d):
        arr_max_plus_one[j] = np.uint64(arr_max[j] - arr_min[j] + 1)
    
    # Fortran style indexing
    for i in prange(n):
        key = np.uint64(0)
        for j in range(d - 1):
            key += arr_normalized[i, j]
            key *= arr_max_plus_one[j + 1]
        key += arr_normalized[i, d - 1]
        keys[i] = key
    
    return keys


@jit(nopython=True, parallel=True)
def compute_grid_coord_numba(coord, grid_size):
    """
    Compute grid coordinates accelerated with numba.
    
    Args:
        coord: Point coordinates (N, 3)
        grid_size: Grid size for sampling
        
    Returns:
        grid_coord: Grid coordinates (N, 3)
        scaled_coord: Scaled coordinates (N, 3)
    """
    n = coord.shape[0]
    scaled_coord = coord / grid_size
    grid_coord = np.floor(scaled_coord).astype(np.int64)
    return grid_coord, scaled_coord


@jit(nopython=True)
def shuffle_within_voxels_numba(idx_sort, cumsum_counts, count):
    """
    Shuffle points within each voxel using numba.
    
    Args:
        idx_sort: Sorted indices
        cumsum_counts: Cumulative sum of voxel counts
        count: Number of points in each voxel
        
    Returns:
        Shuffled idx_sort array
    """
    idx_sort_shuffled = idx_sort.copy()
    
    for i in range(len(count)):
        start_idx = cumsum_counts[i]
        end_idx = cumsum_counts[i + 1]
        
        # Fisher-Yates shuffle algorithm
        for j in range(end_idx - start_idx - 1, 0, -1):
            k = np.random.randint(0, j + 1)
            # Swap
            temp = idx_sort_shuffled[start_idx + j]
            idx_sort_shuffled[start_idx + j] = idx_sort_shuffled[start_idx + k]
            idx_sort_shuffled[start_idx + k] = temp
    
    return idx_sort_shuffled


@jit(nopython=True)
def sample_voxels_numba(idx_sort, cumsum_counts, count, num_loops, max_loops, points_per_loop):
    """
    Sample points from voxels using numba acceleration.
    
    Args:
        idx_sort: Sorted (and possibly shuffled) indices
        cumsum_counts: Cumulative sum of voxel counts
        count: Number of points in each voxel
        num_loops: Number of sampling loops
        max_loops: Maximum loops threshold
        points_per_loop: Points to sample per loop in extreme cases
        
    Returns:
        List of sampled index arrays
    """
    num_voxels = len(count)
    
    # Pre-allocate result arrays
    result_list = []
    
    for loop_idx in range(num_loops):
        # Estimate size for this loop
        estimated_size = 0
        for voxel_idx in range(num_voxels):
            voxel_count = count[voxel_idx]
            if voxel_count <= max_loops:
                estimated_size += 1
            else:
                sample_start = loop_idx * points_per_loop
                sample_end = min(sample_start + points_per_loop, voxel_count)
                if sample_start < voxel_count:
                    estimated_size += (sample_end - sample_start)
        
        # Allocate array for this loop
        idx_part = np.empty(estimated_size, dtype=np.int64)
        current_pos = 0
        
        for voxel_idx in range(num_voxels):
            voxel_count = count[voxel_idx]
            start_idx = cumsum_counts[voxel_idx]
            
            if voxel_count <= max_loops:
                # Normal case: sample one point
                local_idx = loop_idx % voxel_count
                idx_part[current_pos] = idx_sort[start_idx + local_idx]
                current_pos += 1
            else:
                # Extreme case: sample multiple points
                sample_start = loop_idx * points_per_loop
                sample_end = min(sample_start + points_per_loop, voxel_count)
                
                if sample_start < voxel_count:
                    for local_idx in range(sample_start, sample_end):
                        idx_part[current_pos] = idx_sort[start_idx + local_idx]
                        current_pos += 1
        
        # Only keep the filled portion
        if current_pos > 0:
            result_list.append(idx_part[:current_pos])
    
    return result_list


class GridSampler:
    """
    Grid sampling for point clouds using ravel hash with numba acceleration.
    Only returns point indices in test mode.
    """
    
    def __init__(self, grid_size=0.05, max_loops=30, shuffle_points=True):
        """
        Initialize grid sampler.
        
        Args:
            grid_size: Size of the grid cell for sampling
            max_loops: Maximum number of sampling iterations (to avoid extreme cases)
            shuffle_points: Whether to shuffle points within each voxel for randomness
        """
        self.grid_size = grid_size
        self.max_loops = max_loops
        self.shuffle_points = shuffle_points
    
    def sample(self, points: np.ndarray) -> List[np.ndarray]:
        """
        Perform grid sampling on point cloud (test mode).
        Returns list of index arrays for each sampling iteration.
        
        Args:
            points: Point cloud array (N, 3) containing xyz coordinates
            
        Returns:
            List of index arrays, each corresponding to one sampling iteration
        """
        # 1. Compute grid coordinates using numba
        grid_coord, scaled_coord = compute_grid_coord_numba(
            points.astype(np.float64), 
            np.float64(self.grid_size)
        )
        
        # 2. Normalize grid coordinates
        min_coord = grid_coord.min(0)
        grid_coord = grid_coord - min_coord
        
        # 3. Compute hash using numba
        arr_min = np.zeros(3, dtype=np.int64)
        arr_max = grid_coord.max(0)
        key = ravel_hash_vec_numba(grid_coord, arr_min, arr_max)
        
        # 4. Sort by hash key
        idx_sort = np.argsort(key, kind='mergesort')
        key_sort = key[idx_sort]
        
        # 5. Get unique keys and counts
        _, inverse, count = np.unique(key_sort, return_inverse=True, return_counts=True)
        
        # 6. Prepare cumsum for voxel boundaries
        cumsum_counts = np.cumsum(np.insert(count, 0, 0))
        
        # 7. Shuffle points within each voxel for randomness (using numba)
        if self.shuffle_points:
            idx_sort = shuffle_within_voxels_numba(idx_sort, cumsum_counts, count)
        
        # 8. Test mode with max_loops control
        max_count = count.max()
        
        # è®¡ç®—å®é™…çš„å¾ªç¯æ¬¡æ•°å’Œæ¯æ¬¡é‡‡æ ·æ•°
        if max_count <= self.max_loops:
            # æ­£å¸¸æƒ…å†µï¼šæ¯æ¬¡é‡‡1ä¸ªç‚¹
            num_loops = max_count
            points_per_loop = 1
        else:
            # æç«¯æƒ…å†µï¼šé™åˆ¶å¾ªç¯æ¬¡æ•°ï¼Œæ¯æ¬¡é‡‡å¤šä¸ªç‚¹
            num_loops = self.max_loops
            points_per_loop = int(np.ceil(max_count / self.max_loops))
        
        # 9. Sample using numba-accelerated function
        data_part_list = sample_voxels_numba(
            idx_sort, cumsum_counts, count, 
            num_loops, self.max_loops, points_per_loop
        )
        
        return data_part_list


class LASProcessorToBinWithGridSample:
    def __init__(self,
                 input_path: Union[str, Path],
                 output_dir: Union[str, Path] = None,
                 window_size: Tuple[float, float] = (50.0, 50.0),
                 min_points: Optional[int] = 1000,
                 max_points: Optional[int] = 5000,
                 overlap: bool = False,
                 grid_size: Optional[float] = None,
                 max_loops: int = 30,
                 shuffle_points: bool = True,
                 ground_class: Optional[int] = 2):
        """
        Initialize LAS point cloud processor with grid sampling.
        
        Args:
            input_path: Path to LAS file or directory containing LAS files
            output_dir: Directory to save processed files (default: same as input)
            window_size: (x_size, y_size) for rectangular windows (in units of the LAS file)
            min_points: Minimum points threshold for a valid segment (None to skip)
            max_points: Maximum points threshold before further segmentation (None to skip)
            overlap: Whether to use overlap mode (offset grid by half window size)
            grid_size: Grid size for grid sampling (None to skip grid sampling)
            max_loops: Maximum number of sampling iterations for grid sampling
            shuffle_points: Whether to shuffle points within each voxel for randomness
            ground_class: Classification value for ground points (default: 2, None to skip is_ground generation)
        """
        self.input_path = Path(input_path)
        self.output_dir = Path(output_dir) if output_dir else self.input_path.parent
        self.window_size = window_size
        self.min_points = min_points
        self.max_points = max_points
        self.overlap = overlap
        self.grid_size = grid_size
        self.max_loops = max_loops
        self.shuffle_points = shuffle_points
        self.ground_class = ground_class
        
        # Initialize grid sampler if grid_size is specified
        self.grid_sampler = GridSampler(grid_size, max_loops, shuffle_points) if grid_size is not None else None
        
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True)
            
        self.las_files = self._find_las_files()
    
    def _find_las_files(self) -> List[Path]:
        """Find all LAS files in the input path."""
        if self.input_path.is_file() and self.input_path.suffix.lower() in ['.las', '.laz']:
            return [self.input_path]
        elif self.input_path.is_dir():
            return list(self.input_path.glob('*.las')) + list(self.input_path.glob('*.laz'))
        else:
            raise ValueError(f"Input path {self.input_path} is not a valid LAS file or directory")
    
    def process_all_files(self, n_workers: int = None):
        """
        Process all discovered LAS files.
        å¹¶è¡Œå¤„ç†åœ¨å•ä¸ªLASæ–‡ä»¶å†…éƒ¨è¿›è¡Œï¼Œè€Œä¸æ˜¯è·¨æ–‡ä»¶å¹¶è¡Œã€‚
        
        Args:
            n_workers: Number of parallel workers for segment processing (None = auto)
        """
        import time
        import multiprocessing
        
        if n_workers is None:
            n_workers = max(1, multiprocessing.cpu_count() - 1)
        
        start_time = time.time()
        
        print("="*70)
        print(f"Starting LAS to BIN/PKL conversion with Grid Sampling")
        print("="*70)
        print(f"Total files: {len(self.las_files)}")
        print(f"Window size: {self.window_size}")
        print(f"Min points: {self.min_points}")
        print(f"Max points: {self.max_points}")
        if self.grid_sampler:
            print(f"Grid sampling: âœ… Enabled")
            print(f"  - Grid size: {self.grid_size}")
            print(f"  - Max loops: {self.max_loops}")
            print(f"  - Shuffle points: {'âœ… Yes' if self.shuffle_points else 'âŒ No'}")
        else:
            print(f"Grid sampling: âŒ Disabled")
        print(f"Overlap mode: {'âœ… Enabled' if self.overlap else 'âŒ Disabled'}")
        if self.ground_class is not None:
            print(f"Ground classification: {self.ground_class} â†’ is_ground field")
        else:
            print(f"Ground classification: âŒ Disabled")
        print(f"Parallel workers: {n_workers} (per file)")
        print("-"*70)
        
        # é¡ºåºå¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼Œä½†æ–‡ä»¶å†…éƒ¨å¹¶è¡Œå¤„ç†segments
        for las_file in tqdm(self.las_files, desc="Processing files", unit="file",
                            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'):
            try:
                self.process_file(las_file, n_workers=n_workers)
            except Exception as e:
                print(f"\n[ERROR] {las_file.name}: {e}")
                import traceback
                traceback.print_exc()
        
        elapsed_time = time.time() - start_time
        print("\n" + "="*70)
        print(f"Conversion completed successfully!")
        print(f"Total time: {elapsed_time:.2f}s ({elapsed_time/60:.2f}min)")
        print(f"Average: {elapsed_time/len(self.las_files):.2f}s per file")
        print("="*70)
    
    def process_file(self, las_file: Union[str, Path], n_workers: int = 4):
        """
        Process a single LAS file and save to bin+pkl format.
        
        Args:
            las_file: Path to LAS file
            n_workers: Number of parallel workers for segment processing
        """
        import time
        las_file = Path(las_file)
        
        file_start = time.time()
        print(f"\n{'='*70}")
        print(f"ğŸ“„ Processing: {las_file.name}")
        print(f"{'='*70}")
        
        # 1. è¯»å–LASæ–‡ä»¶
        t0 = time.time()
        with laspy.open(las_file) as fh:
            las_data = fh.read()
        t1 = time.time()
        print(f"  âœ“ è¯»å–LASæ–‡ä»¶: {t1-t0:.2f}s ({len(las_data.points):,} ç‚¹)")
        
        # 2. å‡†å¤‡ç‚¹äº‘æ•°æ®
        t0 = time.time()
        point_data = np.vstack((
            las_data.x, 
            las_data.y, 
            las_data.z
        )).transpose()
        t1 = time.time()
        print(f"  âœ“ å‡†å¤‡ç‚¹äº‘æ•°æ®: {t1-t0:.2f}s")
        
        # 3. åˆ†å‰²å¤„ç†ï¼ˆè¿™é‡Œä¼šæœ‰è¯¦ç»†å­é˜¶æ®µè¾“å‡ºï¼‰
        t0 = time.time()
        segments = self.segment_point_cloud(point_data, n_workers=n_workers)
        t1 = time.time()
        print(f"  âœ“ æ€»åˆ†å‰²æ—¶é—´: {t1-t0:.2f}s â†’ {len(segments)} segments")
        
        # 4. ä¿å­˜æ–‡ä»¶
        t0 = time.time()
        self.save_segments_as_bin_pkl(las_file, las_data, segments)
        t1 = time.time()
        print(f"  âœ“ ä¿å­˜æ–‡ä»¶: {t1-t0:.2f}s")
        
        file_total = time.time() - file_start
        print(f"  ğŸ¯ æ€»è®¡: {file_total:.2f}s")
        print(f"{'='*70}")
    
    def segment_point_cloud(self, points: np.ndarray, n_workers: int = 4) -> List[np.ndarray]:
        """
        Segment point cloud into tiles based on window size.
        
        Args:
            points: Point cloud array (N, 3)
            n_workers: Number of parallel workers
            
        Returns:
            List of segment indices
        """
        import time
        
        print(f"  ğŸ“¦ å¼€å§‹åˆ†å‰² ({n_workers} workers)...")
        
        if not self.overlap:
            # æ­£å¸¸æ¨¡å¼ï¼šå•æ¬¡ç½‘æ ¼åˆ†å‰²
            t0 = time.time()
            segments = self._grid_segmentation(points, offset_x=0, offset_y=0, n_workers=n_workers)
            t1 = time.time()
            print(f"     å•æ¬¡ç½‘æ ¼åˆ†å‰²: {t1-t0:.2f}s")
            return segments
        else:
            # Overlapæ¨¡å¼ï¼šä¸¤æ¬¡ç½‘æ ¼åˆ†å‰²ï¼ˆåç§»åŠä¸ªçª—å£ï¼‰
            x_size, y_size = self.window_size
            
            # ç¬¬ä¸€æ¬¡åˆ†å‰²ï¼šæ­£å¸¸ç½‘æ ¼
            t0 = time.time()
            segments1 = self._grid_segmentation(points, offset_x=0, offset_y=0, n_workers=n_workers)
            t1 = time.time()
            print(f"     ç¬¬1æ¬¡ç½‘æ ¼åˆ†å‰²: {t1-t0:.2f}s â†’ {len(segments1)} segments")
            
            # ç¬¬äºŒæ¬¡åˆ†å‰²ï¼šåç§»åŠä¸ªçª—å£
            t0 = time.time()
            segments2 = self._grid_segmentation(points, offset_x=x_size/2, offset_y=y_size/2, n_workers=n_workers)
            t1 = time.time()
            print(f"     ç¬¬2æ¬¡ç½‘æ ¼åˆ†å‰²: {t1-t0:.2f}s â†’ {len(segments2)} segments")
            
            # åˆå¹¶ä¸¤æ¬¡åˆ†å‰²ç»“æœ
            all_segments = segments1 + segments2
            
            return all_segments
    
    def _grid_segmentation(self, points: np.ndarray, offset_x: float = 0, offset_y: float = 0, n_workers: int = 4) -> List[np.ndarray]:
        """
        Perform grid-based segmentation with optional offset.
        
        Args:
            points: Point cloud array (N, 3)
            offset_x: X offset for grid origin
            offset_y: Y offset for grid origin
            n_workers: Number of parallel workers
            
        Returns:
            List of segment indices
        """
        import time
        
       # 1. çª—å£åˆ†ç»„ (ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨ argsort ä»£æ›¿ where å¾ªç¯)
        t0 = time.time()
        x_size, y_size = self.window_size
        
        # è®¡ç®—åŸç‚¹
        min_x, min_y = np.min(points[:, 0]), np.min(points[:, 1])
        origin_x = min_x - offset_x
        origin_y = min_y - offset_y
        
        # è®¡ç®—çª—å£ç´¢å¼•
        # ä¼˜åŒ–ï¼šç›´æ¥è®¡ç®— long å‹ç´¢å¼•ï¼Œä¸è®¡ç®— num_windowsï¼Œé¿å…æº¢å‡ºé£é™©
        x_bins = ((points[:, 0] - origin_x) / x_size).astype(np.int64)
        y_bins = ((points[:, 1] - origin_y) / y_size).astype(np.int64)
        
        # ä½¿ç”¨ cantor pairing æˆ–ç±»ä¼¼çš„ hash æ–¹å¼ç»„åˆäºŒç»´ç´¢å¼•ï¼Œæˆ–è€…ç®€å•çš„å­—ç¬¦ä¸²ç»„åˆï¼ˆæ…¢ï¼‰
        # è¿™é‡Œä¸ºäº†é€Ÿåº¦ï¼Œå‡è®¾ y_bins èŒƒå›´ä¸ä¼šå¤ªå¤§ï¼Œä½¿ç”¨å¤§æ•°ä¹˜æ³•ç»„åˆ
        # å‡è®¾ y æ–¹å‘ä¸ä¼šè¶…è¿‡ 1,000,000 ä¸ª grid
        y_multiplier = 1000000
        window_ids = x_bins * y_multiplier + y_bins
        
        # ğŸš€ æ ¸å¿ƒä¼˜åŒ–ï¼šä½¿ç”¨ argsort ä¸€æ¬¡æ€§åˆ†ç»„ï¼Œé¿å… N æ¬¡å…¨é‡æ‰«æ
        sort_idx = np.argsort(window_ids)
        sorted_window_ids = window_ids[sort_idx]
        
        # æ‰¾åˆ°åˆ‡åˆ†ç‚¹
        unique_ids, split_indices = np.unique(sorted_window_ids, return_index=True)
        # split_indices[0] æ˜¯ 0ï¼Œæˆ‘ä»¬éœ€è¦çš„åˆ‡åˆ†ç‚¹æ˜¯ split_indices[1:]
        # np.split ä¼šè¿”å›åˆ—è¡¨
        segments = np.split(sort_idx, split_indices[1:])
        
        # è¿‡æ»¤ç©º segment (np.unique ä¿è¯äº† unique_ids å¯¹åº”å­˜åœ¨çš„ segmentsï¼Œé€šå¸¸ä¸éœ€è¦è¿‡æ»¤ï¼Œä½† split ä¼šäº§ç”Ÿç¬¬ä¸€ä¸ªç©ºå¦‚æœç´¢å¼•0æœ‰å€¼)
        # np.unique return_index è¿”å›çš„æ˜¯æ¯ä¸ªå”¯ä¸€å€¼ç¬¬ä¸€æ¬¡å‡ºç°çš„ç´¢å¼•
        # å®é™… segments åº”è¯¥æ˜¯ [split_indices[i]:split_indices[i+1]]
        
        t1 = time.time()
        print(f"       - çª—å£åˆ†ç»„: {t1-t0:.3f}s â†’ {len(segments)} çª—å£")
        
        # 2. Miné˜ˆå€¼å¤„ç†ï¼ˆä¼˜å…ˆå¤„ç†ï¼Œåˆå¹¶è¾¹ç•Œä¸Šç‚¹å°‘çš„æ— æ•ˆçª—å£ï¼‰
        if self.min_points is not None:
            t0 = time.time()
            before_count = len(segments)
            segments = self.apply_min_threshold(points, segments, min_threshold=self.min_points)
            t1 = time.time()
            print(f"       - Miné˜ˆå€¼å¤„ç†: {t1-t0:.3f}s ({before_count} â†’ {len(segments)} segments)")
        
        # 3. Grid Samplingå¤„ç†ï¼ˆåœ¨Minå’ŒMaxä¹‹é—´ï¼‰
        if self.grid_sampler is not None:
            t0 = time.time()
            before_count = len(segments)
            total_points_before = sum(len(seg) for seg in segments)
            segments = self.apply_grid_sampling(points, segments)
            total_points_after = sum(len(seg) for seg in segments)
            t1 = time.time()
            print(f"       - Gridé‡‡æ ·å¤„ç†: {t1-t0:.3f}s ({before_count} â†’ {len(segments)} segments, "
                  f"{total_points_before:,} â†’ {total_points_after:,} points)")
        
        # 4. Maxé˜ˆå€¼å¤„ç†ï¼ˆæœ€åå¤„ç†ï¼‰
        if self.max_points is not None:
            t0 = time.time()
            before_count = len(segments)
            segments = self.apply_max_threshold(points, segments, n_workers=n_workers)
            t1 = time.time()
            print(f"       - Maxé˜ˆå€¼å¤„ç†: {t1-t0:.3f}s ({before_count} â†’ {len(segments)} segments)")
            
        return segments
    
    def apply_grid_sampling(self, points: np.ndarray, segments: List[np.ndarray]) -> List[np.ndarray]:
        """
        Apply grid sampling to each segment.
        
        Args:
            points: Point cloud array (N, 3)
            segments: List of segment indices
            
        Returns:
            List of sampled segment indices (expanded due to multiple sampling iterations)
        """
        sampled_segments = []
        
        for segment in segments:
            segment_points = points[segment]
            
            # Perform grid sampling (returns list of index arrays)
            sampled_indices_list = self.grid_sampler.sample(segment_points)
            
            # Convert local indices to global indices and add to result
            for local_indices in sampled_indices_list:
                global_indices = segment[local_indices]
                sampled_segments.append(global_indices)
        
        return sampled_segments
    
    def apply_max_threshold(self, points: np.ndarray, segments: List[np.ndarray], n_workers: int = 4) -> List[np.ndarray]:
        """
        Apply max_points threshold to segments, subdividing large segments.
        
        Args:
            points: Point cloud array
            segments: List of segment indices
            n_workers: Number of parallel workers
            
        Returns:
            List of processed segment indices
        """
        large_segment_indices = [i for i, segment in enumerate(segments) if len(segment) > self.max_points]
        
        if not large_segment_indices:
            return segments
        
        result_segments = [segment for i, segment in enumerate(segments) if i not in large_segment_indices]
        large_segments = [segments[i] for i in large_segment_indices]
        
        def process_segment(segment):
            if len(segment) <= self.max_points:
                return [segment]
            
            segment_points = points[segment]
            ranges = np.ptp(segment_points[:, :2], axis=0)
            split_dim = np.argmax(ranges[:2])
            sorted_indices = np.argsort(segment_points[:, split_dim])
            
            mid = len(sorted_indices) // 2
            left_half = segment[sorted_indices[:mid]]
            right_half = segment[sorted_indices[mid:]]
            
            result = []
            result.extend(process_segment(left_half))
            result.extend(process_segment(right_half))
            return result
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # ä½¿ç”¨ä¼ å…¥çš„n_workerså‚æ•°
        max_workers = min(n_workers, len(large_segments)) if len(large_segments) > 0 else 1
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_segment, segment) for segment in large_segments]
            for future in as_completed(futures):
                result_segments.extend(future.result())
        
        return result_segments
    
    def apply_min_threshold(self, points: np.ndarray, segments: List[np.ndarray], 
                           min_threshold: Optional[int] = None) -> List[np.ndarray]:
        """
        Apply min_points threshold using KD-Tree.
        
        Args:
            points: Point cloud array
            segments: List of segment indices
            min_threshold: Minimum points threshold (if None, use self.min_points)
        
        Returns:
            List of processed segment indices
        """
        if len(segments) <= 1:
            return segments
        
        # ä½¿ç”¨ä¼ å…¥çš„min_thresholdï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨self.min_points
        effective_min = min_threshold if min_threshold is not None else self.min_points
        
        centroids = np.array([np.mean(points[segment][:, :2], axis=0) for segment in segments])
        small_segments = [i for i, segment in enumerate(segments) if len(segment) < effective_min]
        
        if not small_segments:
            return segments
        
        valid_indices = [i for i in range(len(segments)) if i not in small_segments]
        if not valid_indices:
            return segments
        
        valid_centroids = centroids[valid_indices]
        kdtree = KDTree(valid_centroids)
        
        small_segments.sort(key=lambda i: len(segments[i]))
        
        for small_idx in small_segments:
            if small_idx >= len(segments):
                continue
            
            _, nearest_idx = kdtree.query([centroids[small_idx]], k=1)
            nearest_idx = valid_indices[nearest_idx[0][0]]
            
            if nearest_idx != small_idx and nearest_idx < len(segments):
                segments[nearest_idx] = np.concatenate([segments[nearest_idx], segments[small_idx]])
                segments[small_idx] = np.array([], dtype=int)
        
        return [segment for segment in segments if len(segment) > 0]
    
    def save_segments_as_bin_pkl(self, las_file: Path, las_data: laspy.LasData, segments: List[np.ndarray]):
        """
        Save segmented point clouds to bin+pkl format.
        
        Args:
            las_file: Original LAS file path
            las_data: Original LAS data
            segments: List of index arrays for segments
        """
        import time
        
        base_name = las_file.stem
        
        # å‡†å¤‡ä¿å­˜æ‰€æœ‰ç‚¹äº‘æ•°æ®åˆ°ä¸€ä¸ªbinæ–‡ä»¶
        bin_path = self.output_dir / f"{base_name}.bin"
        pkl_path = self.output_dir / f"{base_name}.pkl"
        
        print(f"  ğŸ’¾ ä¿å­˜åˆ° bin+pkl...")
        
        # 1. æ”¶é›†å­—æ®µ
        t0 = time.time()
        
        # åªä¿å­˜çœŸæ­£æœ‰æ„ä¹‰æ•°æ®çš„å­—æ®µ
        # å¿…é¡»ä¿å­˜çš„æ ¸å¿ƒå­—æ®µ
        core_fields = ['X', 'Y', 'Z']
        
        # å¯é€‰ä½†å¸¸ç”¨çš„å­—æ®µï¼ˆéœ€è¦æ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼‰
        optional_fields = ['intensity', 'return_number', 'number_of_returns', 
                          'classification', 'scan_angle_rank', 'user_data', 
                          'point_source_id', 'gps_time', 
                          'red', 'green', 'blue', 'nir',
                          'edge_of_flight_line']
        
        # æ„å»ºå­—æ®µåˆ—è¡¨ï¼šåªä¿å­˜å®é™…å­˜åœ¨ä¸”æœ‰æ•°æ®çš„å­—æ®µ
        fields_to_save = []
        dtype_list = []
        data_dict = {}
        
        # ä¿å­˜æ ¸å¿ƒå­—æ®µï¼ˆå¿…é¡»æœ‰ï¼‰
        for field in core_fields:
            field_lower = field.lower()
            if hasattr(las_data, field_lower):
                data = getattr(las_data, field_lower)
                fields_to_save.append(field)
                data_dict[field] = data
                dtype_list.append((field, data.dtype))
        
        # ä¿å­˜å¯é€‰å­—æ®µï¼ˆåªæœ‰å­˜åœ¨æ—¶æ‰ä¿å­˜ï¼‰
        has_classification = False
        for field in optional_fields:
            field_lower = field.lower()
            if hasattr(las_data, field_lower):
                data = getattr(las_data, field_lower)
                fields_to_save.append(field)
                data_dict[field] = data
                dtype_list.append((field, data.dtype))
                if field_lower == 'classification':
                    has_classification = True
        
        # å¦‚æœæ²¡æœ‰classificationï¼Œæ·»åŠ é»˜è®¤å€¼0ï¼ˆè¿™æ˜¯å”¯ä¸€æ·»åŠ é»˜è®¤å€¼çš„å­—æ®µï¼‰
        if not has_classification:
            fields_to_save.append('classification')
            data_dict['classification'] = np.zeros(len(las_data.points), dtype=np.uint8)
            dtype_list.append(('classification', np.uint8))
        
        # ç”Ÿæˆ is_ground å­—æ®µï¼ˆåŸºäº classificationï¼‰
        if self.ground_class is not None and has_classification:
            is_ground = (las_data.classification == self.ground_class).astype(np.uint8)
            fields_to_save.append('is_ground')
            data_dict['is_ground'] = is_ground
            dtype_list.append(('is_ground', np.uint8))
        
        # åˆ›å»ºç»“æ„åŒ–æ•°ç»„
        structured_array = np.zeros(len(las_data.points), dtype=dtype_list)
        for field in fields_to_save:
            structured_array[field] = data_dict[field]
        
        t1 = time.time()
        print(f"     - æ”¶é›†å­—æ®µ: {t1-t0:.3f}s ({len(fields_to_save)} ä¸ªå­—æ®µï¼š {fields_to_save})")
        
        # 2. ä¿å­˜ä¸ºbinæ–‡ä»¶
        t0 = time.time()
        structured_array.tofile(bin_path)
        t1 = time.time()
        bin_size_mb = bin_path.stat().st_size / (1024**2)
        print(f"     - å†™å…¥bin: {t1-t0:.3f}s ({bin_size_mb:.1f} MB)")
        
        # å‡†å¤‡pklæ–‡ä»¶çš„å…ƒæ•°æ®
        metadata = {
            'las_file': las_file.name,
            'num_points': len(las_data.points),
            'num_segments': len(segments),
            'fields': fields_to_save,
            'dtype': dtype_list,
            'window_size': self.window_size,
            'min_points': self.min_points,
            'max_points': self.max_points,
            'overlap': self.overlap,
            'grid_size': self.grid_size,
            'max_loops': self.max_loops if self.grid_size else None,
            'shuffle_points': self.shuffle_points if self.grid_size else None,
        }
        
        # æ”¶é›†å®Œæ•´çš„LASå¤´æ–‡ä»¶ä¿¡æ¯
        header_info = {
            'version': f"{las_data.header.version.major}.{las_data.header.version.minor}",
            'point_format': las_data.header.point_format.id,
            'point_count': las_data.header.point_count,
            'x_scale': las_data.header.x_scale,
            'y_scale': las_data.header.y_scale,
            'z_scale': las_data.header.z_scale,
            'x_offset': las_data.header.x_offset,
            'y_offset': las_data.header.y_offset,
            'z_offset': las_data.header.z_offset,
            'x_min': las_data.header.x_min,
            'x_max': las_data.header.x_max,
            'y_min': las_data.header.y_min,
            'y_max': las_data.header.y_max,
            'z_min': las_data.header.z_min,
            'z_max': las_data.header.z_max,
        }
        
        # ä¿å­˜å…¶ä»–å¤´æ–‡ä»¶å±æ€§
        if hasattr(las_data.header, 'system_identifier'):
            header_info['system_identifier'] = las_data.header.system_identifier
        if hasattr(las_data.header, 'generating_software'):
            header_info['generating_software'] = las_data.header.generating_software
        if hasattr(las_data.header, 'creation_date'):
            header_info['creation_date'] = str(las_data.header.creation_date)
        if hasattr(las_data.header, 'global_encoding'):
            try:
                # global_encodingå¯èƒ½æ˜¯å¯¹è±¡ï¼Œéœ€è¦è½¬æ¢
                ge = las_data.header.global_encoding
                if hasattr(ge, 'value'):
                    header_info['global_encoding'] = int(ge.value)
                else:
                    header_info['global_encoding'] = int(ge)
            except:
                pass
        
        # ä¿å­˜åæ ‡ç³»ä¿¡æ¯ï¼ˆVLRs - Variable Length Recordsï¼‰
        vlrs_info = []
        if hasattr(las_data.header, 'vlrs'):
            for vlr in las_data.header.vlrs:
                vlr_dict = {
                    'user_id': vlr.user_id,
                    'record_id': vlr.record_id,
                    'description': vlr.description,
                }
                # ä¿å­˜VLRæ•°æ®ï¼ˆäºŒè¿›åˆ¶ï¼‰
                if hasattr(vlr, 'record_data'):
                    vlr_dict['record_data'] = bytes(vlr.record_data)
                vlrs_info.append(vlr_dict)
        header_info['vlrs'] = vlrs_info
        
        # ä¿å­˜CRSä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(las_data, 'crs'):
            try:
                header_info['crs'] = str(las_data.crs)
            except:
                header_info['crs'] = None
        
        metadata['header_info'] = header_info
        
        # ç»Ÿè®¡æ•´ä¸ªæ–‡ä»¶çš„ç±»åˆ«åˆ†å¸ƒ
        if has_classification:
            unique_labels, counts = np.unique(las_data.classification, return_counts=True)
            label_counts = {int(label): int(count) for label, count in zip(unique_labels, counts)}
        else:
            label_counts = {0: len(las_data.points)}
        metadata['label_counts'] = label_counts
        
        t1 = time.time()
        print(f"     - å‡†å¤‡metadata: {t1-t0:.3f}s")
        
        # 3. æ”¶é›†æ¯ä¸ªåˆ†å—çš„ä¿¡æ¯
        t0 = time.time()
        segments_info = []
        
        # ä¼˜åŒ–ï¼šé¢„å…ˆè·å– numpy æ•°ç»„ï¼Œé¿å…åœ¨å¾ªç¯ä¸­åå¤è®¿é—® las_data å±æ€§ï¼ˆå¯èƒ½è§¦å‘ getter å¼€é”€ï¼‰
        # æ³¨æ„ï¼šä½¿ç”¨ points æ•°ç»„ï¼ˆå¦‚æœä¹‹å‰å·²ç»æœ‰äº†ï¼‰æˆ–è€…ä» las_data æå–
        # è¿™é‡Œç›´æ¥ä½¿ç”¨ las_data çš„æ•°ç»„å¼•ç”¨
        lx, ly, lz = las_data.x, las_data.y, las_data.z
        
        for i, segment_indices in enumerate(segments):
            segment_info = {
                'segment_id': i,
                'indices': segment_indices,
                'num_points': len(segment_indices),
                'bin_file': base_name,
                'bin_path': str(bin_path),
                'pkl_path': str(pkl_path),
            }
            
            # ä¼˜åŒ–ï¼šæå–å½“å‰ segment çš„åæ ‡å­é›†ï¼Œåªåšä¸€æ¬¡åˆ‡ç‰‡
            seg_x = lx[segment_indices]
            seg_y = ly[segment_indices]
            seg_z = lz[segment_indices]
            
            # è®¡ç®—è¾¹ç•Œï¼ˆä½¿ç”¨å­é›†è®¡ç®—ï¼Œå¿«å¾—å¤šï¼‰
            segment_info['x_min'] = float(np.min(seg_x))
            segment_info['x_max'] = float(np.max(seg_x))
            segment_info['y_min'] = float(np.min(seg_y))
            segment_info['y_max'] = float(np.max(seg_y))
            segment_info['z_min'] = float(np.min(seg_z))
            segment_info['z_max'] = float(np.max(seg_z))
            
            segments_info.append(segment_info)
        
        metadata['segments'] = segments_info
        
        t1 = time.time()
        print(f"     - æ”¶é›†segmentsä¿¡æ¯: {t1-t0:.3f}s ({len(segments)} segments)")
        
        # 4. ä¿å­˜pklæ–‡ä»¶
        t0 = time.time()
        with open(pkl_path, 'wb') as f:
            pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)
        t1 = time.time()
        pkl_size_mb = pkl_path.stat().st_size / (1024**2)
        print(f"     - å†™å…¥pkl: {t1-t0:.3f}s ({pkl_size_mb:.1f} MB)")


def process_las_files_to_bin_with_gridsample(input_path, output_dir=None, window_size=(50.0, 50.0), 
                                              min_points=None, max_points=None,
                                              overlap=False, grid_size=None,
                                              max_loops=30, shuffle_points=True,
                                              ground_class=2, n_workers=None):
    """
    Process LAS files with grid sampling and save to bin+pkl format.
    å¹¶è¡Œå¤„ç†åœ¨å•ä¸ªLASæ–‡ä»¶å†…éƒ¨è¿›è¡Œï¼ˆå¤„ç†segmentsï¼‰ï¼Œè€Œä¸æ˜¯è·¨æ–‡ä»¶å¹¶è¡Œã€‚
    
    Args:
        input_path: Path to LAS file or directory containing LAS files
        output_dir: Directory to save processed files (default: same as input)
        window_size: (x_size, y_size) for rectangular windows
        min_points: Minimum points threshold for a valid segment
        max_points: Maximum points threshold before further segmentation
        overlap: Whether to use overlap mode (offset grid by half window size)
        grid_size: Grid size for grid sampling (None to skip grid sampling)
        max_loops: Maximum number of sampling iterations (to avoid extreme cases)
        shuffle_points: Whether to shuffle points within each voxel for randomness
        ground_class: Classification value for ground points (default: 2, None to skip is_ground generation)
        n_workers: Number of parallel workers for segment processing (None = auto, uses CPU count - 1)
    """
    processor = LASProcessorToBinWithGridSample(
        input_path=input_path,
        output_dir=output_dir,
        window_size=window_size,
        min_points=min_points,
        max_points=max_points,
        overlap=overlap,
        grid_size=grid_size,
        max_loops=max_loops,
        shuffle_points=shuffle_points,
        ground_class=ground_class
    )
    processor.process_all_files(n_workers=n_workers)


# æä¾›ä¸€ä¸ªè¾…åŠ©å‡½æ•°ç”¨äºåŠ è½½æ•°æ®
def load_segment_from_bin(bin_path: Union[str, Path], 
                          pkl_path: Union[str, Path], 
                          segment_id: int) -> Tuple[np.ndarray, Dict[str, Any]]:
    """
    ä½¿ç”¨np.memmapä»binæ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šåˆ†å—çš„æ•°æ®ã€‚
    
    Args:
        bin_path: binæ–‡ä»¶è·¯å¾„
        pkl_path: pklæ–‡ä»¶è·¯å¾„
        segment_id: è¦åŠ è½½çš„åˆ†å—ID
        
    Returns:
        (segment_data, segment_info): åˆ†å—çš„ç‚¹äº‘æ•°æ®å’Œå…ƒæ•°æ®
    """
    bin_path = Path(bin_path)
    pkl_path = Path(pkl_path)
    
    # åŠ è½½å…ƒæ•°æ®
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    
    # è·å–åˆ†å—ä¿¡æ¯
    segment_info = metadata['segments'][segment_id]
    indices = segment_info['indices']
    
    # ä½¿ç”¨memmapåŠ è½½æ•°æ®
    dtype = np.dtype(metadata['dtype'])
    mmap_data = np.memmap(bin_path, dtype=dtype, mode='r')
    
    # è¯»å–æŒ‡å®šåˆ†å—çš„æ•°æ®
    segment_data = mmap_data[indices]
    
    return segment_data, segment_info


def load_all_segments_info(pkl_path: Union[str, Path]) -> List[Dict[str, Any]]:
    """
    åŠ è½½æ‰€æœ‰åˆ†å—çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ˆä¸åŠ è½½å®é™…ç‚¹äº‘æ•°æ®ï¼‰ã€‚
    
    Args:
        pkl_path: pklæ–‡ä»¶è·¯å¾„
        
    Returns:
        æ‰€æœ‰åˆ†å—çš„å…ƒæ•°æ®åˆ—è¡¨
    """
    pkl_path = Path(pkl_path)
    
    with open(pkl_path, 'rb') as f:
        metadata = pickle.load(f)
    
    return metadata['segments']


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¤„ç†LASæ–‡ä»¶ï¼ˆå¸¦Grid Samplingï¼‰
    input_path = r"E:\data\DALES\dales_las\test"
    output_dir = r"E:\data\DALES\dales_las\bin\test"
    window_size = (50.0, 50.0)
    min_points = 4096 * 5
    max_points = 4096 * 16 * 4
    overlap = False
    grid_size = None  # ğŸ”¥ è®¾ç½®grid sizeå¯ç”¨grid sampling
    max_loops = 10  # ğŸ”¥ grid sizeå¼€å¯æ—¶çš„æœ€å¤§é‡‡æ ·å¾ªç¯æ¬¡æ•°ï¼ˆé¿å…æç«¯æƒ…å†µï¼‰
    shuffle_points = True  # ğŸ”¥ æ‰“ä¹±ä½“ç´ å†…ç‚¹é¡ºåºï¼ˆæé«˜éšæœºæ€§ï¼‰
    max_workers = 8  # è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
    ground_class = None  # ğŸ”¥ åœ°é¢ç‚¹çš„classificationå€¼ï¼ˆNoneåˆ™ä¸ç”Ÿæˆis_groundå­—æ®µï¼‰
    
    # å¤„ç†æ–‡ä»¶ï¼ˆå¹¶è¡Œå¤„ç†åœ¨å•ä¸ªLASæ–‡ä»¶å†…éƒ¨è¿›è¡Œï¼‰
    process_las_files_to_bin_with_gridsample(
        input_path=input_path,
        output_dir=output_dir,
        window_size=window_size,
        min_points=min_points,
        max_points=max_points,
        overlap=overlap,
        grid_size=grid_size,  # ğŸ”¥ è®¾ç½®grid_sizeå¯ç”¨grid samplingï¼ˆNoneåˆ™è·³è¿‡ï¼‰
        max_loops=max_loops,  # ğŸ”¥ æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼ˆå½“ä½“ç´ å†…ç‚¹>max_loopsæ—¶ï¼Œæ¯æ¬¡é‡‡æ ·å¤šä¸ªç‚¹ï¼‰
        shuffle_points=shuffle_points,  # ğŸ”¥ æ˜¯å¦æ‰“ä¹±ä½“ç´ å†…ç‚¹é¡ºåº
        ground_class=ground_class,  # ğŸ”¥ åœ°é¢ç‚¹classificationå€¼ï¼ˆ2æ˜¯LASæ ‡å‡†ï¼ŒNoneåˆ™ä¸ç”Ÿæˆis_groundï¼‰
        n_workers=max_workers  # ğŸ”¥ å¹¶è¡Œworkeræ•°ï¼ˆNone=è‡ªåŠ¨ï¼Œæ¯ä¸ªæ–‡ä»¶å†…éƒ¨å¹¶è¡Œå¤„ç†segmentsï¼‰
    )
    
    # ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½æ•°æ®
    # print("\n" + "="*50)
    # print("ç¤ºä¾‹ï¼šå¦‚ä½•åŠ è½½åˆ†å—æ•°æ®")
    # print("="*50)
    
    # bin_file = Path(output_dir) / "5080_54400.bin"
    # pkl_file = Path(output_dir) / "5080_54400.pkl"
    
    # if bin_file.exists() and pkl_file.exists():
    #     # åŠ è½½æ‰€æœ‰åˆ†å—ä¿¡æ¯
    #     all_segments = load_all_segments_info(pkl_file)
    #     print(f"\næ€»å…±æœ‰ {len(all_segments)} ä¸ªåˆ†å—")
        
    #     # åŠ è½½ç¬¬ä¸€ä¸ªåˆ†å—çš„æ•°æ®
    #     if len(all_segments) > 0:
    #         segment_data, segment_info = load_segment_from_bin(bin_file, pkl_file, 0)
    #         print(f"\nç¬¬ä¸€ä¸ªåˆ†å—ä¿¡æ¯:")
    #         print(f"  - ç‚¹æ•°: {segment_info['num_points']}")
    #         print(f"  - ç±»åˆ«: {segment_info['unique_labels']}")
    #         print(f"  - ç±»åˆ«åˆ†å¸ƒ: {segment_info['label_counts']}")
    #         print(f"\nç‚¹äº‘æ•°æ®shape: {segment_data.shape}")
    #         print(f"å¯ç”¨å­—æ®µ: {segment_data.dtype.names}")
    #         print(f"\nå‰5ä¸ªç‚¹çš„xyzåæ ‡:")
    #         # å­—æ®µåæ˜¯å¤§å†™çš„ X, Y, Z
    #         for i in range(min(5, len(segment_data))):
    #             print(f"  Point {i}: X={segment_data['X'][i]:.2f}, Y={segment_data['Y'][i]:.2f}, Z={segment_data['Z'][i]:.2f}, class={segment_data['classification'][i]}")
