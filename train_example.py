"""
ç®€å•çš„è®­ç»ƒè„šæœ¬ç¤ºä¾‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python train_example.py

æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•æ‰‹åŠ¨ç¼–å†™è®­ç»ƒè„šæœ¬ã€‚
      ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ LightningCLI + é…ç½®æ–‡ä»¶çš„æ–¹å¼ã€‚
"""

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor

# å¯¼å…¥ PointSuite ç»„ä»¶
from pointsuite.data import BinPklDataModule
from pointsuite.data.transforms import (
    CenterShift, AutoNormalizeHNorm, RandomRotate, RandomScale,
    Collect, ToTensor
)
from pointsuite.tasks import SemanticSegmentationTask
from pointsuite.models.backbones import PointTransformerV2m5
from pointsuite.models.heads import SegmentationHead


def create_datamodule():
    """åˆ›å»ºæ•°æ®æ¨¡å—"""
    
    # å®šä¹‰è®­ç»ƒ transforms
    train_transforms = [
        CenterShift(apply_z=True),
        RandomRotate(angle=[-180, 180], axis='z', p=0.5),
        RandomScale(scale=[0.9, 1.1], p=0.5),
        AutoNormalizeHNorm(clip_range=None),
        Collect(
            keys=['coord', 'class'],
            offset_key={'offset': 'coord'},
            feat_keys={'feat': ['coord', 'h_norm', 'intensity', 'echo']}
        ),
        ToTensor()
    ]
    
    # å®šä¹‰éªŒè¯/æµ‹è¯• transforms
    val_transforms = [
        CenterShift(apply_z=True),
        AutoNormalizeHNorm(clip_range=None),
        Collect(
            keys=['coord', 'class'],
            offset_key={'offset': 'coord'},
            feat_keys={'feat': ['coord', 'h_norm', 'intensity', 'echo']}
        ),
        ToTensor()
    ]
    
    # åˆ›å»º DataModule
    datamodule = BinPklDataModule(
        # æ•°æ®è·¯å¾„ - ä¿®æ”¹ä¸ºä½ çš„æ•°æ®è·¯å¾„
        train_data='data/train',
        val_data='data/val',
        test_data='data/test',
        
        # DataLoader å‚æ•°
        batch_size=8,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True,
        
        # Dataset å‚æ•°
        assets=['coord', 'intensity', 'echo', 'h_norm', 'classification'],
        ignore_label=-1,
        
        # Loop å‚æ•°
        train_loop=1,
        val_loop=1,
        test_loop=1,
        
        # åŠ¨æ€ Batch
        use_dynamic_batch=True,
        max_points=500000,
        use_dynamic_batch_inference=True,
        max_points_inference=800000,
        
        # Transforms
        train_transforms=train_transforms,
        val_transforms=val_transforms,
        test_transforms=val_transforms,
    )
    
    return datamodule


def create_model(num_classes=8, in_channels=6, class_mapping=None):
    """åˆ›å»ºæ¨¡å‹"""
    
    # åˆ›å»º Backbone
    backbone = PointTransformerV2m5(
        in_channels=in_channels,  # coord(3) + h_norm(1) + intensity(1) + echo(1) = 6
        num_classes=num_classes,
        patch_embed_depth=1,
        patch_embed_channels=48,
        patch_embed_groups=6,
        patch_embed_neighbours=16,
        enc_depths=[2, 2, 6, 2],
        enc_channels=[48, 96, 192, 384],
        enc_num_head=[3, 6, 12, 24],
        enc_patch_size=[128, 128, 128, 128],
        dec_depths=[1, 1, 1, 1],
        dec_channels=[48, 96, 192, 384],
        dec_num_head=[3, 6, 12, 24],
        dec_patch_size=[128, 128, 128, 128],
    )
    
    # åˆ›å»º Head
    head = SegmentationHead(
        in_channels=48,  # backbone è¾“å‡ºé€šé“
        num_classes=num_classes,
        hidden_channels=64
    )
    
    # åˆ›å»º Task
    model = SemanticSegmentationTask(
        backbone=backbone,
        head=head,
        learning_rate=0.001,
        
        # ğŸ”¥ é‡è¦ï¼šä¼ å…¥ class_mappingï¼Œå°†è¢«ä¿å­˜åˆ° checkpoint
        class_mapping=class_mapping,
        
        # æŸå¤±å‡½æ•°é…ç½®
        loss_configs=[
            {
                'type': 'pointsuite.models.losses.CrossEntropyLoss',
                'weight': 1.0,
                'init_args': {'ignore_index': -1}
            }
        ],
        
        # æŒ‡æ ‡é…ç½®
        metric_configs=[
            {
                'type': 'pointsuite.utils.metrics.OverallAccuracy',
                'name': 'OA',
                'init_args': {'ignore_index': -1}
            },
            {
                'type': 'pointsuite.utils.metrics.MeanIoU',
                'name': 'mIoU',
                'init_args': {'num_classes': num_classes, 'ignore_index': -1}
            }
        ]
    )
    
    return model


def main():
    """ä¸»å‡½æ•°"""
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(42)
    
    print("="*60)
    print("PointSuite è®­ç»ƒç¤ºä¾‹")
    print("="*60)
    
    # ğŸ”¥ å®šä¹‰ç±»åˆ«æ˜ å°„ï¼ˆå¦‚æœéœ€è¦ï¼‰
    # å¦‚æœä½ çš„ç±»åˆ«æ ‡ç­¾ä¸è¿ç»­ï¼Œéœ€è¦å®šä¹‰æ˜ å°„
    class_mapping = None  # é»˜è®¤ä¸ä½¿ç”¨æ˜ å°„
    # class_mapping = {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}  # ç¤ºä¾‹ï¼š5ä¸ªç±»åˆ«
    
    # 1. åˆ›å»º DataModule
    print("\n[1/4] åˆ›å»º DataModule...")
    datamodule = create_datamodule()
    # å¦‚æœä½¿ç”¨ class_mappingï¼Œéœ€è¦ä¼ å…¥ DataModule
    if class_mapping is not None:
        datamodule.class_mapping = class_mapping
    datamodule.print_info()  # æ‰“å°æ•°æ®ä¿¡æ¯
    
    # 2. åˆ›å»º Model
    print("\n[2/4] åˆ›å»º Model...")
    num_classes = 8  # å¦‚æœä½¿ç”¨ class_mappingï¼Œåº”è¯¥æ˜¯æ˜ å°„åçš„ç±»åˆ«æ•°
    model = create_model(
        num_classes=num_classes,
        in_channels=6,
        class_mapping=class_mapping  # ğŸ”¥ ä¼ å…¥ class_mappingï¼Œä¿å­˜åˆ° checkpoint
    )
    print(f"âœ“ Model created: {model.__class__.__name__}")
    print(f"  - Backbone: {model.backbone.__class__.__name__}")
    print(f"  - Head: {model.head.__class__.__name__}")
    print(f"  - Learning rate: {model.learning_rate}")
    if class_mapping is not None:
        print(f"  - Class mapping: {class_mapping}")
        print(f"  - å°†è¢«ä¿å­˜åˆ° checkpointï¼Œé¢„æµ‹æ—¶è‡ªåŠ¨åŠ è½½")
    
    # 3. åˆ›å»º Trainer
    print("\n[3/4] åˆ›å»º Trainer...")
    trainer = pl.Trainer(
        max_epochs=100,
        accelerator='gpu',
        devices=1,
        precision='16-mixed',
        
        # æ¢¯åº¦ç›¸å…³
        gradient_clip_val=1.0,
        accumulate_grad_batches=1,
        
        # éªŒè¯ç›¸å…³
        val_check_interval=1.0,
        check_val_every_n_epoch=1,
        
        # æ—¥å¿—ç›¸å…³
        log_every_n_steps=50,
        
        # å›è°ƒ
        callbacks=[
            ModelCheckpoint(
                dirpath='checkpoints/',
                filename='epoch={epoch}-val_loss={val/total_loss:.4f}',
                monitor='val/total_loss',
                mode='min',
                save_top_k=3,
                save_last=True,
                auto_insert_metric_name=False
            ),
            EarlyStopping(
                monitor='val/total_loss',
                patience=20,
                mode='min',
                verbose=True
            ),
            LearningRateMonitor(logging_interval='step')
        ],
        
        # æ—¥å¿—å™¨
        logger=True,  # ä½¿ç”¨é»˜è®¤ TensorBoard logger
    )
    print(f"âœ“ Trainer created")
    print(f"  - Max epochs: {trainer.max_epochs}")
    print(f"  - Devices: {trainer.num_devices}")
    print(f"  - Precision: {trainer.precision}")
    
    # 4. è®­ç»ƒ
    print("\n[4/4] å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    
    try:
        trainer.fit(model, datamodule)
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        
        # 5. æµ‹è¯•
        print("\n[5/5] å¼€å§‹æµ‹è¯•...")
        trainer.test(model, datamodule, ckpt_path='best')
        print("\nâœ… æµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
