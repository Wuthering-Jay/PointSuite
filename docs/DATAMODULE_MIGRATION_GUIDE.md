# DataModule é‡æ„è¿ç§»æŒ‡å—

## ğŸ¯ é‡æ„ç›®æ ‡

è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
1. âœ… **æ–‡ä»¶è·¯å¾„æ›´çµæ´»**ï¼šæ”¯æŒè·¨ç›®å½•æ–‡ä»¶é…ç½®
2. âœ… **ç‹¬ç«‹çš„é¢„æµ‹æ•°æ®é›†**ï¼štest å’Œ predict å¯ä»¥ä¸åŒ
3. âœ… **WeightedRandomSampler ç‹¬ç«‹æ§åˆ¶**ï¼šä¸ä¾èµ– DynamicBatchSampler
4. âœ… **loop ä¸ weights åŒ¹é…**ï¼šæ˜ç¡®è¦æ±‚ weights é•¿åº¦è€ƒè™‘ loop
5. âœ… **weights ä¸ä¿å­˜è¶…å‚æ•°**ï¼šé¿å…è¶…å‚æ•°è†¨èƒ€

## ğŸ“‹ API å˜åŒ–

### æ—§ APIï¼ˆå·²åºŸå¼ƒï¼‰

```python
BinPklDataModule(
    data_root='data/dales',           # å•ä¸€æ ¹ç›®å½•
    train_files=['train.pkl'],        # æ–‡ä»¶ååˆ—è¡¨
    val_files=['val.pkl'],
    test_files=['test.pkl'],
    # ... å…¶ä»–å‚æ•°
)
```

**é™åˆ¶**ï¼š
- æ‰€æœ‰æ–‡ä»¶å¿…é¡»åœ¨åŒä¸€ä¸ª `data_root` ä¸‹
- test å’Œ predict å…±äº«ç›¸åŒæ•°æ®
- WeightedRandomSampler åªèƒ½ä¸ DynamicBatchSampler ä¸€èµ·ä½¿ç”¨

### æ–° APIï¼ˆæ¨èï¼‰

```python
BinPklDataModule(
    train_data='data/train',          # å®Œæ•´è·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶åˆ—è¡¨ï¼‰
    val_data='data/val',
    test_data='data/test',
    predict_data='data/predict',      # ç‹¬ç«‹çš„é¢„æµ‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
    use_weighted_sampler=True,        # ç‹¬ç«‹æ§åˆ¶
    train_sampler_weights=weights,    # é•¿åº¦ = len(dataset) * loop
    # ... å…¶ä»–å‚æ•°
)
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ–‡ä»¶å¯ä»¥åœ¨ä»»æ„ç›®å½•
- âœ… é¢„æµ‹æ•°æ®é›†ç‹¬ç«‹é…ç½®
- âœ… WeightedRandomSampler ç‹¬ç«‹å¯ç”¨
- âœ… æ˜ç¡® weights ä¸ loop çš„å…³ç³»

## ğŸ”„ è¿ç§»æ­¥éª¤

### æƒ…å†µ 1: æ‰€æœ‰æ–‡ä»¶åœ¨åŒä¸€ç›®å½•

**æ—§ä»£ç **ï¼š
```python
datamodule = BinPklDataModule(
    data_root='data/dales',
    train_files=['train.pkl'],
    val_files=['val.pkl'],
    test_files=['test.pkl'],
    batch_size=8,
)
```

**æ–°ä»£ç **ï¼š
```python
datamodule = BinPklDataModule(
    train_data='data/dales/train.pkl',     # æˆ– 'data/dales' è‡ªåŠ¨å‘ç°
    val_data='data/dales/val.pkl',
    test_data='data/dales/test.pkl',
    batch_size=8,
)
```

### æƒ…å†µ 2: æ–‡ä»¶è·¨ç›®å½•

**æ—§ä»£ç **ï¼ˆä¸æ”¯æŒï¼‰ï¼š
```python
# æ— æ³•å®ç°è·¨ç›®å½•
```

**æ–°ä»£ç **ï¼š
```python
datamodule = BinPklDataModule(
    train_data=[
        'data/scene1/train.pkl',
        'data/scene2/train.pkl',
        'other_data/extra_train.pkl',
    ],
    val_data='data/scene1/val.pkl',
    test_data='data/test_set',  # æ•´ä¸ªç›®å½•
    batch_size=8,
)
```

### æƒ…å†µ 3: ä½¿ç”¨ DynamicBatchSampler + WeightedRandomSampler

**æ—§ä»£ç **ï¼š
```python
# å‡è®¾ original_weights é•¿åº¦ = åŸå§‹æ ·æœ¬æ•°ï¼ˆä¸è€ƒè™‘ loopï¼‰
datamodule = BinPklDataModule(
    data_root='data/dales',
    use_dynamic_batch=True,
    max_points=500000,
    train_sampler_weights=original_weights,  # âŒ é”™è¯¯ï¼šæœªè€ƒè™‘ loop
    loop=2,  # æ•°æ®é›†å¾ªç¯ 2 æ¬¡
)
```

**æ–°ä»£ç **ï¼š
```python
# æ­£ç¡®å¤„ç† loop
original_weights = [...]  # é•¿åº¦ = åŸå§‹æ ·æœ¬æ•°
loop = 2

# weights å¿…é¡»é‡å¤ loop æ¬¡
train_weights = original_weights * loop  # é•¿åº¦ = åŸå§‹æ ·æœ¬æ•° * loop

datamodule = BinPklDataModule(
    train_data='data/dales',
    use_dynamic_batch=True,
    max_points=500000,
    use_weighted_sampler=True,           # âœ… æ˜¾å¼å¯ç”¨
    train_sampler_weights=train_weights, # âœ… æ­£ç¡®é•¿åº¦
    loop=2,
)
```

### æƒ…å†µ 4: ä¸ä½¿ç”¨ DynamicBatchSamplerï¼Œä½†éœ€è¦ WeightedRandomSampler

**æ—§ä»£ç **ï¼ˆä¸æ”¯æŒï¼‰ï¼š
```python
# WeightedRandomSampler åªèƒ½ä¸ DynamicBatchSampler ä¸€èµ·ä½¿ç”¨
```

**æ–°ä»£ç **ï¼š
```python
# ç°åœ¨å¯ä»¥ç‹¬ç«‹ä½¿ç”¨
datamodule = BinPklDataModule(
    train_data='data/dales',
    batch_size=8,                        # å›ºå®š batch size
    use_dynamic_batch=False,             # ä¸ä½¿ç”¨åŠ¨æ€æ‰¹æ¬¡
    use_weighted_sampler=True,           # âœ… ç‹¬ç«‹å¯ç”¨åŠ æƒé‡‡æ ·
    train_sampler_weights=weights,       # è€ƒè™‘ loop
    loop=1,
)
```

### æƒ…å†µ 5: ç‹¬ç«‹çš„é¢„æµ‹æ•°æ®é›†

**æ—§ä»£ç **ï¼š
```python
# test å’Œ predict å…±äº«ç›¸åŒæ•°æ®
datamodule = BinPklDataModule(
    data_root='data',
    test_files=['test.pkl'],
)
# predict ä¼šä½¿ç”¨ test æ•°æ®
```

**æ–°ä»£ç **ï¼š
```python
datamodule = BinPklDataModule(
    train_data='data/train',
    val_data='data/val',
    test_data='data/test_labeled',      # æœ‰æ ‡ç­¾çš„æµ‹è¯•é›†
    predict_data='data/new_scenes',     # âœ… æ— æ ‡ç­¾çš„æ–°åœºæ™¯
)
```

## ğŸ“– å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: DALES æ•°æ®é›†ï¼ˆåŸºæœ¬é…ç½®ï¼‰

```python
from pointsuite.data.datamodule_bin import BinPklDataModule
from pointsuite.data import transforms as T

# å®šä¹‰ transforms
train_transforms = [
    T.RandomRotate(angle=[-180, 180], axis='z'),
    T.RandomScale([0.95, 1.05]),
    T.RandomFlip(p=0.5),
    T.AutoNormalizeIntensity(),
    T.Collect(keys=['coord', 'intensity', 'class'], feat_keys=['intensity']),
]

val_transforms = [
    T.AutoNormalizeIntensity(),
    T.Collect(keys=['coord', 'intensity', 'class'], feat_keys=['intensity']),
]

# åˆ›å»º DataModule
datamodule = BinPklDataModule(
    train_data='data/dales/train',
    val_data='data/dales/val',
    test_data='data/dales/test',
    batch_size=8,
    num_workers=4,
    assets=['coord', 'intensity', 'class'],
    train_transforms=train_transforms,
    val_transforms=val_transforms,
    test_transforms=val_transforms,
    class_mapping={0: 0, 1: 1, 2: 2, 6: 3, 8: 4},  # 5 ç±»
    ignore_label=-1,
)

# ä½¿ç”¨
datamodule.setup()
datamodule.print_info()
```

### ç¤ºä¾‹ 2: ä½¿ç”¨ DynamicBatchSamplerï¼ˆå†…å­˜æ§åˆ¶ï¼‰

```python
datamodule = BinPklDataModule(
    train_data=[
        'data/scene1/train.pkl',
        'data/scene2/train.pkl',
    ],
    val_data='data/scene1/val.pkl',
    test_data='data/test',
    use_dynamic_batch=True,       # å¯ç”¨åŠ¨æ€æ‰¹æ¬¡
    max_points=500000,            # æ¯æ‰¹æ¬¡æœ€å¤š 50 ä¸‡ç‚¹
    num_workers=8,
    assets=['coord', 'intensity', 'h_norm', 'class'],
    train_transforms=train_transforms,
    val_transforms=val_transforms,
    class_mapping={0: 0, 1: 1, 2: 2, 6: 3, 8: 4},
)
```

### ç¤ºä¾‹ 3: åŠ æƒé‡‡æ ·å¤„ç†ç±»åˆ«ä¸å¹³è¡¡

```python
import numpy as np

# å‡è®¾ä½ æœ‰ç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†
# åŸå§‹æ ·æœ¬æ•° = 1000ï¼Œä½¿ç”¨ loop=2
num_samples = 1000
loop = 2

# è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆåŸºäºç±»åˆ«é¢‘ç‡çš„å€’æ•°ï¼‰
# è¿™é‡Œå‡è®¾ä½ å·²ç»ä» dataset ä¸­è·å–äº†æ¯ä¸ªæ ·æœ¬çš„ä¸»è¦ç±»åˆ«
sample_labels = [...]  # é•¿åº¦ = 1000

# è®¡ç®—ç±»åˆ«æƒé‡
from collections import Counter
label_counts = Counter(sample_labels)
class_weights = {label: 1.0 / count for label, count in label_counts.items()}

# ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
original_weights = [class_weights[label] for label in sample_labels]

# âš ï¸ å…³é”®ï¼šè€ƒè™‘ loopï¼Œé‡å¤æƒé‡
train_weights = original_weights * loop  # é•¿åº¦ = 2000

datamodule = BinPklDataModule(
    train_data='data/train',
    val_data='data/val',
    test_data='data/test',
    use_dynamic_batch=True,
    max_points=500000,
    use_weighted_sampler=True,        # å¯ç”¨åŠ æƒé‡‡æ ·
    train_sampler_weights=train_weights,  # é•¿åº¦å¿…é¡» = 2000
    loop=loop,                        # loop = 2
    num_workers=8,
    # ... å…¶ä»–å‚æ•°
)
```

### ç¤ºä¾‹ 4: ç‹¬ç«‹é¢„æµ‹æ•°æ®é›†

```python
datamodule = BinPklDataModule(
    train_data='data/dales/train',
    val_data='data/dales/val',
    test_data='data/dales/test',           # æœ‰æ ‡ç­¾çš„æµ‹è¯•é›†
    predict_data='data/new_scenes',        # æ— æ ‡ç­¾çš„æ–°åœºæ™¯
    predict_transforms=[
        T.AutoNormalizeIntensity(),
        T.Collect(keys=['coord', 'intensity'], feat_keys=['intensity']),
    ],
    # ... å…¶ä»–å‚æ•°
)

# æµ‹è¯•é˜¶æ®µï¼šä½¿ç”¨æœ‰æ ‡ç­¾çš„ test æ•°æ®
trainer.test(model, datamodule)

# é¢„æµ‹é˜¶æ®µï¼šä½¿ç”¨æ— æ ‡ç­¾çš„ predict æ•°æ®
trainer.predict(model, datamodule)
```

## âš ï¸ å¸¸è§é”™è¯¯

### é”™è¯¯ 1: weights é•¿åº¦ä¸åŒ¹é…

```python
# âŒ é”™è¯¯
original_weights = [1.0] * 1000  # é•¿åº¦ = 1000
datamodule = BinPklDataModule(
    train_data='data/train',
    train_sampler_weights=original_weights,  # âŒ é•¿åº¦ä¸å¯¹
    loop=2,  # dataset é•¿åº¦ä¼šå˜æˆ 2000
)
```

**é”™è¯¯ä¿¡æ¯**ï¼š
```
ValueError: train_sampler_weights é•¿åº¦ (1000) ä¸ dataset é•¿åº¦ (2000) ä¸åŒ¹é…ã€‚
æç¤ºï¼šå¦‚æœä½¿ç”¨ loop > 1ï¼Œweights éœ€è¦é‡å¤ loop æ¬¡ã€‚
ä¾‹å¦‚ï¼šweights = original_weights * loop
```

**ä¿®å¤**ï¼š
```python
# âœ… æ­£ç¡®
original_weights = [1.0] * 1000
train_weights = original_weights * 2  # é•¿åº¦ = 2000
datamodule = BinPklDataModule(
    train_data='data/train',
    train_sampler_weights=train_weights,  # âœ… é•¿åº¦æ­£ç¡®
    loop=2,
)
```

### é”™è¯¯ 2: å¿˜è®°å¯ç”¨ use_weighted_sampler

```python
# âŒ ä¸ä¼šç”Ÿæ•ˆ
datamodule = BinPklDataModule(
    train_data='data/train',
    train_sampler_weights=[1.0] * 2000,  # æä¾›äº† weights
    # use_weighted_sampler=True,  # âŒ å¿˜è®°å¯ç”¨
)
# weights ä¼šè¢«å¿½ç•¥ï¼
```

**ä¿®å¤**ï¼š
```python
# âœ… æ­£ç¡®
datamodule = BinPklDataModule(
    train_data='data/train',
    use_weighted_sampler=True,           # âœ… æ˜¾å¼å¯ç”¨
    train_sampler_weights=[1.0] * 2000,
)
```

### é”™è¯¯ 3: ä½¿ç”¨æ—§çš„ data_root API

```python
# âŒ æ—§ APIï¼ˆå·²åºŸå¼ƒï¼‰
datamodule = BinPklDataModule(
    data_root='data/dales',  # âŒ å‚æ•°ä¸å­˜åœ¨
    train_files=['train.pkl'],
)
```

**ä¿®å¤**ï¼š
```python
# âœ… æ–° API
datamodule = BinPklDataModule(
    train_data='data/dales/train.pkl',  # æˆ– 'data/dales' è‡ªåŠ¨å‘ç°
    val_data='data/dales/val.pkl',
    test_data='data/dales/test.pkl',
)
```

## ğŸ”§ å®ç”¨å·¥å…·å‡½æ•°

### è®¡ç®—ç±»åˆ«æƒé‡

```python
import numpy as np
from collections import Counter

def compute_sample_weights(dataset, loop=1):
    """
    ä¸ºæ•°æ®é›†è®¡ç®—æ ·æœ¬æƒé‡ä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    
    Args:
        dataset: BinPklDataset å®ä¾‹
        loop: æ•°æ®é›†å¾ªç¯æ¬¡æ•°
        
    Returns:
        weights: æ ·æœ¬æƒé‡åˆ—è¡¨ï¼Œé•¿åº¦ = len(dataset) * loop
    """
    # ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„ä¸»è¦ç±»åˆ«
    sample_labels = []
    for i in range(len(dataset.data_list)):
        data = dataset[i]
        labels = data['class'].numpy()
        # ä½¿ç”¨æœ€é¢‘ç¹çš„ç±»åˆ«ä½œä¸ºæ ·æœ¬æ ‡ç­¾
        most_common = Counter(labels).most_common(1)[0][0]
        sample_labels.append(most_common)
    
    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆé¢‘ç‡çš„å€’æ•°ï¼‰
    label_counts = Counter(sample_labels)
    total = len(sample_labels)
    class_weights = {label: total / count for label, count in label_counts.items()}
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
    original_weights = [class_weights[label] for label in sample_labels]
    
    # è€ƒè™‘ loop
    final_weights = original_weights * loop
    
    print(f"ç±»åˆ«æƒé‡: {class_weights}")
    print(f"åŸå§‹æ ·æœ¬æ•°: {len(original_weights)}")
    print(f"æœ€ç»ˆæƒé‡æ•°é‡: {len(final_weights)} (loop={loop})")
    
    return final_weights

# ä½¿ç”¨ç¤ºä¾‹
from pointsuite.data.datasets.dataset_bin import BinPklDataset

dataset = BinPklDataset(
    data_root='data/train',
    split='train',
    loop=2,
)

weights = compute_sample_weights(dataset, loop=2)

datamodule = BinPklDataModule(
    train_data='data/train',
    val_data='data/val',
    use_weighted_sampler=True,
    train_sampler_weights=weights,
    loop=2,
)
```

## ğŸ“Š å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | æ—§ API | æ–° API |
|------|--------|--------|
| **æ–‡ä»¶è·¯å¾„é…ç½®** | `data_root + files` | ç›´æ¥å®Œæ•´è·¯å¾„æˆ–åˆ—è¡¨ |
| **è·¨ç›®å½•æ–‡ä»¶** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ |
| **ç‹¬ç«‹é¢„æµ‹æ•°æ®** | âŒ ä¸ test å…±äº« | âœ… ç‹¬ç«‹ `predict_data` |
| **WeightedRandomSampler** | ä»…ä¸ DynamicBatch | âœ… ç‹¬ç«‹æ§åˆ¶ |
| **loop ä¸ weights** | âš ï¸ å®¹æ˜“å‡ºé”™ | âœ… æ˜ç¡®è¦æ±‚åŒ¹é… |
| **weights ä¿å­˜** | âœ… ä¿å­˜åˆ°è¶…å‚æ•° | âœ… ä¸ä¿å­˜ï¼ˆé¿å…è†¨èƒ€ï¼‰|
| **çµæ´»æ€§** | â­â­ | â­â­â­â­â­ |

## ğŸ¯ æœ€ä½³å®è·µ

1. **æ€»æ˜¯ä½¿ç”¨å®Œæ•´è·¯å¾„**ï¼šé¿å…ä¾èµ–å·¥ä½œç›®å½•
2. **æ˜ç¡® loop ä¸ weights**ï¼šç¡®ä¿é•¿åº¦åŒ¹é…
3. **ç‹¬ç«‹æ§åˆ¶é‡‡æ ·ç­–ç•¥**ï¼šæ ¹æ®éœ€è¦å¯ç”¨ `use_weighted_sampler`
4. **é¢„æµ‹æ•°æ®ç‹¬ç«‹é…ç½®**ï¼šå¦‚æœä¸æµ‹è¯•ä¸åŒï¼Œä½¿ç”¨ `predict_data`
5. **åˆ©ç”¨ç±»å‹æç¤º**ï¼šIDE ä¼šæä¾›æ›´å¥½çš„è‡ªåŠ¨è¡¥å…¨

éœ€è¦å¸®åŠ©è¿ç§»ç°æœ‰ä»£ç å—ï¼Ÿ
