# Class Mapping ä¸ Ignore Label æœºåˆ¶

## ğŸ“‹ è®¾è®¡ç›®æ ‡

å°†ä¸åœ¨ `class_mapping` ä¸­çš„ç±»åˆ«è®¾ç½®ä¸º `ignore_label`ï¼Œä½¿è¿™äº›ç‚¹ï¼š
1. âœ… **å‚ä¸å‰å‘ä¼ æ’­**ï¼šä¿æŒç‚¹äº‘çš„ç©ºé—´è¿ç»­æ€§ï¼Œä¸äº§ç”Ÿç©ºæ´
2. âŒ **ä¸å‚ä¸æŸå¤±è®¡ç®—**ï¼šé€šè¿‡ `ignore_index` æœºåˆ¶è¢«æ’é™¤
3. âŒ **ä¸å‚ä¸ç²¾åº¦è¯„ä¼°**ï¼šMetrics è‡ªåŠ¨è¿‡æ»¤è¿™äº›ç‚¹

## ğŸ”§ å®ç°æ–¹å¼

### 1. Dataset å±‚é¢ï¼ˆå·²ä¿®æ”¹ï¼‰

**æ–‡ä»¶**: `pointsuite/data/datasets/dataset_bin.py`

```python
# åŸæ¥çš„å®ç°ï¼ˆé”™è¯¯ï¼‰
if self.class_mapping is not None:
    mapped_classification = classification.copy()  # ä¿æŒåŸå€¼
    for original_label, new_label in self.class_mapping.items():
        mask = (classification == original_label)
        mapped_classification[mask] = new_label

# æ–°çš„å®ç°ï¼ˆæ­£ç¡®ï¼‰âœ…
if self.class_mapping is not None:
    # åˆå§‹åŒ–ä¸º ignore_labelï¼ˆé»˜è®¤ -1ï¼‰
    mapped_classification = np.full_like(classification, self.ignore_label, dtype=np.int64)
    
    # åªæ˜ å°„ class_mapping ä¸­å®šä¹‰çš„ç±»åˆ«
    for original_label, new_label in self.class_mapping.items():
        mask = (classification == original_label)
        mapped_classification[mask] = new_label
    
    data['class'] = mapped_classification
```

**å…³é”®å˜åŒ–**:
- åŸæ¥ï¼šä¸åœ¨ mapping ä¸­çš„ç±»åˆ«**ä¿æŒåŸå€¼**
- ç°åœ¨ï¼šä¸åœ¨ mapping ä¸­çš„ç±»åˆ«**è®¾ä¸º ignore_label**

### 2. Loss å±‚é¢ï¼ˆå·²æ”¯æŒï¼‰

**æ–‡ä»¶**: `pointsuite/models/losses/*.py`

æ‰€æœ‰æŸå¤±å‡½æ•°å·²ç»æ­£ç¡®æ”¯æŒ `ignore_index` å‚æ•°ï¼š

#### CrossEntropyLoss
```python
self.ce_loss = nn.CrossEntropyLoss(
    weight=weight,
    ignore_index=ignore_index,  # é»˜è®¤ -1
    reduction=reduction,
    label_smoothing=label_smoothing
)
```

#### FocalLoss
```python
# è¿‡æ»¤ ignore_index
if self.ignore_index >= 0:
    valid_mask = target != self.ignore_index
    logits = logits[valid_mask]
    target = target[valid_mask]
```

#### LovaszLoss, LACLoss ç­‰
éƒ½æœ‰ç±»ä¼¼çš„ `ignore_index` å¤„ç†æœºåˆ¶ã€‚

### 3. Metrics å±‚é¢ï¼ˆå·²æ”¯æŒï¼‰

**æ–‡ä»¶**: `pointsuite/utils/metrics.py`

æ‰€æœ‰æŒ‡æ ‡éƒ½æ­£ç¡®è¿‡æ»¤ `ignore_index`ï¼š

```python
# OverallAccuracy, MeanIoU, Precision, Recall ç­‰
def update(self, preds: torch.Tensor, target: torch.Tensor):
    # è½¬æ¢ preds ä¸º labels
    pred_labels = _convert_preds_to_labels(preds)
    
    # è¿‡æ»¤ ignore_index âœ…
    if self.ignore_index >= 0:
        valid_mask = target != self.ignore_index
        pred_labels = pred_labels[valid_mask]
        target = target[valid_mask]
    
    # åç»­è®¡ç®—...
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### åœºæ™¯ 1: DALES æ•°æ®é›†ï¼ˆ9 ä¸ªç±»åˆ«ï¼‰

å‡è®¾åŸå§‹æ•°æ®æœ‰ç±»åˆ« `[0, 1, 2, 3, 4, 5, 6, 7, 8]`ï¼Œä½†ä½ åªæƒ³è®­ç»ƒå…¶ä¸­ 5 ä¸ªï¼š

```python
# configs/experiments/dales_5class.py
class_mapping = {
    0: 0,  # Ground
    1: 1,  # Vegetation
    2: 2,  # Building
    6: 3,  # Car
    8: 4,  # Pole
    # æ³¨æ„ï¼šç±»åˆ« 3, 4, 5, 7 æ²¡æœ‰åœ¨ mapping ä¸­
}

# datamodule
datamodule = BinPklDataModule(
    data_root="data/dales",
    class_mapping=class_mapping,
    ignore_label=-1,  # é»˜è®¤å€¼
)

# task
task = SemanticSegmentationTask(
    model_cfg=...,
    loss_cfg={
        'type': 'CrossEntropyLoss',
        'ignore_index': -1,  # ä¸ datamodule ä¸€è‡´
    },
    num_classes=5,  # æ˜ å°„åçš„ç±»åˆ«æ•°
)
```

**ç»“æœ**:
- åŸå§‹ç±»åˆ« `[0, 1, 2, 6, 8]` â†’ æ˜ å°„ä¸º `[0, 1, 2, 3, 4]`
- åŸå§‹ç±»åˆ« `[3, 4, 5, 7]` â†’ æ˜ å°„ä¸º `-1` (ignore_label)
- è¿™äº› `-1` æ ‡ç­¾çš„ç‚¹ï¼š
  - âœ… å‚ä¸å‰å‘ä¼ æ’­ï¼ˆä¿æŒç‚¹äº‘å®Œæ•´æ€§ï¼‰
  - âŒ ä¸è®¡ç®—æŸå¤±ï¼ˆ`ignore_index=-1`ï¼‰
  - âŒ ä¸ç»Ÿè®¡ç²¾åº¦ï¼ˆMetrics è‡ªåŠ¨è¿‡æ»¤ï¼‰

### åœºæ™¯ 2: è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆå¿½ç•¥èƒŒæ™¯ç±»ï¼‰

```python
class_mapping = {
    1: 0,  # Tree
    2: 1,  # Building
    3: 2,  # Ground
    # ç±»åˆ« 0 (èƒŒæ™¯) ä¸åœ¨ mapping ä¸­ï¼Œä¼šè¢«è®¾ä¸º ignore_label
}

datamodule = BinPklDataModule(
    data_root="data/custom",
    class_mapping=class_mapping,
    ignore_label=-1,
)
```

### åœºæ™¯ 3: ä¸ä½¿ç”¨ class_mappingï¼ˆå…¨éƒ¨è®­ç»ƒï¼‰

```python
# ä¸æä¾› class_mappingï¼Œæ‰€æœ‰ç±»åˆ«éƒ½å‚ä¸è®­ç»ƒ
datamodule = BinPklDataModule(
    data_root="data/custom",
    class_mapping=None,  # é»˜è®¤
    ignore_label=-1,
)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. `num_classes` å¿…é¡»ä¸æ˜ å°„åä¸€è‡´

```python
# âŒ é”™è¯¯ï¼šnum_classes ä¸ class_mapping ä¸ä¸€è‡´
class_mapping = {0: 0, 1: 1, 2: 2, 6: 3, 8: 4}  # 5 ä¸ªç±»åˆ«
task = SemanticSegmentationTask(
    num_classes=9,  # é”™è¯¯ï¼åº”è¯¥æ˜¯ 5
    ...
)

# âœ… æ­£ç¡®
task = SemanticSegmentationTask(
    num_classes=5,  # ä¸ class_mapping çš„ç›®æ ‡ç±»åˆ«æ•°ä¸€è‡´
    ...
)
```

### 2. ç¡®ä¿ Loss å’Œ Metrics çš„ `ignore_index` ä¸€è‡´

```python
# Task åˆå§‹åŒ–æ—¶ä¼ é€’ ignore_label
task = SemanticSegmentationTask(
    model_cfg=...,
    loss_cfg={
        'type': 'CrossEntropyLoss',
        'ignore_index': -1,  # ä¸ datamodule.ignore_label ä¸€è‡´
    },
    metric_cfg={
        'ignore_index': -1,  # ä¸ datamodule.ignore_label ä¸€è‡´
    },
)
```

### 3. SegmentationWriter ä¼šä¿å­˜æ‰€æœ‰ç‚¹

åœ¨é¢„æµ‹æ—¶ï¼Œå³ä½¿æŸäº›ç‚¹çš„æ ‡ç­¾æ˜¯ `ignore_label`ï¼Œå®ƒä»¬ä»ç„¶ä¼šï¼š
- å‚ä¸æ¨¡å‹æ¨ç†
- è¢«èµ‹äºˆé¢„æµ‹ç±»åˆ«
- ä¿å­˜åˆ°è¾“å‡º LAS æ–‡ä»¶ä¸­

è¿™æ˜¯**é¢„æœŸè¡Œä¸º**ï¼Œå› ä¸ºï¼š
1. ä¿æŒç‚¹äº‘å®Œæ•´æ€§
2. ç”¨æˆ·å¯èƒ½éœ€è¦è¿™äº›ç‚¹çš„é¢„æµ‹ç»“æœï¼ˆå³ä½¿è®­ç»ƒæ—¶æ²¡ç”¨ï¼‰

å¦‚æœä½ æƒ³åœ¨è¾“å‡ºä¸­æ ‡è®°è¿™äº›ç‚¹ï¼Œå¯ä»¥ä¿®æ”¹ `SegmentationWriter`ï¼š

```python
# åœ¨ callbacks.py ä¸­æ·»åŠ é€‰é¡¹
class SegmentationWriter:
    def __init__(
        self,
        output_dir,
        mark_ignored_points=False,  # æ–°å¢
        ignored_label_value=255,    # æ–°å¢ï¼šç”¨ä»€ä¹ˆå€¼æ ‡è®°
    ):
        self.mark_ignored_points = mark_ignored_points
        self.ignored_label_value = ignored_label_value
```

## ğŸ” éªŒè¯æ–¹æ³•

### æµ‹è¯• 1: æ£€æŸ¥ Dataset è¾“å‡º

```python
from pointsuite.data.datasets.dataset_bin import BinPklDataset

dataset = BinPklDataset(
    data_root="data/dales/train",
    class_mapping={0: 0, 1: 1, 2: 2, 6: 3, 8: 4},
    ignore_label=-1,
    assets=['coord', 'class'],
)

# åŠ è½½ä¸€ä¸ªæ ·æœ¬
sample = dataset[0]
labels = sample['class']

print(f"å”¯ä¸€æ ‡ç­¾: {np.unique(labels)}")
# é¢„æœŸè¾“å‡º: [âˆ’1, 0, 1, 2, 3, 4]
#           â†‘ ignore_label
print(f"ignore_label ç‚¹æ•°: {(labels == -1).sum()}")
```

### æµ‹è¯• 2: æ£€æŸ¥ Loss æ˜¯å¦å¿½ç•¥

```python
import torch
from pointsuite.models.losses.cross_entropy import CrossEntropyLoss

loss_fn = CrossEntropyLoss(ignore_index=-1)

# æ¨¡æ‹Ÿæ•°æ®
preds = torch.randn(100, 5)  # [N, C]
target = torch.randint(0, 5, (100,))  # [N]
target[0:10] = -1  # å‰ 10 ä¸ªç‚¹è®¾ä¸º ignore_label

loss = loss_fn(preds, {'class': target})

# éªŒè¯ï¼šä¿®æ”¹ ignore_label ç‚¹çš„é¢„æµ‹ï¼Œloss åº”è¯¥ä¸å˜
preds_modified = preds.clone()
preds_modified[0:10] = torch.randn(10, 5) * 100  # å¤§å¹…ä¿®æ”¹
loss_modified = loss_fn(preds_modified, {'class': target})

print(f"Loss: {loss:.4f}")
print(f"Loss (modified): {loss_modified:.4f}")
print(f"Difference: {abs(loss - loss_modified):.6f}")  # åº”è¯¥æ¥è¿‘ 0
```

### æµ‹è¯• 3: æ£€æŸ¥ Metrics æ˜¯å¦è¿‡æ»¤

```python
from pointsuite.utils.metrics import OverallAccuracy

metric = OverallAccuracy(ignore_index=-1)

# æ¨¡æ‹Ÿæ•°æ®
preds = torch.tensor([0, 1, 2, 3, 4, -1, -1, -1])
target = torch.tensor([0, 1, 2, 3, 4, -1, -1, -1])

metric.update(preds, target)
acc = metric.compute()

print(f"Accuracy: {acc:.4f}")  # åº”è¯¥æ˜¯ 1.0
# å› ä¸ºå‰ 5 ä¸ªç‚¹å…¨å¯¹ï¼Œå 3 ä¸ª ignore_label ç‚¹è¢«è¿‡æ»¤
```

## ğŸ“Š ä¼˜åŠ¿æ€»ç»“

| æ–¹é¢ | åŸæ¥çš„å®ç° | æ–°çš„å®ç° |
|------|-----------|---------|
| **ä¸åœ¨ mapping ä¸­çš„ç±»åˆ«** | ä¿æŒåŸå§‹æ ‡ç­¾å€¼ | è®¾ä¸º `ignore_label` |
| **æŸå¤±è®¡ç®—** | âŒ é”™è¯¯åœ°å‚ä¸è®¡ç®— | âœ… æ­£ç¡®åœ°è¢«æ’é™¤ |
| **ç²¾åº¦è¯„ä¼°** | âŒ é”™è¯¯åœ°è¢«ç»Ÿè®¡ | âœ… æ­£ç¡®åœ°è¢«è¿‡æ»¤ |
| **ç‚¹äº‘å®Œæ•´æ€§** | âœ… ä¿æŒ | âœ… ä¿æŒ |
| **è¯­ä¹‰æ­£ç¡®æ€§** | âŒ ä¸ç¬¦åˆé¢„æœŸ | âœ… ç¬¦åˆé¢„æœŸ |

## ğŸ¯ æ€»ç»“

ä½ çš„æ–¹æ¡ˆæ˜¯**å®Œå…¨æ­£ç¡®çš„**ï¼ä¿®æ”¹åçš„å®ç°ï¼š

1. âœ… **Dataset å±‚é¢**: ä¸åœ¨ `class_mapping` ä¸­çš„ç±»åˆ« â†’ `ignore_label`
2. âœ… **Loss å±‚é¢**: å·²æ”¯æŒ `ignore_index`ï¼Œè‡ªåŠ¨æ’é™¤
3. âœ… **Metrics å±‚é¢**: å·²æ”¯æŒ `ignore_index`ï¼Œè‡ªåŠ¨è¿‡æ»¤
4. âœ… **SegmentationWriter**: ä¿æŒç‚¹äº‘å®Œæ•´æ€§ï¼Œä»ç„¶è¾“å‡ºæ‰€æœ‰ç‚¹

**æ²¡æœ‰æ¼æ´**ï¼Œæ¶æ„å·²ç»å®Œç¾æ”¯æŒè¿™ä¸ªæœºåˆ¶ï¼
