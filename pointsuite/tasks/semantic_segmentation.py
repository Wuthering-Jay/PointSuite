import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List, Optional

from .base_task import BaseTask
from ..utils.logger import Colors, log_warning


# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—å­—ç¬¦ä¸²çš„æ˜¾ç¤ºå®½åº¦ï¼ˆä¸­æ–‡å­—ç¬¦å  2 ä¸ªå®½åº¦ï¼‰
def _display_width(s: str) -> int:
    """è®¡ç®—å­—ç¬¦ä¸²çš„æ˜¾ç¤ºå®½åº¦ï¼ˆä¸­æ–‡å­—ç¬¦å  2 ä¸ªå®½åº¦ï¼‰"""
    width = 0
    for c in s:
        if '\u4e00' <= c <= '\u9fff':  # CJK ç»Ÿä¸€æ±‰å­—
            width += 2
        else:
            width += 1
    return width


def _pad_to_width(s: str, target_width: int) -> str:
    """å°†å­—ç¬¦ä¸²å¡«å……åˆ°æŒ‡å®šæ˜¾ç¤ºå®½åº¦"""
    current_width = _display_width(s)
    padding = target_width - current_width
    return s + ' ' * max(0, padding)


class SemanticSegmentationTask(BaseTask):
    """
    è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ (LightningModule)ã€‚

    å®ƒç»§æ‰¿è‡ª BaseTaskï¼Œå¹¶æ·»åŠ äº†ç‰¹å®šäºè¯­ä¹‰åˆ†å‰²çš„ç»„ä»¶ï¼š
    1. ä¸€ä¸ª `head` (åˆ†å‰²å¤´)ã€‚
    2. ä¿®æ”¹äº† `forward` é€»è¾‘ï¼Œä»¥è¿æ¥ backbone å’Œ headã€‚
    3. ä¿®æ”¹äº† `predict_step` ä»¥è¾“å‡ºæœ€ç»ˆçš„ argmax é¢„æµ‹ã€‚
    4. è¿½è¸ªæœ€ä½³ mIoU å¹¶æ‰“å°è¯¦ç»†çš„æ¯ç±»æŒ‡æ ‡ã€‚
    """
    
    def __init__(self,
                 backbone: nn.Module = None,
                 head: nn.Module = None,
                 model_config: Dict[str, Any] = None,
                 **kwargs): # æ¥æ”¶æ¥è‡ª BaseTask çš„æ‰€æœ‰å‚æ•° (learning_rate, loss_configs, etc.)
        """
        Args:
            backbone (nn.Module): å·²ç»å®ä¾‹åŒ–çš„éª¨å¹²ç½‘ç»œ (ä¾‹å¦‚ PT-v2m5)ã€‚
            head (nn.Module): å·²ç»å®ä¾‹åŒ–çš„åˆ†å‰²å¤´ (ä¾‹å¦‚ SegmentationHead)ã€‚
            model_config (Dict): æ¨¡å‹é…ç½®å­—å…¸ï¼Œç”¨äºä»é…ç½®å®ä¾‹åŒ– backbone å’Œ headã€‚
                                 å¦‚æœæä¾›äº† model_configï¼Œåˆ™å¿½ç•¥ backbone å’Œ head å‚æ•°ã€‚
                                 æ ¼å¼:
                                 {
                                     'backbone': {'class_path': '...', 'init_args': {...}},
                                     'head': {'class_path': '...', 'init_args': {...}}
                                 }
            **kwargs: ä¼ é€’ç»™ BaseTask çš„å‚æ•°ã€‚
        """
        super().__init__(**kwargs)
        
        # è¿½è¸ªæœ€ä½³ mIoUï¼ˆè¯­ä¹‰åˆ†å‰²ç‰¹å®šï¼‰
        self.best_miou = 0.0
        self.best_miou_epoch = -1
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¿å­˜ hyperparameters
        # å¦‚æœä½¿ç”¨ model_configï¼Œæˆ‘ä»¬å¿½ç•¥ backbone å’Œ head å¯¹è±¡ï¼Œé¿å…é‡å¤ä¿å­˜å’Œè­¦å‘Š
        # å¦‚æœä½¿ç”¨ backbone/head å¯¹è±¡ï¼Œæˆ‘ä»¬å¿…é¡»ä¿å­˜å®ƒä»¬ä»¥æ”¯æŒè‡ªåŠ¨é‡å»ºï¼ˆå°½ç®¡ä¼šæœ‰è­¦å‘Šï¼‰
        if model_config is not None:
            self.save_hyperparameters(ignore=['backbone', 'head'])
            
            # ä»é…ç½®å®ä¾‹åŒ–
            backbone_cfg = model_config.get('backbone')
            head_cfg = model_config.get('head')
            
            if backbone_cfg:
                backbone_cls = self._import_class(backbone_cfg['class_path'])
                self.backbone = backbone_cls(**backbone_cfg.get('init_args', {}))
            
            if head_cfg:
                head_cls = self._import_class(head_cfg['class_path'])
                self.head = head_cls(**head_cfg.get('init_args', {}))
                
        else:
            # å…¼å®¹æ—§æ–¹å¼ï¼šç›´æ¥ä¼ å…¥å¯¹è±¡
            # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä¸ ignore backbone/headï¼Œä»¥ä¾¿ load_from_checkpoint èƒ½å·¥ä½œ
            # ç”¨æˆ·ä¼šçœ‹åˆ° PL çš„è­¦å‘Šï¼Œä½†è¿™æ˜¯é¢„æœŸçš„
            self.save_hyperparameters()
            self.backbone = backbone
            self.head = head
            
        # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
        if not hasattr(self, 'backbone') or self.backbone is None:
            raise ValueError("Backbone æœªåˆå§‹åŒ–ï¼è¯·æä¾› backbone å¯¹è±¡æˆ– model_config")
        if not hasattr(self, 'head') or self.head is None:
            raise ValueError("Head æœªåˆå§‹åŒ–ï¼è¯·æä¾› head å¯¹è±¡æˆ– model_config")

    def forward(self, batch: Dict[str, Any]) -> torch.Tensor:
        """
        å®šä¹‰æ¨¡å‹çš„å•æ¬¡å‰å‘ä¼ æ’­ã€‚
        
        Args:
            batch (Dict): æ¥è‡ª DataLoader çš„æ‰¹æ¬¡æ•°æ® (ç”± collate_fn äº§ç”Ÿ)ã€‚
                          æˆ‘ä»¬çš„ collate_fn æä¾›:
                          - 'coord': [N, 3] ç‚¹åæ ‡
                          - 'feat': [N, C] ç‚¹ç‰¹å¾
                          - 'class': [N] ç‚¹æ ‡ç­¾
                          - 'offset': [B] ç´¯ç§¯åç§»é‡
        
        Returns:
            torch.Tensor: æ¨¡å‹çš„åŸå§‹ Logits è¾“å‡º (shape: [N_total_points, num_classes])ã€‚
        """
        # 1. Backbone æå–ç‰¹å¾
        # ä¸åŒ backbone å¯èƒ½æœ‰ä¸åŒçš„è¾“å…¥æ ¼å¼ï¼š
        # - ç®€å•æ¨¡å‹ï¼šç›´æ¥æ¥æ”¶ batch['feat']
        # - PointTransformerV2/PointNet++ï¼šéœ€è¦æ•´ä¸ª batch å­—å…¸
        
        # æ£€æŸ¥ backbone æ˜¯å¦éœ€è¦æ•´ä¸ª batch å­—å…¸
        # æ–¹æ³•1: æ£€æŸ¥å‚æ•°åæ˜¯å¦ä¸º 'batch' æˆ– 'data_dict'
        # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦ä¸º PointTransformerV2 ç­‰å·²çŸ¥éœ€è¦ dict çš„æ¨¡å‹
        forward_params = self.backbone.forward.__code__.co_varnames if hasattr(self.backbone, 'forward') else []
        needs_dict = ('batch' in forward_params or 
                     'data_dict' in forward_params or
                     'PointTransformerV2' in self.backbone.__class__.__name__)
        
        if needs_dict:
            # Backbone æ¥æ”¶æ•´ä¸ª batch å­—å…¸
            backbone_output = self.backbone(batch)
        else:
            # Backbone åªæ¥æ”¶ç‰¹å¾å¼ é‡ï¼ˆå¦‚ç®€å• MLPï¼‰
            backbone_output = self.backbone(batch.get('feat', batch.get('coord')))
        
        # 2. å¤„ç† backbone è¾“å‡º
        # å¦‚æœè¾“å‡ºæ˜¯å­—å…¸ï¼ˆå¦‚ PointNet++ è¿”å› {'feat': ..., 'sa_xyz': ...}ï¼‰
        if isinstance(backbone_output, dict):
            features = backbone_output['feat']  # æå–ç‰¹å¾
        else:
            features = backbone_output
        
        # 3. Head ç”Ÿæˆ logits
        logits = self.head(features)
        return logits

    def training_step(self, batch: Dict[str, Any], batch_idx: int) -> torch.Tensor:
        """
        æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤ã€‚
        """
        # 1. å‰å‘ä¼ æ’­
        preds_logits = self.forward(batch)
        
        # 2. è®¡ç®—æŸå¤± (ä½¿ç”¨ BaseTask çš„è¾…åŠ©å‡½æ•°)
        #    BaseTask._calculate_total_loss é»˜è®¤ä¼šè°ƒç”¨ loss(preds, batch)
        #    æ‚¨çš„æŸå¤±å‡½æ•° (ä¾‹å¦‚ CrossEntropyLoss) éœ€è¦çŸ¥é“å¦‚ä½•ä» 'preds' (logits)
        #    å’Œ 'batch' (åŒ…å« 'class') ä¸­æå–æ‰€éœ€ä¿¡æ¯ã€‚
        loss_dict = self._calculate_total_loss(preds_logits, batch)
        
        # 3. è®°å½•è®­ç»ƒæŸå¤± (PL ä¼šè‡ªåŠ¨æ·»åŠ  'train/' å‰ç¼€)
        #    prog_bar=True ä¼šåœ¨è¿›åº¦æ¡ä¸Šæ˜¾ç¤º 'total_loss'
        batch_size = self._get_batch_size(batch)
        self.log_dict(loss_dict, on_step=True, on_epoch=True, prog_bar=True, batch_size=batch_size)
        
        # 4. è¿”å›æ€»æŸå¤±
        return loss_dict["total_loss"]

    def predict_step(self, batch: Dict[str, Any], batch_idx: int, dataloader_idx: int = 0) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡Œå•ä¸ªé¢„æµ‹æ­¥éª¤ï¼ˆç”¨äºç”Ÿäº§ç¯å¢ƒã€æ— çœŸå€¼æ ‡ç­¾ï¼‰
        
        ä¸ test_step çš„åŒºåˆ«ï¼š
        - predict_step: æ— çœŸå€¼æ ‡ç­¾ï¼Œä¸è®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡ï¼Œåªè¿”å›é¢„æµ‹ç»“æœ
        - test_step: æœ‰çœŸå€¼æ ‡ç­¾ï¼Œè®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡ï¼Œå¯é€‰ä¿å­˜é¢„æµ‹ç»“æœ
        
        ä½¿ç”¨åœºæ™¯ï¼š
        - æ–°åœºæ™¯é¢„æµ‹ï¼ˆæ— æ ‡ç­¾ï¼‰
        - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
        - éœ€è¦ä¿å­˜ .las æ–‡ä»¶æ—¶ä½¿ç”¨ Trainer.predict() + SemanticPredictLasWriter
        """
        # 1. å‰å‘ä¼ æ’­
        preds = self.forward(batch)
        
        # 2. åå¤„ç†é¢„æµ‹ (æ”¯æŒ Mask3D ç­‰å¤æ‚è¾“å‡º)
        #    å­ç±»å¯ä»¥è¦†ç›– postprocess_predictions æ¥è‡ªå®šä¹‰è¡Œä¸º
        processed_preds = self.postprocess_predictions(preds)
        
        # 3. è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒPredictionWriter å›è°ƒå°†å¤„ç†è¿™ä¸ªå­—å…¸
        #    æˆ‘ä»¬è¿”å› CPU å¼ é‡ä»¥é‡Šæ”¾ GPU å†…å­˜
        results = {
            "logits": processed_preds.cpu(),  # å¯ä»¥æ˜¯ logits [N, C] æˆ– labels [N]
        }
        
        # (å¯é€‰) å¦‚æœéœ€è¦åŸå§‹ç´¢å¼• (ç”¨äºæ‹¼æ¥/æŠ•ç¥¨)
        # æˆ‘ä»¬çš„æ•°æ®é›†å¯èƒ½æä¾› 'indices' å­—æ®µ
        if "indices" in batch:
            results["indices"] = batch["indices"].cpu()
        
        # ğŸ”¥ ä¼ é€’æ–‡ä»¶ä¿¡æ¯åˆ° callbackï¼ˆç”¨äºé¢„æµ‹ç»“æœçš„æ–‡ä»¶çº§èšåˆï¼‰
        # è¿™äº›ä¿¡æ¯ç”± dataset åœ¨ test/predict split æ—¶æä¾›
        if "bin_file" in batch:
            results["bin_file"] = batch["bin_file"]  # æ–‡ä»¶æ ‡è¯†ç¬¦ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        if "bin_path" in batch:
            results["bin_path"] = batch["bin_path"]  # åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
        if "pkl_path" in batch:
            results["pkl_path"] = batch["pkl_path"]  # å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        
        # ä¿å­˜åæ ‡ä¿¡æ¯ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        if "coord" in batch:
            results["coord"] = batch["coord"].cpu()
            
        return results
    
    def _print_validation_metrics(self, print_metrics: Dict[str, Any]):
        """
        æ‰“å°è¯­ä¹‰åˆ†å‰²çš„è¯¦ç»†éªŒè¯æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¯ç±»çš„ IoUã€Precisionã€Recallã€F1
        
        Args:
            print_metrics: åŒ…å«æ‰€æœ‰è®¡ç®—å‡ºçš„æŒ‡æ ‡çš„å­—å…¸
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ mIoU ä¿¡æ¯
        miou_key = 'mean_iou'
        if miou_key not in print_metrics:
            # å›é€€åˆ°åŸºç±»çš„ç®€å•æ‰“å°
            super()._print_validation_metrics(print_metrics)
            return
        
        try:
            current_miou = float(print_metrics[miou_key])
            
            # æ›´æ–°æœ€ä½³ mIoU
            if current_miou > self.best_miou:
                self.best_miou = current_miou
                self.best_miou_epoch = self.current_epoch
            
            # è·å–å…¶ä»–æŒ‡æ ‡
            overall_acc = print_metrics.get('overall_accuracy', None)
            
            # å‡†å¤‡æ¯ç±»æŒ‡æ ‡
            per_class_iou = print_metrics.get('iou_per_class', print_metrics.get('per_class_iou', None))
            per_class_precision = print_metrics.get('precision_per_class', print_metrics.get('per_class_precision', None))
            per_class_recall = print_metrics.get('recall_per_class', print_metrics.get('per_class_recall', None))
            per_class_f1 = print_metrics.get('f1_per_class', print_metrics.get('per_class_f1', None))
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥æä¾›æ¯ç±»æŒ‡æ ‡ï¼Œå°è¯•ä» MeanIoU metric å¯¹è±¡ä¸­è·å– (å…¼å®¹æ—§ä»£ç )
            if per_class_iou is None and 'mean_iou' in self.val_metrics:
                metric = self.val_metrics['mean_iou']
                if hasattr(metric, 'confusion_matrix'):
                    confmat = metric.confusion_matrix.cpu().numpy()
                    intersection = np.diag(confmat)
                    union = confmat.sum(1) + confmat.sum(0) - np.diag(confmat)
                    per_class_iou = intersection / (union + 1e-10)
                    per_class_precision = intersection / (confmat.sum(0) + 1e-10)
                    per_class_recall = intersection / (confmat.sum(1) + 1e-10)
                    per_class_f1 = 2 * per_class_precision * per_class_recall / (per_class_precision + per_class_recall + 1e-10)

            # è¾“å‡ºæ ‡é¢˜å’Œæ€»ä½“æŒ‡æ ‡ (epoch ä» 1 å¼€å§‹æ˜¾ç¤º)
            display_epoch = self.current_epoch + 1
            print(f"\n{Colors.BOLD}{Colors.SUCCESS}{'='*100}{Colors.RESET}")
            print(f"{Colors.BOLD}Validation Epoch {display_epoch} - Metrics{Colors.RESET}")
            print(f"{Colors.BOLD}{Colors.SUCCESS}{'='*100}{Colors.RESET}")
            if overall_acc is not None:
                print(f"Overall Accuracy: {Colors.SUCCESS}{overall_acc:.4f}{Colors.RESET} ({overall_acc*100:.2f}%)")
            print(f"Mean IoU (current): {Colors.SUCCESS}{current_miou:.4f}{Colors.RESET}")
            print(f"Mean IoU (best)   : {Colors.SUCCESS}{self.best_miou:.4f}{Colors.RESET} (Epoch {self.best_miou_epoch + 1})")
            if current_miou > self.best_miou - 1e-6:  # å½“å‰æ˜¯æœ€ä½³
                print(f"{Colors.SUCCESS}* New best mIoU achieved!{Colors.RESET}")
            print(f"{Colors.BOLD}{Colors.SUCCESS}{'='*100}{Colors.RESET}")
            
            # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
            if per_class_iou is not None:
                self._print_per_class_metrics(
                    per_class_iou, per_class_precision, per_class_recall, per_class_f1,
                    print_metrics, current_miou
                )
            print(f"{Colors.BOLD}{Colors.SUCCESS}{'='*100}{Colors.RESET}\n")
        except Exception as e:
            log_warning(f"Could not print detailed metrics: {e}")
            import traceback
            traceback.print_exc()
    
    def _print_test_metrics(self, print_metrics: Dict[str, Any]):
        """
        æ‰“å°è¯­ä¹‰åˆ†å‰²çš„è¯¦ç»†æµ‹è¯•æŒ‡æ ‡
        
        Args:
            print_metrics: åŒ…å«æ‰€æœ‰è®¡ç®—å‡ºçš„æŒ‡æ ‡çš„å­—å…¸
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ mIoU ä¿¡æ¯
        miou_key = 'mean_iou'
        if miou_key not in print_metrics:
            # å›é€€åˆ°åŸºç±»çš„ç®€å•æ‰“å°
            super()._print_test_metrics(print_metrics)
            return
        
        try:
            current_miou = float(print_metrics[miou_key])
            overall_acc = print_metrics.get('overall_accuracy', None)
            
            # å‡†å¤‡æ¯ç±»æŒ‡æ ‡
            per_class_iou = print_metrics.get('iou_per_class', print_metrics.get('per_class_iou', None))
            per_class_precision = print_metrics.get('precision_per_class', print_metrics.get('per_class_precision', None))
            per_class_recall = print_metrics.get('recall_per_class', print_metrics.get('per_class_recall', None))
            per_class_f1 = print_metrics.get('f1_per_class', print_metrics.get('per_class_f1', None))
            
            # å…¼å®¹æ—§ä»£ç 
            if per_class_iou is None and 'mean_iou' in self.test_metrics:
                metric = self.test_metrics['mean_iou']
                if hasattr(metric, 'confusion_matrix'):
                    confmat = metric.confusion_matrix.cpu().numpy()
                    intersection = np.diag(confmat)
                    union = confmat.sum(1) + confmat.sum(0) - np.diag(confmat)
                    per_class_iou = intersection / (union + 1e-10)
                    per_class_precision = intersection / (confmat.sum(0) + 1e-10)
                    per_class_recall = intersection / (confmat.sum(1) + 1e-10)
                    per_class_f1 = 2 * per_class_precision * per_class_recall / (per_class_precision + per_class_recall + 1e-10)

            print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*100}{Colors.RESET}")
            print(f"{Colors.BOLD}Test Results - Metrics{Colors.RESET}")
            print(f"{Colors.BOLD}{Colors.CYAN}{'='*100}{Colors.RESET}")
            if overall_acc is not None:
                print(f"Overall Accuracy: {Colors.SUCCESS}{overall_acc:.4f}{Colors.RESET} ({overall_acc*100:.2f}%)")
            print(f"Mean IoU: {Colors.SUCCESS}{current_miou:.4f}{Colors.RESET}")
            print(f"{Colors.BOLD}{Colors.CYAN}{'='*100}{Colors.RESET}")
            
            if per_class_iou is not None:
                self._print_per_class_metrics(
                    per_class_iou, per_class_precision, per_class_recall, per_class_f1,
                    print_metrics, current_miou
                )
            print(f"{Colors.BOLD}{Colors.CYAN}{'='*100}{Colors.RESET}\n")
        except Exception as e:
            log_warning(f"Could not print detailed test metrics: {e}")
            import traceback
            traceback.print_exc()
    
    def _print_per_class_metrics(
        self, 
        per_class_iou, 
        per_class_precision, 
        per_class_recall, 
        per_class_f1,
        print_metrics: Dict[str, Any],
        mean_iou: float
    ):
        """
        æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
        
        Args:
            per_class_iou: æ¯ç±» IoU æ•°ç»„
            per_class_precision: æ¯ç±» Precision æ•°ç»„
            per_class_recall: æ¯ç±» Recall æ•°ç»„
            per_class_f1: æ¯ç±» F1 æ•°ç»„
            print_metrics: æŒ‡æ ‡å­—å…¸ï¼ˆç”¨äºè·å–ç±»åˆ«åï¼‰
            mean_iou: å¹³å‡ IoU
        """
        # è·å–ç±»åˆ«å
        class_names = print_metrics.get('class_names', None)
        if class_names is None:
            class_names = self.hparams.get('class_names', None) if hasattr(self, 'hparams') else None
        
        # ç¡®ä¿æ˜¯ numpy æ•°ç»„
        if isinstance(per_class_iou, torch.Tensor): 
            per_class_iou = per_class_iou.cpu().numpy()
        if isinstance(per_class_precision, torch.Tensor): 
            per_class_precision = per_class_precision.cpu().numpy()
        if isinstance(per_class_recall, torch.Tensor): 
            per_class_recall = per_class_recall.cpu().numpy()
        if isinstance(per_class_f1, torch.Tensor): 
            per_class_f1 = per_class_f1.cpu().numpy()
        
        num_classes = len(per_class_iou)
        
        # è®¡ç®—æœ€å¤§ç±»åˆ«åå®½åº¦
        max_name_width = 8  # æœ€å°å®½åº¦
        for i in range(num_classes):
            c_name = class_names[i] if class_names and i < len(class_names) else f"Class {i}"
            max_name_width = max(max_name_width, _display_width(c_name))
        max_name_width = min(max_name_width, 20)  # æœ€å¤§å®½åº¦é™åˆ¶
        
        # è¡¨å¤´
        header_class = _pad_to_width("Class", max_name_width)
        print(f"  {header_class}  {'IoU':>8}  {'Precision':>10}  {'Recall':>8}  {'F1-Score':>10}")
        print(f"  {'-'*max_name_width}  {'-'*8}  {'-'*10}  {'-'*8}  {'-'*10}")
        
        for i in range(num_classes):
            c_name = class_names[i] if class_names and i < len(class_names) else f"Class {i}"
            c_name_padded = _pad_to_width(c_name, max_name_width)
            print(f"  {c_name_padded}  {per_class_iou[i]:8.4f}  {per_class_precision[i]:10.4f}  "
                  f"{per_class_recall[i]:8.4f}  {per_class_f1[i]:10.4f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        mean_precision = np.nanmean(per_class_precision)
        mean_recall = np.nanmean(per_class_recall)
        mean_f1 = np.nanmean(per_class_f1)
        
        print(f"  {'-'*max_name_width}  {'-'*8}  {'-'*10}  {'-'*8}  {'-'*10}")
        mean_label = _pad_to_width("Mean", max_name_width)
        print(f"  {mean_label}  {mean_iou:8.4f}  {mean_precision:10.4f}  "
              f"{mean_recall:8.4f}  {mean_f1:10.4f}")