import torch
import torch.nn as nn
from typing import Dict, Any

from .base_task import BaseTask

class SemanticSegmentationTask(BaseTask):
    """
    è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ (LightningModule)ã€‚

    å®ƒç»§æ‰¿è‡ª BaseTaskï¼Œå¹¶æ·»åŠ äº†ç‰¹å®šäºè¯­ä¹‰åˆ†å‰²çš„ç»„ä»¶ï¼š
    1. ä¸€ä¸ª `head` (åˆ†å‰²å¤´)ã€‚
    2. ä¿®æ”¹äº† `forward` é€»è¾‘ï¼Œä»¥è¿æ¥ backbone å’Œ headã€‚
    3. ä¿®æ”¹äº† `predict_step` ä»¥è¾“å‡ºæœ€ç»ˆçš„ argmax é¢„æµ‹ã€‚
    """
    
    def __init__(self,
                 backbone: nn.Module = None,
                 head: nn.Module = None,
                 model_config: Dict[str, Any] = None,
                 **kwargs): # æ¥æ”¶æ¥è‡ª BaseTask çš„æ‰€æœ‰å‚æ•° (learning_rate, loss_configs, etc.)
        """
        Args:
            backbone (nn.Module): å·²ç»å®ä¾‹åŒ–çš„éª¨å¹²ç½‘ç»œ (ä¾‹å¦‚ PT-v2m5)ã€‚
            head (nn.Module): å·²ç»å®ä¾‹åŒ–çš„åˆ†å‰²å¤´ (ä¾‹å¦‚ SegmentationHead)ã€‚
            model_config (Dict): æ¨¡å‹é…ç½®å­—å…¸ï¼Œç”¨äºä»é…ç½®å®ä¾‹åŒ– backbone å’Œ headã€‚
                                 å¦‚æœæä¾›äº† model_configï¼Œåˆ™å¿½ç•¥ backbone å’Œ head å‚æ•°ã€‚
                                 æ ¼å¼:
                                 {
                                     'backbone': {'class_path': '...', 'init_args': {...}},
                                     'head': {'class_path': '...', 'init_args': {...}}
                                 }
            **kwargs: ä¼ é€’ç»™ BaseTask çš„å‚æ•°ã€‚
        """
        super().__init__(**kwargs)
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¿å­˜ hyperparameters
        # å¦‚æœä½¿ç”¨ model_configï¼Œæˆ‘ä»¬å¿½ç•¥ backbone å’Œ head å¯¹è±¡ï¼Œé¿å…é‡å¤ä¿å­˜å’Œè­¦å‘Š
        # å¦‚æœä½¿ç”¨ backbone/head å¯¹è±¡ï¼Œæˆ‘ä»¬å¿…é¡»ä¿å­˜å®ƒä»¬ä»¥æ”¯æŒè‡ªåŠ¨é‡å»ºï¼ˆå°½ç®¡ä¼šæœ‰è­¦å‘Šï¼‰
        if model_config is not None:
            self.save_hyperparameters(ignore=['backbone', 'head'])
            
            # ä»é…ç½®å®ä¾‹åŒ–
            backbone_cfg = model_config.get('backbone')
            head_cfg = model_config.get('head')
            
            if backbone_cfg:
                backbone_cls = self._import_class(backbone_cfg['class_path'])
                self.backbone = backbone_cls(**backbone_cfg.get('init_args', {}))
            
            if head_cfg:
                head_cls = self._import_class(head_cfg['class_path'])
                self.head = head_cls(**head_cfg.get('init_args', {}))
                
        else:
            # å…¼å®¹æ—§æ–¹å¼ï¼šç›´æ¥ä¼ å…¥å¯¹è±¡
            # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä¸ ignore backbone/headï¼Œä»¥ä¾¿ load_from_checkpoint èƒ½å·¥ä½œ
            # ç”¨æˆ·ä¼šçœ‹åˆ° PL çš„è­¦å‘Šï¼Œä½†è¿™æ˜¯é¢„æœŸçš„
            self.save_hyperparameters()
            self.backbone = backbone
            self.head = head
            
        # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
        if not hasattr(self, 'backbone') or self.backbone is None:
            raise ValueError("Backbone æœªåˆå§‹åŒ–ï¼è¯·æä¾› backbone å¯¹è±¡æˆ– model_config")
        if not hasattr(self, 'head') or self.head is None:
            raise ValueError("Head æœªåˆå§‹åŒ–ï¼è¯·æä¾› head å¯¹è±¡æˆ– model_config")

    def forward(self, batch: Dict[str, Any]) -> torch.Tensor:
        """
        å®šä¹‰æ¨¡å‹çš„å•æ¬¡å‰å‘ä¼ æ’­ã€‚
        
        Args:
            batch (Dict): æ¥è‡ª DataLoader çš„æ‰¹æ¬¡æ•°æ® (ç”± collate_fn äº§ç”Ÿ)ã€‚
                          æˆ‘ä»¬çš„ collate_fn æä¾›:
                          - 'coord': [N, 3] ç‚¹åæ ‡
                          - 'feat': [N, C] ç‚¹ç‰¹å¾
                          - 'class': [N] ç‚¹æ ‡ç­¾
                          - 'offset': [B] ç´¯ç§¯åç§»é‡
        
        Returns:
            torch.Tensor: æ¨¡å‹çš„åŸå§‹ Logits è¾“å‡º (shape: [N_total_points, num_classes])ã€‚
        """
        # 1. Backbone æå–ç‰¹å¾
        # ä¸åŒ backbone å¯èƒ½æœ‰ä¸åŒçš„è¾“å…¥æ ¼å¼ï¼š
        # - ç®€å•æ¨¡å‹ï¼šç›´æ¥æ¥æ”¶ batch['feat']
        # - PointTransformerV2/PointNet++ï¼šéœ€è¦æ•´ä¸ª batch å­—å…¸
        
        # æ£€æŸ¥ backbone æ˜¯å¦éœ€è¦æ•´ä¸ª batch å­—å…¸
        # æ–¹æ³•1: æ£€æŸ¥å‚æ•°åæ˜¯å¦ä¸º 'batch' æˆ– 'data_dict'
        # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦ä¸º PointTransformerV2 ç­‰å·²çŸ¥éœ€è¦ dict çš„æ¨¡å‹
        forward_params = self.backbone.forward.__code__.co_varnames if hasattr(self.backbone, 'forward') else []
        needs_dict = ('batch' in forward_params or 
                     'data_dict' in forward_params or
                     'PointTransformerV2' in self.backbone.__class__.__name__)
        
        if needs_dict:
            # Backbone æ¥æ”¶æ•´ä¸ª batch å­—å…¸
            backbone_output = self.backbone(batch)
        else:
            # Backbone åªæ¥æ”¶ç‰¹å¾å¼ é‡ï¼ˆå¦‚ç®€å• MLPï¼‰
            backbone_output = self.backbone(batch.get('feat', batch.get('coord')))
        
        # 2. å¤„ç† backbone è¾“å‡º
        # å¦‚æœè¾“å‡ºæ˜¯å­—å…¸ï¼ˆå¦‚ PointNet++ è¿”å› {'feat': ..., 'sa_xyz': ...}ï¼‰
        if isinstance(backbone_output, dict):
            features = backbone_output['feat']  # æå–ç‰¹å¾
        else:
            features = backbone_output
        
        # 3. Head ç”Ÿæˆ logits
        logits = self.head(features)
        return logits

    def training_step(self, batch: Dict[str, Any], batch_idx: int) -> torch.Tensor:
        """
        æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤ã€‚
        """
        # 1. å‰å‘ä¼ æ’­
        preds_logits = self.forward(batch)
        
        # 2. è®¡ç®—æŸå¤± (ä½¿ç”¨ BaseTask çš„è¾…åŠ©å‡½æ•°)
        #    BaseTask._calculate_total_loss é»˜è®¤ä¼šè°ƒç”¨ loss(preds, batch)
        #    æ‚¨çš„æŸå¤±å‡½æ•° (ä¾‹å¦‚ CrossEntropyLoss) éœ€è¦çŸ¥é“å¦‚ä½•ä» 'preds' (logits)
        #    å’Œ 'batch' (åŒ…å« 'class') ä¸­æå–æ‰€éœ€ä¿¡æ¯ã€‚
        loss_dict = self._calculate_total_loss(preds_logits, batch)
        
        # 3. è®°å½•è®­ç»ƒæŸå¤± (PL ä¼šè‡ªåŠ¨æ·»åŠ  'train/' å‰ç¼€)
        #    prog_bar=True ä¼šåœ¨è¿›åº¦æ¡ä¸Šæ˜¾ç¤º 'total_loss'
        batch_size = self._get_batch_size(batch)
        self.log_dict(loss_dict, on_step=True, on_epoch=True, prog_bar=True, batch_size=batch_size)
        
        # 4. è¿”å›æ€»æŸå¤±
        return loss_dict["total_loss"]

    def predict_step(self, batch: Dict[str, Any], batch_idx: int, dataloader_idx: int = 0) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡Œå•ä¸ªé¢„æµ‹æ­¥éª¤ï¼ˆç”¨äºç”Ÿäº§ç¯å¢ƒã€æ— çœŸå€¼æ ‡ç­¾ï¼‰
        
        ä¸ test_step çš„åŒºåˆ«ï¼š
        - predict_step: æ— çœŸå€¼æ ‡ç­¾ï¼Œä¸è®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡ï¼Œåªè¿”å›é¢„æµ‹ç»“æœ
        - test_step: æœ‰çœŸå€¼æ ‡ç­¾ï¼Œè®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡ï¼Œå¯é€‰ä¿å­˜é¢„æµ‹ç»“æœ
        
        ä½¿ç”¨åœºæ™¯ï¼š
        - æ–°åœºæ™¯é¢„æµ‹ï¼ˆæ— æ ‡ç­¾ï¼‰
        - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
        - éœ€è¦ä¿å­˜ .las æ–‡ä»¶æ—¶ä½¿ç”¨ Trainer.predict() + SemanticPredictLasWriter
        """
        # 1. å‰å‘ä¼ æ’­
        preds = self.forward(batch)
        
        # 2. åå¤„ç†é¢„æµ‹ (æ”¯æŒ Mask3D ç­‰å¤æ‚è¾“å‡º)
        #    å­ç±»å¯ä»¥è¦†ç›– postprocess_predictions æ¥è‡ªå®šä¹‰è¡Œä¸º
        processed_preds = self.postprocess_predictions(preds)
        
        # 3. è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒPredictionWriter å›è°ƒå°†å¤„ç†è¿™ä¸ªå­—å…¸
        #    æˆ‘ä»¬è¿”å› CPU å¼ é‡ä»¥é‡Šæ”¾ GPU å†…å­˜
        results = {
            "logits": processed_preds.cpu(),  # å¯ä»¥æ˜¯ logits [N, C] æˆ– labels [N]
        }
        
        # (å¯é€‰) å¦‚æœéœ€è¦åŸå§‹ç´¢å¼• (ç”¨äºæ‹¼æ¥/æŠ•ç¥¨)
        # æˆ‘ä»¬çš„æ•°æ®é›†å¯èƒ½æä¾› 'indices' å­—æ®µ
        if "indices" in batch:
            results["indices"] = batch["indices"].cpu()
        
        # ğŸ”¥ ä¼ é€’æ–‡ä»¶ä¿¡æ¯åˆ° callbackï¼ˆç”¨äºé¢„æµ‹ç»“æœçš„æ–‡ä»¶çº§èšåˆï¼‰
        # è¿™äº›ä¿¡æ¯ç”± dataset åœ¨ test/predict split æ—¶æä¾›
        if "bin_file" in batch:
            results["bin_file"] = batch["bin_file"]  # æ–‡ä»¶æ ‡è¯†ç¬¦ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        if "bin_path" in batch:
            results["bin_path"] = batch["bin_path"]  # åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
        if "pkl_path" in batch:
            results["pkl_path"] = batch["pkl_path"]  # å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        
        # ä¿å­˜åæ ‡ä¿¡æ¯ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        if "coord" in batch:
            results["coord"] = batch["coord"].cpu()
            
        return results