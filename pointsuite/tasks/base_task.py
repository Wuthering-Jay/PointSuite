import torch
import torch.nn as nn
import pytorch_lightning as pl
import torchmetrics
import importlib
import yaml
from typing import List, Dict, Any

class BaseTask(pl.LightningModule):
    """
    ä¸€ä¸ªæŠ½è±¡çš„ä»»åŠ¡åŸºç±» (LightningModule)ã€‚
    
    å®ƒè´Ÿè´£å¤„ç†æ‰€æœ‰ä»»åŠ¡å…±æœ‰çš„é€»è¾‘ï¼š
    1. è‡ªåŠ¨ä» YAML é…ç½®ä¸­å®ä¾‹åŒ–æŸå¤±å‡½æ•° (losses)ã€‚
    2. è‡ªåŠ¨ä» YAML é…ç½®ä¸­å®ä¾‹åŒ–æŒ‡æ ‡ (metrics)ã€‚
    3. è‡ªåŠ¨åœ¨ validation/test epoch ç»“æŸæ—¶è®¡ç®—å’Œè®°å½•æ‰€æœ‰æŒ‡æ ‡ã€‚
    
    æ³¨æ„: 
    æˆ‘ä»¬ *ä¸* åœ¨è¿™é‡Œå®ç° `configure_optimizers`ã€‚
    PyTorch Lightning çš„ `LightningCLI` ä¼šè‡ªåŠ¨è¯»å–æ‚¨åœ¨
    `configs/schedules/` ç›®å½•ä¸­å®šä¹‰çš„ `optimizer` å’Œ `lr_scheduler` 
    é…ç½®ï¼Œå¹¶è‡ªåŠ¨ä¸ºæ‚¨é…ç½®å®ƒä»¬ã€‚è¿™ä¿æŒäº† Task æ¨¡å—çš„ç®€æ´ã€‚
    """
    
    def __init__(self,
                 learning_rate: float = 1e-3,
                 loss_configs: List[Dict[str, Any]] = None,
                 metric_configs: List[Dict[str, Any]] = None,
                 class_mapping: Dict[int, int] = None,
                 class_names: List[str] = None,
                 ignore_label: int = -1):
        """
        Args:
            learning_rate (float): å­¦ä¹ ç‡ã€‚
                                   æˆ‘ä»¬åœ¨æ­¤å¤„æ¥æ”¶ learning_rate (è€Œä¸æ˜¯ä»…åœ¨ä¼˜åŒ–å™¨é…ç½®ä¸­)
                                   ä¸»è¦æœ‰ä¸¤ä¸ªåŸå› :
                                   1. æ—¥å¿—è®°å½•: 'self.save_hyperparameters()' ä¼šè‡ªåŠ¨
                                      å°† 'learning_rate' è®°å½•åˆ° TensorBoard/Wandbã€‚
                                   2. çµæ´»æ€§: å…è®¸åœ¨ä¸ä½¿ç”¨ 'LightningCLI' çš„çº¯ Python æ¨¡å¼ä¸‹
                                      è½»æ¾è®¿é—® 'self.hparams.learning_rate' æ¥é…ç½®ä¼˜åŒ–å™¨ã€‚
                                   
                                   åœ¨ YAML é…ç½®ä¸­ï¼Œæˆ‘ä»¬åº”å°†æ­¤ 'learning_rate' è§†ä¸ºâ€œå•ä¸€äº‹å®æ¥æºâ€ï¼Œ
                                   å¹¶åœ¨ 'optimizer' é…ç½®ä¸­ä½¿ç”¨ YAML é“¾æ¥ (ä¾‹å¦‚:
                                   lr: ${model.init_args.learning_rate}) æ¥å¼•ç”¨å®ƒã€‚
                                   
            loss_configs (List[Dict]): 
                æ¥è‡ª YAML çš„æŸå¤±å‡½æ•°é…ç½®åˆ—è¡¨ã€‚
                ç¤ºä¾‹: 
                - class_path: point_suite.models.losses.focal_loss.FocalLoss
                  init_args: { gamma: 2.0 }
                  weight: 1.0 # (å¯é€‰) æŸå¤±çš„æƒé‡
                  
            metric_configs (List[Dict]): 
                æ¥è‡ª YAML çš„æŒ‡æ ‡é…ç½®åˆ—è¡¨ã€‚
                ç¤ºä¾‹:
                - class_path: pointsuite.utils.metrics.OverallAccuracy
                  init_args: { num_classes: 8 }
                  
            class_mapping (Dict[int, int]): 
                åŸå§‹ç±»åˆ«æ ‡ç­¾ -> è¿ç»­ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„
                ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
                æ­¤æ˜ å°„å°†è¢«ä¿å­˜åˆ° checkpointï¼Œå¹¶åœ¨é¢„æµ‹æ—¶è‡ªåŠ¨åŠ è½½åˆ° SemanticPredictLasWriter
                å¦‚æœä¸º Noneï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ç±»åˆ«æ˜ å°„
                
            class_names (List[str]): 
                ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨äºéªŒè¯æ—¶æ˜¾ç¤º
                ä¾‹å¦‚: ['Ground', 'Vegetation', 'Building']
                å¦‚æœä¸º Noneï¼ŒéªŒè¯æ—¶æ˜¾ç¤º Class 0, Class 1, ...
        """
        super().__init__()
        # å°†è¶…å‚æ•°ä¿å­˜åˆ° checkpoint
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¿å­˜æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬ loss_configs å’Œ metric_configs
        # è¿™æ · load_from_checkpoint æ‰èƒ½æ­£ç¡®é‡å»º Task
        self.save_hyperparameters()
        
        # ä¿å­˜ class_mapping ç”¨äº SemanticPredictLasWriter
        self.class_mapping = class_mapping
        
        # ğŸ”¥ è‡ªå®šä¹‰ hparams ä¿å­˜é’©å­ï¼Œç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
        self._custom_save_hparams()
        
        # --- 1. åŠ¨æ€å®ä¾‹åŒ–æŸå¤±å‡½æ•° ---
        self.losses = nn.ModuleDict()
        self.loss_weights = {}
        if loss_configs:
            for cfg in loss_configs:
                # 'loss_name' æ˜¯æˆ‘ä»¬ç»™è¿™ä¸ªæŸå¤±èµ·çš„åå­—ï¼Œä¾‹å¦‚ 'focal_loss'
                loss_name = cfg.get("name", cfg["class_path"].split('.')[-1].lower())
                loss_class = self._import_class(cfg["class_path"])
                init_args = cfg.get("init_args", {})
                
                self.losses[loss_name] = loss_class(**init_args)
                self.loss_weights[loss_name] = cfg.get("weight", 1.0)
                
        # --- 2. åŠ¨æ€å®ä¾‹åŒ–æŒ‡æ ‡ ---
        # æˆ‘ä»¬ä½¿ç”¨ ModuleDict æ¥ç¡®ä¿æŒ‡æ ‡è¢«æ­£ç¡®ç§»åŠ¨åˆ° GPU
        self.val_metrics = nn.ModuleDict()
        self.test_metrics = nn.ModuleDict()
        if metric_configs:
            for cfg in metric_configs:
                metric_name = cfg.get("name", cfg["class_path"].split('.')[-1].lower())
                metric_class = self._import_class(cfg["class_path"])
                init_args = cfg.get("init_args", {})
                
                # ä¸º val å’Œ test åˆ†åˆ«åˆ›å»ºå®ä¾‹ï¼Œä»¥é¿å…çŠ¶æ€å†²çª
                self.val_metrics[metric_name] = metric_class(**init_args)
                self.test_metrics[metric_name] = metric_class(**init_args)

    def configure_optimizers(self):
        """
        é»˜è®¤ä¼˜åŒ–å™¨é…ç½®ã€‚
        å­ç±»æˆ–ç”¨æˆ·å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•ä»¥ä½¿ç”¨è‡ªå®šä¹‰ä¼˜åŒ–å™¨ã€‚
        """
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.hparams.get('learning_rate', 1e-3), 
            weight_decay=1e-4
        )
        return optimizer
    
    def _custom_save_hparams(self):
        """
        è‡ªå®šä¹‰ä¿å­˜ hparams.yamlï¼Œç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£ç¡®æ˜¾ç¤º
        è¦†ç›– PyTorch Lightning é»˜è®¤çš„ YAML dump è¡Œä¸º
        """
        try:
            import os
            # è·å– log_dir
            if hasattr(self.logger, 'log_dir') and self.logger.log_dir:
                hparams_file = os.path.join(self.logger.log_dir, 'hparams.yaml')
                # å»¶è¿Ÿä¿å­˜ï¼šåœ¨ trainer å®Œæˆè®¾ç½®åå†ä¿å­˜
                # è¿™é‡Œåªæ˜¯æ ‡è®°ï¼Œå®é™…ä¿å­˜ä¼šåœ¨ on_train_start ä¸­è¿›è¡Œ
                self._pending_hparams_save = True
        except Exception:
            pass  # å¦‚æœå¤±è´¥å°±ä½¿ç”¨é»˜è®¤è¡Œä¸º
    
    def on_train_start(self):
        """è®­ç»ƒå¼€å§‹æ—¶ä¿å­˜ hparamsï¼ˆç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤ºï¼‰"""
        if hasattr(self, '_pending_hparams_save') and self._pending_hparams_save:
            try:
                import os
                if hasattr(self.logger, 'log_dir') and self.logger.log_dir:
                    hparams_file = os.path.join(self.logger.log_dir, 'hparams.yaml')
                    # ä½¿ç”¨ allow_unicode=True ç¡®ä¿ä¸­æ–‡æ­£ç¡®ä¿å­˜
                    with open(hparams_file, 'w', encoding='utf-8') as f:
                        yaml.dump(
                            dict(self.hparams), 
                            f, 
                            allow_unicode=True,  # ğŸ”¥ å…³é”®ï¼šå…è®¸ Unicode å­—ç¬¦
                            default_flow_style=False,
                            sort_keys=False
                        )
                self._pending_hparams_save = False
            except Exception as e:
                print(f"Warning: Could not save hparams with Chinese characters: {e}")

    def _import_class(self, class_path: str) -> type:
        """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºä»å­—ç¬¦ä¸²è·¯å¾„åŠ¨æ€å¯¼å…¥ç±»"""
        module_name, class_name = class_path.rsplit('.', 1)
        module = importlib.import_module(module_name)
        return getattr(module, class_name)

    def _calculate_total_loss(self, preds: Any, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """
        (å­ç±»å¯ä»¥è¦†ç›–)
        è®¡ç®—æ‰€æœ‰æŸå¤±å‡½æ•°çš„åŠ æƒæ€»å’Œã€‚
        
        æ³¨æ„ï¼šLoss è®¡ç®—å¼ºåˆ¶åœ¨ FP32 ä¸‹è¿è¡Œï¼Œä»¥é¿å…æ··åˆç²¾åº¦è®­ç»ƒä¸­çš„æ•°å€¼ä¸ç¨³å®šé—®é¢˜ï¼Œ
        ç‰¹åˆ«æ˜¯å½“ä½¿ç”¨ ignore_index=-1 æ—¶ã€‚
        
        Args:
            preds (Any): æ¨¡å‹çš„ forward() è¾“å‡ºã€‚
            batch (Dict): æ¥è‡ª DataLoader çš„æ‰¹æ¬¡æ•°æ®ã€‚
            
        Returns:
            Dict[str, torch.Tensor]: åŒ…å« 'total_loss' å’Œæ¯ä¸ªå•ç‹¬æŸå¤±çš„å­—å…¸ã€‚
        """
        loss_dict = {}
        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
        
        # å¼ºåˆ¶ Loss è®¡ç®—åœ¨ FP32 ä¸‹è¿è¡Œï¼Œé¿å…æ··åˆç²¾åº¦é—®é¢˜
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦åŒæ—¶ç¦ç”¨ autocast å¹¶å°† tensors è½¬ä¸º FP32
        with torch.amp.autocast('cuda', enabled=False):
            # å°† preds è½¬æ¢ä¸º FP32ï¼Œä½†ä¿ç•™æ¢¯åº¦
            if isinstance(preds, torch.Tensor):
                if preds.is_floating_point() and preds.dtype != torch.float32:
                    preds = preds.float()
            elif isinstance(preds, dict):
                preds_fp32 = {}
                for k, v in preds.items():
                    if isinstance(v, torch.Tensor) and v.is_floating_point() and v.dtype != torch.float32:
                        preds_fp32[k] = v.float()
                    else:
                        preds_fp32[k] = v
                preds = preds_fp32
            
            # åŒæ ·å¤„ç† batch ä¸­çš„ target (è™½ç„¶é€šå¸¸æ˜¯ long ç±»å‹)
            batch_fp32 = {}
            for k, v in batch.items():
                if isinstance(v, torch.Tensor) and v.is_floating_point() and v.dtype != torch.float32:
                    batch_fp32[k] = v.float()
                else:
                    batch_fp32[k] = v
            
            for name, loss_fn in self.losses.items():
                # æŸå¤±å‡½æ•°æ¥æ”¶ (preds, batch)
                loss = loss_fn(preds, batch_fp32)
                # ç¡®ä¿ loss ä¹Ÿæ˜¯ FP32
                if loss.dtype != torch.float32:
                    loss = loss.float()
                loss_dict[name] = loss
                total_loss += self.loss_weights[name] * loss
            
        loss_dict["total_loss"] = total_loss
        return loss_dict
    
    def _get_batch_size(self, batch: Dict[str, Any]) -> int:
        """
        ä» batch ä¸­æ¨æ–­ batch_sizeã€‚
        
        é€‚é…æˆ‘ä»¬é¡¹ç›®çš„ collate_fnï¼š
        - å¦‚æœæœ‰ 'batch_index'ï¼Œä½¿ç”¨ max + 1
        - å¦‚æœæœ‰ 'offset'ï¼Œä½¿ç”¨ len(offset)
        - å¦åˆ™è¿”å› 1
        """
        if 'batch_index' in batch:
            return batch['batch_index'].max().item() + 1
        elif 'offset' in batch:
            return len(batch['offset'])
        else:
            return 1
    
    def postprocess_predictions(self, preds: Any) -> torch.Tensor:
        """
        åå¤„ç†é¢„æµ‹ç»“æœï¼Œå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæ ‡ç­¾æˆ– logits
        
        è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„é’©å­æ–¹æ³•ï¼Œå­ç±»å¯ä»¥è¦†ç›–ä»¥æ”¯æŒå¤æ‚çš„è¾“å‡ºå¤„ç†ã€‚
        
        é»˜è®¤è¡Œä¸º:
        - å¦‚æœ preds æ˜¯å­—å…¸ä¸”åŒ…å« 'logits' é”®ï¼Œè¿”å› preds['logits']
        - å¦‚æœ preds æ˜¯å­—å…¸ä¸”åŒ…å« 'labels' é”®ï¼Œè¿”å› preds['labels']
        - å¦åˆ™å‡è®¾ preds å°±æ˜¯ logits/labelsï¼Œç›´æ¥è¿”å›
        
        ç”¨é€”:
        1. Mask3D: å¯ä»¥åœ¨è¿™é‡Œå®ç° class_logits @ mask_logits
        2. å¤šä»»åŠ¡æ¨¡å‹: å¯ä»¥æå–ç‰¹å®šä»»åŠ¡çš„è¾“å‡º
        3. åå¤„ç†: argmax, softmax, sigmoid ç­‰
        
        Args:
            preds: æ¨¡å‹çš„åŸå§‹è¾“å‡º (å¯ä»¥æ˜¯ Tensor, Dict, Tuple ç­‰)
            
        Returns:
            torch.Tensor: 
                - å¯¹äºéªŒè¯/æµ‹è¯•: è¿”å› logits [N, C] ç”¨äº metrics è®¡ç®—
                - å¯¹äºé¢„æµ‹: è¿”å› logits [N, C] æˆ– labels [N] ç”¨äºä¿å­˜
        
        Examples:
            >>> # ç¤ºä¾‹ 1: æ ‡å‡†è¯­ä¹‰åˆ†å‰² (é»˜è®¤)
            >>> def postprocess_predictions(self, preds):
            >>>     return preds  # ç›´æ¥è¿”å› logits
            
            >>> # ç¤ºä¾‹ 2: Mask3D
            >>> def postprocess_predictions(self, preds):
            >>>     class_logits = preds['class_logits']  # [N_queries, C]
            >>>     mask_logits = preds['mask_logits']    # [N_queries, N]
            >>>     point_logits = class_logits.T @ mask_logits  # [C, N]
            >>>     return point_logits.T  # [N, C]
            
            >>> # ç¤ºä¾‹ 3: ç›´æ¥è¿”å›æ ‡ç­¾
            >>> def postprocess_predictions(self, preds):
            >>>     if isinstance(preds, dict) and 'labels' in preds:
            >>>         return preds['labels']  # [N] - Metrics ä¼šè‡ªåŠ¨æ£€æµ‹
            >>>     return torch.argmax(preds, dim=-1)  # æ‰‹åŠ¨ argmax
        """
        # é»˜è®¤å®ç°ï¼šå¤„ç†å¸¸è§çš„å­—å…¸æ ¼å¼
        if isinstance(preds, dict):
            if 'logits' in preds:
                return preds['logits']
            elif 'labels' in preds:
                return preds['labels']
            elif 'pred' in preds:
                return preds['pred']
            else:
                # å¦‚æœæ˜¯å­—å…¸ä½†æ²¡æœ‰æ ‡å‡†é”®ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå€¼
                # è¿™å¯èƒ½éœ€è¦å­ç±»è¦†ç›–
                return next(iter(preds.values()))
        
        # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œå‡è®¾å°±æ˜¯ logits/labels
        return preds
    
    # --- è®­ç»ƒ (Training) é€»è¾‘ ---
    
    def training_step(self, batch: Dict[str, Any], batch_idx: int):
        """
        è®­ç»ƒæ­¥éª¤ã€‚
        """
        # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼ï¼ˆè§£å†³ eval mode è­¦å‘Šï¼‰
        if batch_idx == 0:
            self.train()
        
        # å‰å‘ä¼ æ’­
        try:
            preds = self(batch)
        except Exception as e:
            # ä¿å­˜é—®é¢˜æ•°æ®
            import pickle
            error_data_path = f'error_batch_{batch_idx}_step_{self.global_step}.pkl'
            with open(error_data_path, 'wb') as f:
                pickle.dump({
                    'batch_idx': batch_idx,
                    'global_step': self.global_step,
                    'batch': {k: v.cpu() if isinstance(v, torch.Tensor) else v 
                             for k, v in batch.items()},
                    'error': str(e)
                }, f)
            print(f"\n{'='*80}")
            print(f"[ERROR] åœ¨ batch_idx={batch_idx}, global_step={self.global_step} æ—¶å‘ç”Ÿé”™è¯¯")
            print(f"é—®é¢˜æ•°æ®å·²ä¿å­˜åˆ°: {error_data_path}")
            print(f"{'='*80}\n")
            raise
        
        # Loss è®¡ç®—
        loss_dict = self._calculate_total_loss(preds, batch)
        total_loss = loss_dict["total_loss"]
        
        # ä¿å­˜æœ€æ–°çš„ loss åˆ°æ¨¡å—ä¸­ï¼Œä¾› CustomProgressBar ç›´æ¥è¯»å–
        # é¿å… PL é»˜è®¤è¿›åº¦æ¡çš„å¹³æ»‘å¤„ç†å¯¼è‡´æ•°å€¼çœ‹èµ·æ¥"å¡æ­»"
        current_loss = total_loss.item()
        self.last_loss = current_loss
        # å¼ºåˆ¶æ›´æ–° trainer ä¸Šçš„å±æ€§ï¼Œç¡®ä¿ CustomProgressBar èƒ½è¯»å–åˆ°æœ€æ–°å€¼
        if self.trainer is not None:
            self.trainer.live_loss = current_loss
        
        # è®°å½•æŸå¤±
        batch_size = self._get_batch_size(batch)
        for name, loss_value in loss_dict.items():
            # æ‰€æœ‰æŸå¤±éƒ½æ˜¾ç¤ºåœ¨è¿›åº¦æ¡ï¼ŒåŒæ—¶è®°å½•åˆ° TensorBoard
            self.log(
                f"{name}_step",
                loss_value,
                on_step=True,
                on_epoch=False,
                prog_bar=True,
                batch_size=batch_size,
            )
        
        return total_loss

    def on_train_epoch_end(self):
        """
        åœ¨è®­ç»ƒ epoch ç»“æŸæ—¶è°ƒç”¨ï¼Œæ¸…ç†æ˜¾å­˜ä»¥ä¾¿éªŒè¯ã€‚
        """
        # å¼ºåˆ¶æ¸…ç† CUDA ç¼“å­˜ï¼Œé¿å…éªŒè¯æ—¶ OOM
        if torch.cuda.is_available():
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    def on_validation_start(self):
        """
        åœ¨éªŒè¯å¼€å§‹å‰å†æ¬¡æ¸…ç†æ˜¾å­˜ã€‚
        """
        if torch.cuda.is_available():
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    def validation_step(self, batch: Dict[str, Any], batch_idx: int):
        # 1. å‰å‘ä¼ æ’­
        preds = self.forward(batch)
        
        # 2. è®¡ç®—æŸå¤±
        loss_dict = self._calculate_total_loss(preds, batch)
        
        # 3. è®°å½•æŸå¤± (PL ä¼šè‡ªåŠ¨æ·»åŠ  'val/' å‰ç¼€)
        batch_size = self._get_batch_size(batch)
        self.log_dict(loss_dict, on_step=False, on_epoch=True, prog_bar=False, batch_size=batch_size)
        
        # 4. åå¤„ç†é¢„æµ‹ç»“æœ (æ”¯æŒ Mask3D ç­‰å¤æ‚è¾“å‡º)
        processed_preds = self.postprocess_predictions(preds)
        
        # 5. æ›´æ–°æŒ‡æ ‡
        # æå–ç›®æ ‡æ ‡ç­¾ (æ”¯æŒå¤šç§å‘½åçº¦å®š)
        target = batch.get('class', batch.get('label', batch.get('labels', batch.get('target'))))
        for metric in self.val_metrics.values():
            metric.update(processed_preds, target)

    def on_validation_epoch_end(self):
        """
        åœ¨éªŒè¯ epoch ç»“æŸæ—¶è®¡ç®—å¹¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
        
        å­ç±»å¯ä»¥è¦†ç›– _print_validation_metrics() æ¥è‡ªå®šä¹‰æ‰“å°æ ¼å¼
        """
        # åœ¨ epoch ç»“æŸæ—¶ï¼Œè®¡ç®—å¹¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
        metric_results = {}
        
        # ä¸´æ—¶å­˜å‚¨ç”¨äºæ‰“å°çš„æŒ‡æ ‡
        print_metrics = {}
        
        for name, metric in self.val_metrics.items():
            val = metric.compute()
            
            # å¤„ç†è¿”å›å­—å…¸çš„æŒ‡æ ‡ (å¦‚ SegmentationMetrics)
            if isinstance(val, dict):
                # è®°å½•åˆ° metric_results (ç”¨äº log)
                for k, v in val.items():
                    # è¿‡æ»¤æ‰éæ ‡é‡å€¼ (å¦‚ class_names åˆ—è¡¨)
                    if isinstance(v, (torch.Tensor, float, int)):
                        # ç¡®ä¿ tensor æ˜¯æ ‡é‡
                        if isinstance(v, torch.Tensor) and v.numel() > 1:
                            continue
                        metric_results[k] = v
                
                # ä¿å­˜å®Œæ•´ç»“æœç”¨äºæ‰“å°
                print_metrics.update(val)
                
            else:
                metric_results[name] = val
                print_metrics[name] = val
            
            metric.reset() # é‡ç½®æŒ‡æ ‡çŠ¶æ€
        
        # è®°å½•æŒ‡æ ‡ (prog_bar=False ä»¥é¿å…æ±¡æŸ“è¿›åº¦æ¡)
        self.log_dict(metric_results, on_step=False, on_epoch=True, prog_bar=False)
        
        # è°ƒç”¨é’©å­æ–¹æ³•æ¥æ‰“å°è¯¦ç»†æŒ‡æ ‡ï¼ˆå­ç±»å¯è¦†ç›–ï¼‰
        self._print_validation_metrics(print_metrics)
    
    def _print_validation_metrics(self, print_metrics: Dict[str, Any]):
        """
        æ‰“å°éªŒè¯æŒ‡æ ‡çš„é’©å­æ–¹æ³•
        
        é»˜è®¤å®ç°åªæ‰“å°åŸºæœ¬æŒ‡æ ‡ã€‚
        è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•æ¥æ‰“å°è¯¦ç»†çš„æ¯ç±»æŒ‡æ ‡ã€‚
        
        Args:
            print_metrics: åŒ…å«æ‰€æœ‰è®¡ç®—å‡ºçš„æŒ‡æ ‡çš„å­—å…¸
        """
        # é»˜è®¤ï¼šåªæ‰“å°ç®€å•çš„æ‘˜è¦
        display_epoch = self.current_epoch + 1
        print(f"\n{'='*60}")
        print(f"Validation Epoch {display_epoch} - Metrics")
        print(f"{'='*60}")
        
        for name, value in print_metrics.items():
            if isinstance(value, (float, int)):
                print(f"  {name}: {value:.4f}")
            elif isinstance(value, torch.Tensor) and value.numel() == 1:
                print(f"  {name}: {value.item():.4f}")
        
        print(f"{'='*60}\n")

    # --- æµ‹è¯• (Test) é€»è¾‘ ---
    
    def test_step(self, batch: Dict[str, Any], batch_idx: int):
        """
        æµ‹è¯•æ­¥éª¤ï¼šè®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡
        """
        # é€»è¾‘ä¸ validation_step ç›¸åŒ
        preds = self.forward(batch)
        loss_dict = self._calculate_total_loss(preds, batch)
        batch_size = self._get_batch_size(batch)
        self.log_dict(loss_dict, on_step=False, on_epoch=True, batch_size=batch_size)
        
        # åå¤„ç†é¢„æµ‹ç»“æœ
        processed_preds = self.postprocess_predictions(preds)
        
        # æå–ç›®æ ‡æ ‡ç­¾ (æ”¯æŒå¤šç§å‘½åçº¦å®š)
        target = batch.get('class', batch.get('label', batch.get('labels', batch.get('target'))))
        for metric in self.test_metrics.values():
            metric.update(processed_preds, target)

    def on_test_epoch_end(self):
        """
        åœ¨æµ‹è¯• epoch ç»“æŸæ—¶è®¡ç®—å¹¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
        
        å­ç±»å¯ä»¥è¦†ç›– _print_test_metrics() æ¥è‡ªå®šä¹‰æ‰“å°æ ¼å¼
        """
        # åœ¨ epoch ç»“æŸæ—¶ï¼Œè®¡ç®—å¹¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
        metric_results = {}
        print_metrics = {}
        
        for name, metric in self.test_metrics.items():
            val = metric.compute()
            
            if isinstance(val, dict):
                for k, v in val.items():
                    if isinstance(v, (torch.Tensor, float, int)):
                        # ç¡®ä¿ tensor æ˜¯æ ‡é‡
                        if isinstance(v, torch.Tensor) and v.numel() > 1:
                            continue
                        metric_results[k] = v
                
                print_metrics.update(val)
            else:
                metric_results[name] = val
                print_metrics[name] = val
            
            metric.reset()
            
        self.log_dict(metric_results, on_step=False, on_epoch=True)
        
        # è°ƒç”¨é’©å­æ–¹æ³•æ¥æ‰“å°è¯¦ç»†æŒ‡æ ‡ï¼ˆå­ç±»å¯è¦†ç›–ï¼‰
        self._print_test_metrics(print_metrics)
    
    def _print_test_metrics(self, print_metrics: Dict[str, Any]):
        """
        æ‰“å°æµ‹è¯•æŒ‡æ ‡çš„é’©å­æ–¹æ³•
        
        é»˜è®¤å®ç°åªæ‰“å°åŸºæœ¬æŒ‡æ ‡ã€‚
        è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•æ¥æ‰“å°è¯¦ç»†çš„æ¯ç±»æŒ‡æ ‡ã€‚
        
        Args:
            print_metrics: åŒ…å«æ‰€æœ‰è®¡ç®—å‡ºçš„æŒ‡æ ‡çš„å­—å…¸
        """
        # é»˜è®¤ï¼šåªæ‰“å°ç®€å•çš„æ‘˜è¦
        print(f"\n{'='*60}")
        print(f"Test Results - Metrics")
        print(f"{'='*60}")
        
        for name, value in print_metrics.items():
            if isinstance(value, (float, int)):
                print(f"  {name}: {value:.4f}")
            elif isinstance(value, torch.Tensor) and value.numel() == 1:
                print(f"  {name}: {value.item():.4f}")
        
        print(f"{'='*60}\n")