import torch
import torch.nn as nn
import pytorch_lightning as pl
import torchmetrics
import importlib
import yaml
from typing import List, Dict, Any

class BaseTask(pl.LightningModule):
    """
    ä¸€ä¸ªæŠ½è±¡çš„ä»»åŠ¡åŸºç±» (LightningModule)ã€‚
    
    å®ƒè´Ÿè´£å¤„ç†æ‰€æœ‰ä»»åŠ¡å…±æœ‰çš„é€»è¾‘ï¼š
    1. è‡ªåŠ¨ä» YAML é…ç½®ä¸­å®ä¾‹åŒ–æŸå¤±å‡½æ•° (losses)ã€‚
    2. è‡ªåŠ¨ä» YAML é…ç½®ä¸­å®ä¾‹åŒ–æŒ‡æ ‡ (metrics)ã€‚
    3. è‡ªåŠ¨åœ¨ validation/test epoch ç»“æŸæ—¶è®¡ç®—å’Œè®°å½•æ‰€æœ‰æŒ‡æ ‡ã€‚
    
    æ³¨æ„: 
    æˆ‘ä»¬ *ä¸* åœ¨è¿™é‡Œå®ç° `configure_optimizers`ã€‚
    PyTorch Lightning çš„ `LightningCLI` ä¼šè‡ªåŠ¨è¯»å–æ‚¨åœ¨
    `configs/schedules/` ç›®å½•ä¸­å®šä¹‰çš„ `optimizer` å’Œ `lr_scheduler` 
    é…ç½®ï¼Œå¹¶è‡ªåŠ¨ä¸ºæ‚¨é…ç½®å®ƒä»¬ã€‚è¿™ä¿æŒäº† Task æ¨¡å—çš„ç®€æ´ã€‚
    """
    
    def __init__(self,
                 learning_rate: float = 1e-3,
                 loss_configs: List[Dict[str, Any]] = None,
                 metric_configs: List[Dict[str, Any]] = None,
                 class_mapping: Dict[int, int] = None,
                 class_names: List[str] = None):
        """
        Args:
            learning_rate (float): å­¦ä¹ ç‡ã€‚
                                   æˆ‘ä»¬åœ¨æ­¤å¤„æ¥æ”¶ learning_rate (è€Œä¸æ˜¯ä»…åœ¨ä¼˜åŒ–å™¨é…ç½®ä¸­)
                                   ä¸»è¦æœ‰ä¸¤ä¸ªåŸå› :
                                   1. æ—¥å¿—è®°å½•: 'self.save_hyperparameters()' ä¼šè‡ªåŠ¨
                                      å°† 'learning_rate' è®°å½•åˆ° TensorBoard/Wandbã€‚
                                   2. çµæ´»æ€§: å…è®¸åœ¨ä¸ä½¿ç”¨ 'LightningCLI' çš„çº¯ Python æ¨¡å¼ä¸‹
                                      è½»æ¾è®¿é—® 'self.hparams.learning_rate' æ¥é…ç½®ä¼˜åŒ–å™¨ã€‚
                                   
                                   åœ¨ YAML é…ç½®ä¸­ï¼Œæˆ‘ä»¬åº”å°†æ­¤ 'learning_rate' è§†ä¸ºâ€œå•ä¸€äº‹å®æ¥æºâ€ï¼Œ
                                   å¹¶åœ¨ 'optimizer' é…ç½®ä¸­ä½¿ç”¨ YAML é“¾æ¥ (ä¾‹å¦‚:
                                   lr: ${model.init_args.learning_rate}) æ¥å¼•ç”¨å®ƒã€‚
                                   
            loss_configs (List[Dict]): 
                æ¥è‡ª YAML çš„æŸå¤±å‡½æ•°é…ç½®åˆ—è¡¨ã€‚
                ç¤ºä¾‹: 
                - class_path: point_suite.models.losses.focal_loss.FocalLoss
                  init_args: { gamma: 2.0 }
                  weight: 1.0 # (å¯é€‰) æŸå¤±çš„æƒé‡
                  
            metric_configs (List[Dict]): 
                æ¥è‡ª YAML çš„æŒ‡æ ‡é…ç½®åˆ—è¡¨ã€‚
                ç¤ºä¾‹:
                - class_path: pointsuite.utils.metrics.OverallAccuracy
                  init_args: { num_classes: 8 }
                  
            class_mapping (Dict[int, int]): 
                åŸå§‹ç±»åˆ«æ ‡ç­¾ -> è¿ç»­ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„
                ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
                æ­¤æ˜ å°„å°†è¢«ä¿å­˜åˆ° checkpointï¼Œå¹¶åœ¨é¢„æµ‹æ—¶è‡ªåŠ¨åŠ è½½åˆ° SegmentationWriter
                å¦‚æœä¸º Noneï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ç±»åˆ«æ˜ å°„
                
            class_names (List[str]): 
                ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨äºéªŒè¯æ—¶æ˜¾ç¤º
                ä¾‹å¦‚: ['Ground', 'Vegetation', 'Building']
                å¦‚æœä¸º Noneï¼ŒéªŒè¯æ—¶æ˜¾ç¤º Class 0, Class 1, ...
        """
        super().__init__()
        # å°†è¶…å‚æ•°ä¿å­˜åˆ° checkpoint
        self.save_hyperparameters("learning_rate", "class_mapping", "class_names")
        
        # ä¿å­˜ class_mapping ç”¨äº SegmentationWriter
        self.class_mapping = class_mapping
        
        # è¿½è¸ªæœ€ä½³ mIoU
        self.best_miou = 0.0
        self.best_miou_epoch = -1
        
        # ğŸ”¥ è‡ªå®šä¹‰ hparams ä¿å­˜é’©å­ï¼Œç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
        self._custom_save_hparams()
        
        # --- 1. åŠ¨æ€å®ä¾‹åŒ–æŸå¤±å‡½æ•° ---
        self.losses = nn.ModuleDict()
        self.loss_weights = {}
        if loss_configs:
            for cfg in loss_configs:
                # 'loss_name' æ˜¯æˆ‘ä»¬ç»™è¿™ä¸ªæŸå¤±èµ·çš„åå­—ï¼Œä¾‹å¦‚ 'focal_loss'
                loss_name = cfg.get("name", cfg["class_path"].split('.')[-1].lower())
                loss_class = self._import_class(cfg["class_path"])
                init_args = cfg.get("init_args", {})
                
                self.losses[loss_name] = loss_class(**init_args)
                self.loss_weights[loss_name] = cfg.get("weight", 1.0)
                
        # --- 2. åŠ¨æ€å®ä¾‹åŒ–æŒ‡æ ‡ ---
        # æˆ‘ä»¬ä½¿ç”¨ ModuleDict æ¥ç¡®ä¿æŒ‡æ ‡è¢«æ­£ç¡®ç§»åŠ¨åˆ° GPU
        self.val_metrics = nn.ModuleDict()
        self.test_metrics = nn.ModuleDict()
        if metric_configs:
            for cfg in metric_configs:
                metric_name = cfg.get("name", cfg["class_path"].split('.')[-1].lower())
                metric_class = self._import_class(cfg["class_path"])
                init_args = cfg.get("init_args", {})
                
                # ä¸º val å’Œ test åˆ†åˆ«åˆ›å»ºå®ä¾‹ï¼Œä»¥é¿å…çŠ¶æ€å†²çª
                self.val_metrics[metric_name] = metric_class(**init_args)
                self.test_metrics[metric_name] = metric_class(**init_args)
    
    def _custom_save_hparams(self):
        """
        è‡ªå®šä¹‰ä¿å­˜ hparams.yamlï¼Œç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£ç¡®æ˜¾ç¤º
        è¦†ç›– PyTorch Lightning é»˜è®¤çš„ YAML dump è¡Œä¸º
        """
        try:
            import os
            # è·å– log_dir
            if hasattr(self.logger, 'log_dir') and self.logger.log_dir:
                hparams_file = os.path.join(self.logger.log_dir, 'hparams.yaml')
                # å»¶è¿Ÿä¿å­˜ï¼šåœ¨ trainer å®Œæˆè®¾ç½®åå†ä¿å­˜
                # è¿™é‡Œåªæ˜¯æ ‡è®°ï¼Œå®é™…ä¿å­˜ä¼šåœ¨ on_train_start ä¸­è¿›è¡Œ
                self._pending_hparams_save = True
        except Exception:
            pass  # å¦‚æœå¤±è´¥å°±ä½¿ç”¨é»˜è®¤è¡Œä¸º
    
    def on_train_start(self):
        """è®­ç»ƒå¼€å§‹æ—¶ä¿å­˜ hparamsï¼ˆç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤ºï¼‰"""
        if hasattr(self, '_pending_hparams_save') and self._pending_hparams_save:
            try:
                import os
                if hasattr(self.logger, 'log_dir') and self.logger.log_dir:
                    hparams_file = os.path.join(self.logger.log_dir, 'hparams.yaml')
                    # ä½¿ç”¨ allow_unicode=True ç¡®ä¿ä¸­æ–‡æ­£ç¡®ä¿å­˜
                    with open(hparams_file, 'w', encoding='utf-8') as f:
                        yaml.dump(
                            dict(self.hparams), 
                            f, 
                            allow_unicode=True,  # ğŸ”¥ å…³é”®ï¼šå…è®¸ Unicode å­—ç¬¦
                            default_flow_style=False,
                            sort_keys=False
                        )
                self._pending_hparams_save = False
            except Exception as e:
                print(f"Warning: Could not save hparams with Chinese characters: {e}")

    def _import_class(self, class_path: str) -> type:
        """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºä»å­—ç¬¦ä¸²è·¯å¾„åŠ¨æ€å¯¼å…¥ç±»"""
        module_name, class_name = class_path.rsplit('.', 1)
        module = importlib.import_module(module_name)
        return getattr(module, class_name)

    def _calculate_total_loss(self, preds: Any, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """
        (å­ç±»å¯ä»¥è¦†ç›–)
        è®¡ç®—æ‰€æœ‰æŸå¤±å‡½æ•°çš„åŠ æƒæ€»å’Œã€‚
        
        æ³¨æ„ï¼šLoss è®¡ç®—å¼ºåˆ¶åœ¨ FP32 ä¸‹è¿è¡Œï¼Œä»¥é¿å…æ··åˆç²¾åº¦è®­ç»ƒä¸­çš„æ•°å€¼ä¸ç¨³å®šé—®é¢˜ï¼Œ
        ç‰¹åˆ«æ˜¯å½“ä½¿ç”¨ ignore_index=-1 æ—¶ã€‚
        
        Args:
            preds (Any): æ¨¡å‹çš„ forward() è¾“å‡ºã€‚
            batch (Dict): æ¥è‡ª DataLoader çš„æ‰¹æ¬¡æ•°æ®ã€‚
            
        Returns:
            Dict[str, torch.Tensor]: åŒ…å« 'total_loss' å’Œæ¯ä¸ªå•ç‹¬æŸå¤±çš„å­—å…¸ã€‚
        """
        loss_dict = {}
        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float32)
        
        # å¼ºåˆ¶ Loss è®¡ç®—åœ¨ FP32 ä¸‹è¿è¡Œï¼Œé¿å…æ··åˆç²¾åº¦é—®é¢˜
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦åŒæ—¶ç¦ç”¨ autocast å¹¶å°† tensors è½¬ä¸º FP32
        with torch.amp.autocast('cuda', enabled=False):
            # å°† preds è½¬æ¢ä¸º FP32ï¼Œä½†ä¿ç•™æ¢¯åº¦
            if isinstance(preds, torch.Tensor):
                if preds.is_floating_point() and preds.dtype != torch.float32:
                    preds = preds.float()
            elif isinstance(preds, dict):
                preds_fp32 = {}
                for k, v in preds.items():
                    if isinstance(v, torch.Tensor) and v.is_floating_point() and v.dtype != torch.float32:
                        preds_fp32[k] = v.float()
                    else:
                        preds_fp32[k] = v
                preds = preds_fp32
            
            # åŒæ ·å¤„ç† batch ä¸­çš„ target (è™½ç„¶é€šå¸¸æ˜¯ long ç±»å‹)
            batch_fp32 = {}
            for k, v in batch.items():
                if isinstance(v, torch.Tensor) and v.is_floating_point() and v.dtype != torch.float32:
                    batch_fp32[k] = v.float()
                else:
                    batch_fp32[k] = v
            
            for name, loss_fn in self.losses.items():
                # æŸå¤±å‡½æ•°æ¥æ”¶ (preds, batch)
                loss = loss_fn(preds, batch_fp32)
                # ç¡®ä¿ loss ä¹Ÿæ˜¯ FP32
                if loss.dtype != torch.float32:
                    loss = loss.float()
                loss_dict[name] = loss
                total_loss += self.loss_weights[name] * loss
            
        loss_dict["total_loss"] = total_loss
        return loss_dict
    
    def _get_batch_size(self, batch: Dict[str, Any]) -> int:
        """
        ä» batch ä¸­æ¨æ–­ batch_sizeã€‚
        
        é€‚é…æˆ‘ä»¬é¡¹ç›®çš„ collate_fnï¼š
        - å¦‚æœæœ‰ 'batch_index'ï¼Œä½¿ç”¨ max + 1
        - å¦‚æœæœ‰ 'offset'ï¼Œä½¿ç”¨ len(offset)
        - å¦åˆ™è¿”å› 1
        """
        if 'batch_index' in batch:
            return batch['batch_index'].max().item() + 1
        elif 'offset' in batch:
            return len(batch['offset'])
        else:
            return 1
    
    def postprocess_predictions(self, preds: Any) -> torch.Tensor:
        """
        åå¤„ç†é¢„æµ‹ç»“æœï¼Œå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæ ‡ç­¾æˆ– logits
        
        è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„é’©å­æ–¹æ³•ï¼Œå­ç±»å¯ä»¥è¦†ç›–ä»¥æ”¯æŒå¤æ‚çš„è¾“å‡ºå¤„ç†ã€‚
        
        é»˜è®¤è¡Œä¸º:
        - å¦‚æœ preds æ˜¯å­—å…¸ä¸”åŒ…å« 'logits' é”®ï¼Œè¿”å› preds['logits']
        - å¦‚æœ preds æ˜¯å­—å…¸ä¸”åŒ…å« 'labels' é”®ï¼Œè¿”å› preds['labels']
        - å¦åˆ™å‡è®¾ preds å°±æ˜¯ logits/labelsï¼Œç›´æ¥è¿”å›
        
        ç”¨é€”:
        1. Mask3D: å¯ä»¥åœ¨è¿™é‡Œå®ç° class_logits @ mask_logits
        2. å¤šä»»åŠ¡æ¨¡å‹: å¯ä»¥æå–ç‰¹å®šä»»åŠ¡çš„è¾“å‡º
        3. åå¤„ç†: argmax, softmax, sigmoid ç­‰
        
        Args:
            preds: æ¨¡å‹çš„åŸå§‹è¾“å‡º (å¯ä»¥æ˜¯ Tensor, Dict, Tuple ç­‰)
            
        Returns:
            torch.Tensor: 
                - å¯¹äºéªŒè¯/æµ‹è¯•: è¿”å› logits [N, C] ç”¨äº metrics è®¡ç®—
                - å¯¹äºé¢„æµ‹: è¿”å› logits [N, C] æˆ– labels [N] ç”¨äºä¿å­˜
        
        Examples:
            >>> # ç¤ºä¾‹ 1: æ ‡å‡†è¯­ä¹‰åˆ†å‰² (é»˜è®¤)
            >>> def postprocess_predictions(self, preds):
            >>>     return preds  # ç›´æ¥è¿”å› logits
            
            >>> # ç¤ºä¾‹ 2: Mask3D
            >>> def postprocess_predictions(self, preds):
            >>>     class_logits = preds['class_logits']  # [N_queries, C]
            >>>     mask_logits = preds['mask_logits']    # [N_queries, N]
            >>>     point_logits = class_logits.T @ mask_logits  # [C, N]
            >>>     return point_logits.T  # [N, C]
            
            >>> # ç¤ºä¾‹ 3: ç›´æ¥è¿”å›æ ‡ç­¾
            >>> def postprocess_predictions(self, preds):
            >>>     if isinstance(preds, dict) and 'labels' in preds:
            >>>         return preds['labels']  # [N] - Metrics ä¼šè‡ªåŠ¨æ£€æµ‹
            >>>     return torch.argmax(preds, dim=-1)  # æ‰‹åŠ¨ argmax
        """
        # é»˜è®¤å®ç°ï¼šå¤„ç†å¸¸è§çš„å­—å…¸æ ¼å¼
        if isinstance(preds, dict):
            if 'logits' in preds:
                return preds['logits']
            elif 'labels' in preds:
                return preds['labels']
            elif 'pred' in preds:
                return preds['pred']
            else:
                # å¦‚æœæ˜¯å­—å…¸ä½†æ²¡æœ‰æ ‡å‡†é”®ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå€¼
                # è¿™å¯èƒ½éœ€è¦å­ç±»è¦†ç›–
                return next(iter(preds.values()))
        
        # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œå‡è®¾å°±æ˜¯ logits/labels
        return preds
    
    # --- è®­ç»ƒ (Training) é€»è¾‘ ---
    
    def training_step(self, batch: Dict[str, Any], batch_idx: int):
        """
        è®­ç»ƒæ­¥éª¤ã€‚
        """
        # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼ï¼ˆè§£å†³ eval mode è­¦å‘Šï¼‰
        if batch_idx == 0:
            self.train()
        
        # è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•è®­ç»ƒæ­¥æ•°å’Œæ•°æ®ç»Ÿè®¡
        if batch_idx % 50 == 0 or batch_idx > 160:  # åœ¨é—®é¢˜åŒºåŸŸé™„è¿‘æ›´é¢‘ç¹è®°å½•
            coord = batch.get('coord', None)
            if coord is not None:
                print(f"\n[Step {self.global_step}] Batch {batch_idx}: "
                      f"points={len(coord)}, "
                      f"coord_range=[{coord.min(0)[0]}, {coord.max(0)[0]}]")
        
        # å‰å‘ä¼ æ’­
        try:
            preds = self(batch)
        except Exception as e:
            # ä¿å­˜é—®é¢˜æ•°æ®
            import pickle
            error_data_path = f'error_batch_{batch_idx}_step_{self.global_step}.pkl'
            with open(error_data_path, 'wb') as f:
                pickle.dump({
                    'batch_idx': batch_idx,
                    'global_step': self.global_step,
                    'batch': {k: v.cpu() if isinstance(v, torch.Tensor) else v 
                             for k, v in batch.items()},
                    'error': str(e)
                }, f)
            print(f"\n{'='*80}")
            print(f"[ERROR] åœ¨ batch_idx={batch_idx}, global_step={self.global_step} æ—¶å‘ç”Ÿé”™è¯¯")
            print(f"é—®é¢˜æ•°æ®å·²ä¿å­˜åˆ°: {error_data_path}")
            print(f"{'='*80}\n")
            raise
        
        # Loss è®¡ç®—
        loss_dict = self._calculate_total_loss(preds, batch)
        total_loss = loss_dict["total_loss"]
        
        # è®°å½•æŸå¤±
        batch_size = self._get_batch_size(batch)
        for name, loss_value in loss_dict.items():
            # æ‰€æœ‰æŸå¤±éƒ½æ˜¾ç¤ºåœ¨è¿›åº¦æ¡ï¼ŒåŒæ—¶è®°å½•åˆ° TensorBoard
            self.log(
                f"{name}_step",
                loss_value,
                on_step=True,
                on_epoch=False,
                prog_bar=True,
                batch_size=batch_size,
            )
        
        return total_loss

    def on_train_epoch_end(self):
        """
        åœ¨è®­ç»ƒ epoch ç»“æŸæ—¶è°ƒç”¨ï¼Œæ¸…ç†æ˜¾å­˜ä»¥ä¾¿éªŒè¯ã€‚
        """
        # å¼ºåˆ¶æ¸…ç† CUDA ç¼“å­˜ï¼Œé¿å…éªŒè¯æ—¶ OOM
        if torch.cuda.is_available():
            import gc
            gc.collect()  # Python åƒåœ¾å›æ”¶
            torch.cuda.empty_cache()  # CUDA ç¼“å­˜æ¸…ç†
            torch.cuda.synchronize()  # åŒæ­¥ CUDA æ“ä½œ
            # è¾“å‡ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"\n[Memory] Epoch {self.current_epoch} training end: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")
            print(f"[Memory] Cleared cache and GC, starting validation...\n")
    
    def on_validation_start(self):
        """
        åœ¨éªŒè¯å¼€å§‹å‰å†æ¬¡æ¸…ç†æ˜¾å­˜ã€‚
        """
        if torch.cuda.is_available():
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"[Memory] Validation start: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB\n")
    
    def validation_step(self, batch: Dict[str, Any], batch_idx: int):
        # 1. å‰å‘ä¼ æ’­
        preds = self.forward(batch)
        
        # 2. è®¡ç®—æŸå¤±
        loss_dict = self._calculate_total_loss(preds, batch)
        
        # 3. è®°å½•æŸå¤± (PL ä¼šè‡ªåŠ¨æ·»åŠ  'val/' å‰ç¼€)
        batch_size = self._get_batch_size(batch)
        self.log_dict(loss_dict, on_step=False, on_epoch=True, prog_bar=False, batch_size=batch_size)
        
        # 4. åå¤„ç†é¢„æµ‹ç»“æœ (æ”¯æŒ Mask3D ç­‰å¤æ‚è¾“å‡º)
        processed_preds = self.postprocess_predictions(preds)
        
        # 5. æ›´æ–°æŒ‡æ ‡
        # æå–ç›®æ ‡æ ‡ç­¾ (æ”¯æŒå¤šç§å‘½åçº¦å®š)
        target = batch.get('class', batch.get('label', batch.get('labels', batch.get('target'))))
        for metric in self.val_metrics.values():
            metric.update(processed_preds, target)

    def on_validation_epoch_end(self):
        # 5. åœ¨ epoch ç»“æŸæ—¶ï¼Œè®¡ç®—å¹¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
        metric_results = {}
        for name, metric in self.val_metrics.items():
            metric_results[name] = metric.compute()
            
            # å¦‚æœæ˜¯ JaccardIndex (IoU)ï¼Œè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
            if name == 'mean_iou':
                try:
                    # è·å–æ··æ·†çŸ©é˜µ
                    if hasattr(metric, 'confmat'):
                        confmat = metric.confmat.cpu().numpy() if hasattr(metric.confmat, 'cpu') else metric.confmat
                    elif hasattr(metric, 'confusion_matrix'):
                        confmat = metric.confusion_matrix.cpu().numpy()
                    else:
                        confmat = None
                    
                    if confmat is not None:
                        # è®¡ç®—æ¯ç±» IoUã€Precisionã€Recallã€F1
                        import numpy as np
                        intersection = np.diag(confmat)
                        union = confmat.sum(1) + confmat.sum(0) - np.diag(confmat)
                        per_class_iou = intersection / (union + 1e-10)
                        
                        # Precision = TP / (TP + FP) = diag / col_sum
                        per_class_precision = intersection / (confmat.sum(0) + 1e-10)
                        # Recall = TP / (TP + FN) = diag / row_sum
                        per_class_recall = intersection / (confmat.sum(1) + 1e-10)
                        # F1 = 2 * P * R / (P + R)
                        per_class_f1 = 2 * per_class_precision * per_class_recall / (per_class_precision + per_class_recall + 1e-10)
                        
                        # æ›´æ–°æœ€ä½³ mIoU
                        current_miou = float(metric_results[name])
                        if current_miou > self.best_miou:
                            self.best_miou = current_miou
                            self.best_miou_epoch = self.current_epoch
                        
                        # è·å– Overall Accuracy
                        overall_acc = metric_results.get('overall_accuracy', None)
                        
                        # è¾“å‡ºæ ‡é¢˜å’Œæ€»ä½“æŒ‡æ ‡
                        print(f"\n{'='*100}")
                        print(f"Validation Epoch {self.current_epoch} - Per-Class Metrics")
                        print(f"{'='*100}")
                        if overall_acc is not None:
                            print(f"Overall Accuracy: {overall_acc:.4f} ({overall_acc*100:.2f}%)")
                        print(f"Mean IoU (current): {current_miou:.4f}")
                        print(f"Mean IoU (best)   : {self.best_miou:.4f} (Epoch {self.best_miou_epoch})")
                        if current_miou > self.best_miou - 1e-6:  # å½“å‰æ˜¯æœ€ä½³
                            print(f"ğŸ‰ New best mIoU achieved!")
                        print(f"{'='*100}")
                        
                        # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
                        print(f"  {'Class':15s}  {'IoU':>8s}  {'Precision':>10s}  {'Recall':>8s}  {'F1-Score':>10s}")
                        print(f"  {'-'*15}  {'-'*8}  {'-'*10}  {'-'*8}  {'-'*10}")
                        
                        # ä» hparams è·å–ç±»åˆ«å
                        class_names = self.hparams.get('class_names', None) if hasattr(self, 'hparams') else None
                        if class_names:
                            for i, class_name in enumerate(class_names):
                                print(f"  {class_name:15s}  {per_class_iou[i]:8.4f}  {per_class_precision[i]:10.4f}  "
                                      f"{per_class_recall[i]:8.4f}  {per_class_f1[i]:10.4f}")
                        else:
                            for i in range(len(per_class_iou)):
                                print(f"  Class {i:2d}        {per_class_iou[i]:8.4f}  {per_class_precision[i]:10.4f}  "
                                      f"{per_class_recall[i]:8.4f}  {per_class_f1[i]:10.4f}")
                        
                        # è®¡ç®—å¹³å‡æŒ‡æ ‡
                        mean_precision = np.nanmean(per_class_precision)
                        mean_recall = np.nanmean(per_class_recall)
                        mean_f1 = np.nanmean(per_class_f1)
                        
                        print(f"  {'-'*15}  {'-'*8}  {'-'*10}  {'-'*8}  {'-'*10}")
                        print(f"  {'Mean':15s}  {current_miou:8.4f}  {mean_precision:10.4f}  "
                              f"{mean_recall:8.4f}  {mean_f1:10.4f}")
                        print(f"{'='*100}\n")
                except Exception as e:
                    print(f"Warning: Could not compute per-class IoU: {e}")
            
            metric.reset() # é‡ç½®æŒ‡æ ‡çŠ¶æ€
        
        self.log_dict(metric_results, on_step=False, on_epoch=True, prog_bar=True)

    # --- æµ‹è¯• (Test) é€»è¾‘ ---
    
    def test_step(self, batch: Dict[str, Any], batch_idx: int):
        """
        æµ‹è¯•æ­¥éª¤ï¼šè®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡
        
        æ³¨æ„ï¼š
        - ä¸ validation ç±»ä¼¼ï¼Œä½†å¯ä»¥é€šè¿‡å›è°ƒï¼ˆå¦‚ SegmentationWriterï¼‰ä¿å­˜é¢„æµ‹ç»“æœ
        - å¦‚æœéœ€è¦ä¿å­˜é¢„æµ‹ç»“æœï¼Œåº”è¯¥ä½¿ç”¨ Trainer.test() å¹¶é…ç½®å›è°ƒ
        - å¦‚æœä¸éœ€è¦ä¿å­˜ç»“æœï¼Œåªæ˜¯è¯„ä¼°æŒ‡æ ‡ï¼Œä½¿ç”¨ Trainer.validate()
        """
        # é€»è¾‘ä¸ validation_step ç›¸åŒ
        preds = self.forward(batch)
        loss_dict = self._calculate_total_loss(preds, batch)
        batch_size = self._get_batch_size(batch)
        self.log_dict(loss_dict, on_step=False, on_epoch=True, batch_size=batch_size)
        
        # åå¤„ç†é¢„æµ‹ç»“æœ
        processed_preds = self.postprocess_predictions(preds)
        
        # æå–ç›®æ ‡æ ‡ç­¾ (æ”¯æŒå¤šç§å‘½åçº¦å®š)
        target = batch.get('class', batch.get('label', batch.get('labels', batch.get('target'))))
        for metric in self.test_metrics.values():
            metric.update(processed_preds, target)

    def on_test_epoch_end(self):
        # åœ¨ epoch ç»“æŸæ—¶ï¼Œè®¡ç®—å¹¶è®°å½•æ‰€æœ‰æŒ‡æ ‡
        metric_results = {}
        for name, metric in self.test_metrics.items():
            metric_results[name] = metric.compute()
            
            # å¦‚æœæ˜¯ JaccardIndex (IoU)ï¼Œè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
            if name == 'mean_iou':
                try:
                    # è·å–æ··æ·†çŸ©é˜µ
                    if hasattr(metric, 'confmat'):
                        confmat = metric.confmat.cpu().numpy() if hasattr(metric.confmat, 'cpu') else metric.confmat
                    elif hasattr(metric, 'confusion_matrix'):
                        confmat = metric.confusion_matrix.cpu().numpy()
                    else:
                        confmat = None
                    
                    if confmat is not None:
                        # è®¡ç®—æ¯ç±» IoUã€Precisionã€Recallã€F1
                        import numpy as np
                        intersection = np.diag(confmat)
                        union = confmat.sum(1) + confmat.sum(0) - np.diag(confmat)
                        per_class_iou = intersection / (union + 1e-10)
                        
                        # Precision = TP / (TP + FP) = diag / col_sum
                        per_class_precision = intersection / (confmat.sum(0) + 1e-10)
                        # Recall = TP / (TP + FN) = diag / row_sum
                        per_class_recall = intersection / (confmat.sum(1) + 1e-10)
                        # F1 = 2 * P * R / (P + R)
                        per_class_f1 = 2 * per_class_precision * per_class_recall / (per_class_precision + per_class_recall + 1e-10)
                        
                        # è·å– Overall Accuracy
                        overall_acc = metric_results.get('overall_accuracy', None)
                        current_miou = float(metric_results[name])
                        
                        # è¾“å‡ºæ ‡é¢˜å’Œæ€»ä½“æŒ‡æ ‡
                        print(f"\n{'='*100}")
                        print(f"Test Results - Per-Class Metrics")
                        print(f"{'='*100}")
                        if overall_acc is not None:
                            print(f"Overall Accuracy: {overall_acc:.4f} ({overall_acc*100:.2f}%)")
                        print(f"Mean IoU: {current_miou:.4f}")
                        print(f"{'='*100}")
                        
                        # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
                        print(f"  {'Class':15s}  {'IoU':>8s}  {'Precision':>10s}  {'Recall':>8s}  {'F1-Score':>10s}")
                        print(f"  {'-'*15}  {'-'*8}  {'-'*10}  {'-'*8}  {'-'*10}")
                        
                        # ä» hparams è·å–ç±»åˆ«å
                        class_names = self.hparams.get('class_names', None) if hasattr(self, 'hparams') else None
                        if class_names:
                            for i, class_name in enumerate(class_names):
                                print(f"  {class_name:15s}  {per_class_iou[i]:8.4f}  {per_class_precision[i]:10.4f}  "
                                      f"{per_class_recall[i]:8.4f}  {per_class_f1[i]:10.4f}")
                        else:
                            for i in range(len(per_class_iou)):
                                print(f"  Class {i:2d}        {per_class_iou[i]:8.4f}  {per_class_precision[i]:10.4f}  "
                                      f"{per_class_recall[i]:8.4f}  {per_class_f1[i]:10.4f}")
                        
                        # è®¡ç®—å¹³å‡æŒ‡æ ‡
                        mean_precision = np.nanmean(per_class_precision)
                        mean_recall = np.nanmean(per_class_recall)
                        mean_f1 = np.nanmean(per_class_f1)
                        
                        print(f"  {'-'*15}  {'-'*8}  {'-'*10}  {'-'*8}  {'-'*10}")
                        print(f"  {'Mean':15s}  {current_miou:8.4f}  {mean_precision:10.4f}  {mean_recall:8.4f}  {mean_f1:10.4f}")
                        print(f"{'='*100}\n")
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•æ‰“å°è¯¦ç»†æŒ‡æ ‡: {e}")
            
            metric.reset()
        self.log_dict(metric_results, on_step=False, on_epoch=True)