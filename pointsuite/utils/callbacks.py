import torch
import numpy as np
import os
import glob
import pickle
import pytorch_lightning as pl
from pytorch_lightning.callbacks import BasePredictionWriter
from typing import List, Any, Dict, Optional
from pathlib import Path
from collections import defaultdict

# å¯¼å…¥ laspy (æ‚¨éœ€è¦ 'pip install laspy')
try:
    import laspy
except ImportError:
    print("è­¦å‘Š: 'laspy' åº“æœªå®‰è£…ã€‚PredictionWriter å°†æ— æ³•ä¿å­˜ .las æ–‡ä»¶ã€‚")
    print("è¯·è¿è¡Œ: pip install laspy")


class SegmentationWriter(BasePredictionWriter):
    """
    ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ PredictionWriter å›è°ƒ (é€‚é… bin+pkl æ•°æ®æ ¼å¼)

    ä¸“ä¸º PointSuite çš„ bin+pkl æ•°æ®ç»“æ„è®¾è®¡ï¼Œä¸ BinPklDataset å’Œ SemanticSegmentationTask ååŒå·¥ä½œã€‚

    æ•°æ®æµç¨‹:
    1. tile.py åœ¨åˆ†å‰² LAS æ–‡ä»¶æ—¶ï¼Œä¸ºæ¯ä¸ª segment ä¿å­˜æ–‡ä»¶å…³è”ä¿¡æ¯:
       - 'bin_file': bin æ–‡ä»¶å
       - 'bin_path': å®Œæ•´ bin æ–‡ä»¶è·¯å¾„
       - 'pkl_path': å®Œæ•´ pkl æ–‡ä»¶è·¯å¾„
       
    2. BinPklDataset (test split) åŠ è½½æ•°æ®æ—¶ï¼Œå°†æ–‡ä»¶ä¿¡æ¯åŠ å…¥ data å­—å…¸:
       {'coord', 'feat', 'indices', 'bin_file', 'bin_path', 'pkl_path', ...}
       
    3. SemanticSegmentationTask.predict_step è¿”å›æ—¶ï¼Œä¼ é€’æ–‡ä»¶ä¿¡æ¯:
       {'logits', 'indices', 'bin_file', 'bin_path', 'pkl_path', 'coord'}
       - 'logits': [N, C] æ¨¡å‹é¢„æµ‹çš„ç±»åˆ« logits
       - 'indices': [N] åŸå§‹ç‚¹ç´¢å¼•
       - 'bin_file': bin æ–‡ä»¶åï¼ˆç›´æ¥æ¥è‡ª datasetï¼‰
       - 'bin_path': bin æ–‡ä»¶å®Œæ•´è·¯å¾„
       - 'pkl_path': pkl æ–‡ä»¶å®Œæ•´è·¯å¾„
       - 'coord': [N, 3] ç‚¹åæ ‡ (ç”¨äºå¯è§†åŒ–)

    4. æœ¬å›è°ƒæ‰§è¡ŒæŠ•ç¥¨å¹¶ä¿å­˜:
       - ç›´æ¥ä½¿ç”¨ä¼ é€’çš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼Œæ— éœ€æ¨æ–­
       - å¯¹äºæ¯ä¸ª bin æ–‡ä»¶ï¼Œç´¯ç§¯æ‰€æœ‰ segment çš„é¢„æµ‹
       - ä½¿ç”¨ logits å¹³å‡è¿›è¡Œå¤šæ¬¡é¢„æµ‹æŠ•ç¥¨
       - ä»åŸå§‹ bin/pkl åŠ è½½å®Œæ•´ç‚¹äº‘å’Œ LAS å¤´ä¿¡æ¯
       - ä¿å­˜ä¸º .las æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹åæ ‡ç³»ç»Ÿå’Œç²¾åº¦

    å·¥ä½œæµç¨‹:
    1. write_on_batch_end: å°†æ¯ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹æµå¼å†™å…¥ä¸´æ—¶æ–‡ä»¶ (.tmp)ï¼Œé˜²æ­¢ OOM
    2. on_predict_end: é¢„æµ‹ç»“æŸåè§¦å‘ï¼Œå¯¹æ¯ä¸ª bin æ–‡ä»¶æ‰§è¡Œ:
       a. æ”¶é›†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
       b. æŒ‰ bin æ–‡ä»¶åˆ†ç»„
       c. æ‰§è¡ŒæŠ•ç¥¨ç´¯ç§¯ (logits å¹³å‡)
       d. ä»åŸå§‹ bin/pkl åŠ è½½åæ ‡å’Œ LAS å¤´
       e. ä¿å­˜å®Œæ•´ç‚¹äº‘ä¸º .las æ–‡ä»¶
       f. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    
    æ³¨æ„: 
    - æ–‡ä»¶ä¿¡æ¯åœ¨æ•´ä¸ªæ•°æ®æµä¸­æ˜¾å¼ä¼ é€’ï¼Œé¿å…äº†æ¨æ–­çš„ä¸ç¡®å®šæ€§
    - å³ä½¿ batch åŒ…å«æ¥è‡ªå¤šä¸ª segment çš„ç‚¹ï¼Œå®ƒä»¬å¿…å®šæ¥è‡ªåŒä¸€ä¸ª bin æ–‡ä»¶
    """
    
    def __init__(self, 
                 output_dir: str, 
                 write_interval: str = "batch", 
                 num_classes: int = -1,
                 save_logits: bool = False,
                 reverse_class_mapping: Optional[Dict[int, int]] = None):
        """
        Args:
            output_dir (str): ä¿å­˜æœ€ç»ˆ .las æ–‡ä»¶çš„ç›®å½•
            write_interval (str): å¿…é¡»æ˜¯ "batch" æ‰èƒ½å®ç°æµå¼ä¼ è¾“
            num_classes (int): ç±»çš„æ•°é‡ï¼Œç”¨äºåˆ›å»ºæŠ•ç¥¨æ•°ç»„
                              å¦‚æœä¸º -1ï¼Œå°†ä» Task çš„ head.out_channels è‡ªåŠ¨æ¨æ–­
            save_logits (bool): æ˜¯å¦åŒæ—¶ä¿å­˜ logits åˆ° .npz æ–‡ä»¶ (ç”¨äºåå¤„ç†/é›†æˆ)
            reverse_class_mapping (Optional[Dict[int, int]]): å°†è¿ç»­æ ‡ç­¾æ˜ å°„å›åŸå§‹æ ‡ç­¾
                                  ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
                                  å¦‚æœä¸º Noneï¼Œåˆ™ä¸åº”ç”¨æ˜ å°„
        """
        super().__init__(write_interval)
        self.output_dir = output_dir
        self.temp_dir = os.path.join(self.output_dir, "temp_predictions")
        self.num_classes = num_classes
        self.save_logits = save_logits
        self.reverse_class_mapping = reverse_class_mapping
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)

    def _get_bin_pkl_path_from_indices(self, indices: torch.Tensor, trainer: 'pl.Trainer') -> Optional[str]:
        """
        ä»ç‚¹ç´¢å¼•åå‘æ¨æ–­å¯¹åº”çš„ bin/pkl æ–‡ä»¶ (å·²å¼ƒç”¨ï¼Œä¿ç•™ä½œä¸ºåå¤‡æ–¹æ¡ˆ)
        
        âš ï¸ å·²å¼ƒç”¨: ç°åœ¨æ–‡ä»¶ä¿¡æ¯ç›´æ¥ä» dataset ä¼ é€’ï¼Œæ— éœ€æ¨æ–­ã€‚
        æ­¤æ–¹æ³•ä»…ä½œä¸ºåå¤‡æ–¹æ¡ˆä¿ç•™ï¼Œç”¨äºå¤„ç†æ—§æ ¼å¼æ•°æ®ã€‚
        
        é€šè¿‡æ£€æŸ¥ dataset.data_list æ¥ç¡®å®šè¿™äº›ç‚¹å±äºå“ªä¸ª bin æ–‡ä»¶ã€‚
        
        Args:
            indices: ç‚¹çš„åŸå§‹ç´¢å¼• [N]
            trainer: PyTorch Lightning Trainer
            
        Returns:
            bin æ–‡ä»¶çš„åŸºç¡€åç§° (ä¸å¸¦æ‰©å±•å)ï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å› None
        """
        try:
            dataset = trainer.predict_dataloaders.dataset
            
            # è·å–ç¬¬ä¸€ä¸ªç´¢å¼•å¯¹åº”çš„æ ·æœ¬ä¿¡æ¯
            # å‡è®¾ä¸€ä¸ª batch çš„æ‰€æœ‰ç‚¹æ¥è‡ªåŒä¸€ä¸ª bin æ–‡ä»¶
            # (è¿™åœ¨æˆ‘ä»¬çš„æ•°æ®ç»“æ„ä¸­é€šå¸¸æ˜¯æˆç«‹çš„ï¼Œå› ä¸ºæ¯ä¸ª segment æ¥è‡ªä¸€ä¸ª bin æ–‡ä»¶)
            
            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°åŒ…å«è¿™äº› indices çš„ segment
            # ç”±äº segment çš„ indices æ˜¯ç¦»æ•£çš„ï¼Œæˆ‘ä»¬æ£€æŸ¥ç¬¬ä¸€ä¸ªç´¢å¼•
            first_idx = indices[0].item() if isinstance(indices[0], torch.Tensor) else indices[0]
            
            # ä» data_list ä¸­æ‰¾åˆ°åŒ…å«è¯¥ç´¢å¼•çš„ segment
            for sample_info in dataset.data_list:
                segment_indices = sample_info.get('indices', [])
                if first_idx in segment_indices:
                    bin_path = Path(sample_info['bin_path'])
                    return bin_path.stem  # è¿”å›æ–‡ä»¶å (ä¸å¸¦æ‰©å±•å)
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ batch_idx ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            return None
            
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•ä» indices æ¨æ–­ bin æ–‡ä»¶: {e}")
            return None

    def _get_bin_info_from_sample(self, trainer: 'pl.Trainer', sample_idx: int = 0) -> Optional[Dict[str, Any]]:
        """
        ä» dataset è·å– bin æ–‡ä»¶ä¿¡æ¯
        
        Args:
            trainer: PyTorch Lightning Trainer
            sample_idx: æ ·æœ¬ç´¢å¼• (é»˜è®¤è·å–ç¬¬ä¸€ä¸ª)
            
        Returns:
            åŒ…å« bin_path, pkl_path, bin_basename ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        try:
            dataset = trainer.predict_dataloaders.dataset
            
            if sample_idx >= len(dataset.data_list):
                sample_idx = 0
                
            sample_info = dataset.data_list[sample_idx]
            
            bin_path = Path(sample_info['bin_path'])
            pkl_path = Path(sample_info['pkl_path'])
            
            return {
                'bin_path': str(bin_path),
                'pkl_path': str(pkl_path),
                'bin_basename': bin_path.stem,
            }
            
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•ä» dataset è·å– bin æ–‡ä»¶ä¿¡æ¯: {e}")
            return None


    def write_on_batch_end(
        self, 
        trainer: 'pl.Trainer', 
        pl_module: 'pl.LightningModule', 
        prediction: Dict[str, torch.Tensor], 
        batch_indices: List[int], 
        batch: Any, 
        batch_idx: int, 
        dataloader_idx: int
    ):
        """ 
        åœ¨æ¯ä¸ªé¢„æµ‹æ‰¹æ¬¡ç»“æŸåï¼Œå°†ç»“æœæµå¼å†™å…¥ä¸´æ—¶æ–‡ä»¶
        
        prediction å­—å…¸åº”åŒ…å«:
        - 'logits': [N, C] æ¨¡å‹é¢„æµ‹
        - 'indices': [N] åŸå§‹ bin æ–‡ä»¶ä¸­çš„ç‚¹ç´¢å¼•
        - 'bin_file': bin æ–‡ä»¶åï¼ˆåˆ—è¡¨ï¼Œæ¯ä¸ªç‚¹å¯¹åº”çš„æ–‡ä»¶ï¼‰
        - 'coord': [N, 3] ç‚¹åæ ‡ (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
        """
        
        if 'logits' not in prediction or 'indices' not in prediction:
            print(f"è­¦å‘Š: predict_step å¿…é¡»è¿”å› 'logits' å’Œ 'indices'ã€‚è·³è¿‡æ‰¹æ¬¡ {batch_idx}")
            return
        
        # 1. ğŸ”¥ ç›´æ¥ä» prediction è·å– bin æ–‡ä»¶ä¿¡æ¯ï¼ˆç”± dataset æä¾›ï¼‰
        if 'bin_file' in prediction and len(prediction['bin_file']) > 0:
            # bin_file æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ˆcollate_fn å¯èƒ½ä¼š stack æˆ–ä¿æŒä¸ºåˆ—è¡¨ï¼‰
            bin_files = prediction['bin_file']
            
            # å–ç¬¬ä¸€ä¸ªæ–‡ä»¶åï¼ˆå‡è®¾ä¸€ä¸ª batch å†…çš„ç‚¹æ¥è‡ªåŒä¸€ä¸ª bin æ–‡ä»¶ï¼‰
            if isinstance(bin_files, list):
                bin_basename = bin_files[0]
            elif isinstance(bin_files, torch.Tensor):
                # å¦‚æœè¢«è½¬æ¢ä¸º tensorï¼Œè½¬å›å­—ç¬¦ä¸²
                bin_basename = str(bin_files[0].item())
            else:
                bin_basename = str(bin_files)
        else:
            # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨æ—§çš„æ¨æ–­æ–¹æ³•ï¼ˆä¸æ¨èï¼Œä½†ä¿æŒå…¼å®¹æ€§ï¼‰
            indices = prediction['indices']
            bin_basename = self._get_bin_pkl_path_from_indices(indices, trainer)
            
            if bin_basename is None:
                bin_basename = f"unknown_batch_{batch_idx}"
                print(f"è­¦å‘Š: æ— æ³•ç¡®å®š batch {batch_idx} å¯¹åº”çš„ bin æ–‡ä»¶ï¼Œä½¿ç”¨ {bin_basename}")
        
        # 2. å®šä¹‰ä¸´æ—¶æ–‡ä»¶å
        tmp_filename = f"{bin_basename}_batch_{batch_idx}.pred.tmp"
        save_path = os.path.join(self.temp_dir, tmp_filename)
        
        # 3. ä¿å­˜é¢„æµ‹ç»“æœåˆ°ç£ç›˜ (åªä¿å­˜å¿…è¦ä¿¡æ¯)
        save_dict = {
            'logits': prediction['logits'].cpu(),      # [N, C]
            'indices': prediction['indices'].cpu(),    # [N]
            'bin_file': bin_basename,                  # ğŸ”¥ ä¿å­˜æ–‡ä»¶å
        }
        
        # ğŸ”¥ ä¿å­˜å®Œæ•´è·¯å¾„ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'bin_path' in prediction:
            save_dict['bin_path'] = prediction['bin_path']
        if 'pkl_path' in prediction:
            save_dict['pkl_path'] = prediction['pkl_path']
        
        # å¯é€‰: ä¿å­˜åæ ‡ç”¨äºè°ƒè¯•
        if 'coord' in prediction and self.save_logits:
            save_dict['coord'] = prediction['coord'].cpu()
        
        torch.save(save_dict, save_path)
        
        # å¯é€‰: æ‰“å°è¿›åº¦
        if batch_idx % 10 == 0:
            pl_module.print(f"[SegmentationWriter] å·²ä¿å­˜ batch {batch_idx} åˆ° {tmp_filename}")

    def on_predict_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """
        åœ¨æ•´ä¸ªé¢„æµ‹ç»“æŸåè§¦å‘ï¼Œå¯¹æ‰€æœ‰ bin æ–‡ä»¶æ‰§è¡ŒæŠ•ç¥¨å’Œä¿å­˜
        
        æ­¤æ–¹æ³•ä¼š:
        1. æŒ‰ bin æ–‡ä»¶åˆ†ç»„æ‰€æœ‰ä¸´æ—¶é¢„æµ‹æ–‡ä»¶
        2. å¯¹æ¯ä¸ª bin æ–‡ä»¶æ‰§è¡ŒæŠ•ç¥¨ (logits å¹³å‡)
        3. ä»åŸå§‹ bin/pkl åŠ è½½å®Œæ•´ç‚¹äº‘æ•°æ®
        4. ä¿å­˜ä¸º .las æ–‡ä»¶
        5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        """
        
        # å¦‚æœ num_classes æœªæŒ‡å®šï¼Œä» Task æ¨æ–­
        if self.num_classes == -1:
            try:
                self.num_classes = pl_module.head.out_channels
                pl_module.print(f"[SegmentationWriter] ä»æ¨¡å‹æ¨æ–­ç±»åˆ«æ•°: {self.num_classes}")
            except Exception as e:
                print(f"é”™è¯¯: æ— æ³•ä»æ¨¡å‹æ¨æ–­ num_classes: {e}")
                print("è¯·åœ¨åˆå§‹åŒ– SegmentationWriter æ—¶æ˜¾å¼æŒ‡å®š num_classes")
                return

        pl_module.print(f"\n[SegmentationWriter] é¢„æµ‹å®Œæˆï¼Œå¼€å§‹æ‹¼æ¥å’ŒæŠ•ç¥¨...")
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        tmp_files = sorted(glob.glob(os.path.join(self.temp_dir, "*.pred.tmp")))
        
        if not tmp_files:
            pl_module.print("[SegmentationWriter] è­¦å‘Š: æœªæ‰¾åˆ°ä¸´æ—¶é¢„æµ‹æ–‡ä»¶")
            return
        
        pl_module.print(f"[SegmentationWriter] æ‰¾åˆ° {len(tmp_files)} ä¸ªä¸´æ—¶é¢„æµ‹æ–‡ä»¶")
        
        # 2. æŒ‰ bin æ–‡ä»¶åˆ†ç»„ä¸´æ—¶æ–‡ä»¶
        # æ–‡ä»¶åæ ¼å¼: {bin_basename}_batch_{batch_idx}.pred.tmp
        bin_file_groups = defaultdict(list)
        
        for tmp_file in tmp_files:
            filename = os.path.basename(tmp_file)
            # æå– bin_basename (å»é™¤ _batch_xxx.pred.tmp éƒ¨åˆ†)
            bin_basename = filename.split('_batch_')[0]
            bin_file_groups[bin_basename].append(tmp_file)
        
        pl_module.print(f"[SegmentationWriter] æ£€æµ‹åˆ° {len(bin_file_groups)} ä¸ªå”¯ä¸€ bin æ–‡ä»¶")
        
        # 3. å¯¹æ¯ä¸ª bin æ–‡ä»¶æ‰§è¡ŒæŠ•ç¥¨å’Œä¿å­˜
        for bin_basename, tmp_file_list in bin_file_groups.items():
            pl_module.print(f"\n[SegmentationWriter] å¤„ç† bin æ–‡ä»¶: {bin_basename} ({len(tmp_file_list)} ä¸ªæ‰¹æ¬¡)")
            
            try:
                self._process_single_bin_file(
                    bin_basename=bin_basename,
                    tmp_files=tmp_file_list,
                    trainer=trainer,
                    pl_module=pl_module
                )
            except Exception as e:
                pl_module.print(f"!!! é”™è¯¯: å¤„ç† {bin_basename} æ—¶å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # 4. æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        pl_module.print(f"\n[SegmentationWriter] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for tmp_file in tmp_files:
            try:
                os.remove(tmp_file)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {tmp_file}: {e}")
        
        pl_module.print(f"[SegmentationWriter] æ‰€æœ‰é¢„æµ‹å·²ä¿å­˜åˆ° {self.output_dir}")
        pl_module.print("="*70)
    
    def _process_single_bin_file(
        self,
        bin_basename: str,
        tmp_files: List[str],
        trainer: 'pl.Trainer',
        pl_module: 'pl.LightningModule'
    ):
        """
        å¤„ç†å•ä¸ª bin æ–‡ä»¶çš„æ‰€æœ‰é¢„æµ‹æ‰¹æ¬¡
        
        æ‰§è¡Œæ­¥éª¤:
        1. ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½æ‰€æœ‰é¢„æµ‹å¹¶æ‰§è¡ŒæŠ•ç¥¨
        2. ä»åŸå§‹ bin/pkl æ–‡ä»¶åŠ è½½å®Œæ•´ç‚¹äº‘æ•°æ®
        3. åº”ç”¨ç±»åˆ«æ˜ å°„ (å¦‚æœæœ‰)
        4. ä¿å­˜ä¸º .las æ–‡ä»¶
        5. (å¯é€‰) ä¿å­˜ logits åˆ° .npz
        
        Args:
            bin_basename: bin æ–‡ä»¶çš„åŸºç¡€åç§° (ä¸å¸¦æ‰©å±•å)
            tmp_files: è¯¥ bin æ–‡ä»¶çš„æ‰€æœ‰ä¸´æ—¶é¢„æµ‹æ–‡ä»¶åˆ—è¡¨
            trainer: PyTorch Lightning Trainer
            pl_module: PyTorch Lightning Module
        """
        
        # 1. ğŸ”¥ ä¼˜å…ˆä»ä¸´æ—¶æ–‡ä»¶ä¸­è·å–å®Œæ•´è·¯å¾„ä¿¡æ¯
        bin_path, pkl_path = None, None
        
        # å°è¯•ä»ç¬¬ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¸­è¯»å–è·¯å¾„ä¿¡æ¯
        if len(tmp_files) > 0:
            try:
                first_tmp = torch.load(tmp_files[0])
                if 'bin_path' in first_tmp and 'pkl_path' in first_tmp:
                    # ä»ä¸´æ—¶æ–‡ä»¶ä¸­ç›´æ¥è·å–è·¯å¾„
                    bin_path_list = first_tmp['bin_path']
                    pkl_path_list = first_tmp['pkl_path']
                    
                    # å¤„ç†åˆ—è¡¨æƒ…å†µï¼ˆcollate_fn å¯èƒ½ä¿æŒä¸ºåˆ—è¡¨ï¼‰
                    if isinstance(bin_path_list, list):
                        bin_path = str(Path(bin_path_list[0]))
                        pkl_path = str(Path(pkl_path_list[0]))
                    else:
                        bin_path = str(Path(bin_path_list))
                        pkl_path = str(Path(pkl_path_list))
                    
                    pl_module.print(f"  - ä»ä¸´æ—¶æ–‡ä»¶è·å–è·¯å¾„ âœ“")
            except Exception as e:
                pl_module.print(f"  - ä»ä¸´æ—¶æ–‡ä»¶è·å–è·¯å¾„å¤±è´¥: {e}")
        
        # 2. å¦‚æœæ²¡æœ‰ä»ä¸´æ—¶æ–‡ä»¶è·å–åˆ°ï¼Œä½¿ç”¨æ—§çš„æŸ¥æ‰¾æ–¹æ³•ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        if bin_path is None or pkl_path is None:
            pl_module.print(f"  - ä½¿ç”¨åå¤‡æ–¹æ¡ˆæŸ¥æ‰¾æ–‡ä»¶...")
            bin_path, pkl_path = self._find_bin_pkl_paths(bin_basename, trainer)
        
        if bin_path is None or pkl_path is None:
            pl_module.print(f"é”™è¯¯: æ— æ³•æ‰¾åˆ° {bin_basename} å¯¹åº”çš„ bin/pkl æ–‡ä»¶")
            return
        
        pl_module.print(f"  - Bin æ–‡ä»¶: {bin_path}")
        pl_module.print(f"  - Pkl æ–‡ä»¶: {pkl_path}")
        
        # 2. ä» pkl åŠ è½½å…ƒæ•°æ®
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        
        # 3. ä½¿ç”¨ memmap åŠ è½½å®Œæ•´ç‚¹äº‘æ•°æ®
        point_data = np.memmap(bin_path, dtype=metadata['dtype'], mode='r')
        num_total_points = len(point_data)
        
        pl_module.print(f"  - æ€»ç‚¹æ•°: {num_total_points:,}")
        
        # 4. åˆ›å»ºæŠ•ç¥¨æ•°ç»„
        logits_sum = torch.zeros((num_total_points, self.num_classes), dtype=torch.float32)
        counts = torch.zeros(num_total_points, dtype=torch.int32)
        
        # 5. åŠ è½½æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶å¹¶ç´¯ç§¯æŠ•ç¥¨
        pl_module.print(f"  - åŠ è½½ {len(tmp_files)} ä¸ªé¢„æµ‹æ‰¹æ¬¡...")
        
        for tmp_file in tmp_files:
            try:
                pred_data = torch.load(tmp_file)
                indices = pred_data['indices']  # [N]
                logits = pred_data['logits']    # [N, C]
                
                # ç´¯ç§¯ logits
                logits_sum.index_add_(0, indices.long(), logits)
                counts.index_add_(0, indices.long(), torch.ones(len(indices), dtype=torch.int32))
                
            except Exception as e:
                pl_module.print(f"    è­¦å‘Š: åŠ è½½ {tmp_file} å¤±è´¥: {e}")
        
        # 6. è®¡ç®—å¹³å‡ logits å’Œæœ€ç»ˆé¢„æµ‹
        pl_module.print(f"  - è®¡ç®—æœ€ç»ˆé¢„æµ‹...")
        
        # å¤„ç†æœªè¢«é¢„æµ‹çš„ç‚¹
        unpredicted_mask = (counts == 0)
        counts[unpredicted_mask] = 1  # é¿å…é™¤ä»¥ 0
        
        # å¹³å‡ logits
        mean_logits = logits_sum / counts.unsqueeze(-1)
        
        # Argmax è·å–ç±»åˆ«
        final_preds = torch.argmax(mean_logits, dim=1).numpy().astype(np.uint8)
        
        # å¯¹æœªé¢„æµ‹çš„ç‚¹èµ‹å€¼ (ä½¿ç”¨ 0 æˆ– ignore_label)
        if unpredicted_mask.any():
            num_unpredicted = unpredicted_mask.sum().item()
            pl_module.print(f"    è­¦å‘Š: {num_unpredicted} ä¸ªç‚¹æœªè¢«é¢„æµ‹ï¼Œå°†èµ‹äºˆæ ‡ç­¾ 0")
            final_preds[unpredicted_mask.numpy()] = 0
        
        # 7. åº”ç”¨åå‘ç±»åˆ«æ˜ å°„ (å¦‚æœæœ‰)
        if self.reverse_class_mapping is not None:
            pl_module.print(f"  - åº”ç”¨åå‘ç±»åˆ«æ˜ å°„...")
            final_preds_mapped = np.zeros_like(final_preds)
            for continuous_label, original_label in self.reverse_class_mapping.items():
                final_preds_mapped[final_preds == continuous_label] = original_label
            final_preds = final_preds_mapped
        
        # 8. æå–åæ ‡ (XYZ)
        xyz = np.stack([
            point_data['X'],
            point_data['Y'],
            point_data['Z']
        ], axis=1).astype(np.float64)
        
        # 9. ä¿å­˜ä¸º .las æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰åŸå§‹å±æ€§ï¼‰
        final_las_path = os.path.join(self.output_dir, f"{bin_basename}_predicted.las")
        
        try:
            # å°† bin_path æ·»åŠ åˆ° metadata ä¸­ï¼Œæ–¹ä¾¿ _save_las_file åŠ è½½åŸå§‹æ•°æ®
            metadata['_bin_path'] = bin_path
            
            self._save_las_file(
                las_path=final_las_path,
                xyz=xyz,
                classification=final_preds,
                metadata=metadata,
                pl_module=pl_module
            )
            pl_module.print(f"  âœ“ å·²ä¿å­˜åˆ°: {final_las_path}")
            
        except Exception as e:
            pl_module.print(f"  !!! é”™è¯¯: ä¿å­˜ .las æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # 10. (å¯é€‰) ä¿å­˜ logits
        if self.save_logits:
            logits_path = os.path.join(self.output_dir, f"{bin_basename}_logits.npz")
            np.savez_compressed(
                logits_path,
                logits=mean_logits.numpy(),
                predictions=final_preds,
                counts=counts.numpy()
            )
            pl_module.print(f"  âœ“ Logits å·²ä¿å­˜åˆ°: {logits_path}")
    
    def _find_bin_pkl_paths(self, bin_basename: str, trainer: 'pl.Trainer') -> tuple:
        """
        æ ¹æ® bin_basename æŸ¥æ‰¾å¯¹åº”çš„ bin å’Œ pkl æ–‡ä»¶è·¯å¾„
        
        Args:
            bin_basename: bin æ–‡ä»¶çš„åŸºç¡€åç§°
            trainer: Trainer å¯¹è±¡
            
        Returns:
            (bin_path, pkl_path) å…ƒç»„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› (None, None)
        """
        try:
            dataset = trainer.predict_dataloaders.dataset
            
            # åœ¨ data_list ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            for sample_info in dataset.data_list:
                bin_path = Path(sample_info['bin_path'])
                if bin_path.stem == bin_basename:
                    pkl_path = Path(sample_info['pkl_path'])
                    return str(bin_path), str(pkl_path)
            
            # å¦‚æœåœ¨ data_list ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä» data_root æœç´¢
            data_root = Path(dataset.data_root) if not isinstance(dataset.data_root, (list, tuple)) else Path(dataset.data_root[0]).parent
            
            bin_path = data_root / f"{bin_basename}.bin"
            pkl_path = data_root / f"{bin_basename}.pkl"
            
            if bin_path.exists() and pkl_path.exists():
                return str(bin_path), str(pkl_path)
            
            return None, None
            
        except Exception as e:
            print(f"é”™è¯¯: æŸ¥æ‰¾ bin/pkl æ–‡ä»¶å¤±è´¥: {e}")
            return None, None
    
    def _save_las_file(
        self,
        las_path: str,
        xyz: np.ndarray,
        classification: np.ndarray,
        metadata: Dict[str, Any],
        pl_module: 'pl.LightningModule'
    ):
        """
        ä¿å­˜ç‚¹äº‘ä¸º .las æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹ LAS å¤´ä¿¡æ¯å’Œæ‰€æœ‰ç‚¹å±æ€§
        
        Args:
            las_path: è¾“å‡º .las æ–‡ä»¶è·¯å¾„
            xyz: [N, 3] ç‚¹åæ ‡
            classification: [N] åˆ†ç±»æ ‡ç­¾ï¼ˆé¢„æµ‹ç»“æœï¼‰
            metadata: ä» pkl æ–‡ä»¶åŠ è½½çš„å…ƒæ•°æ® (åŒ…å« header_info)
            pl_module: PyTorch Lightning Module
        """
        
        try:
            # 1. ä» metadata ä¸­æ¢å¤ LAS å¤´ä¿¡æ¯
            if 'header_info' in metadata:
                header_info = metadata['header_info']
                
                # åˆ›å»º LAS å¤´
                point_format = header_info.get('point_format', 3)
                version_str = header_info.get('version', '1.2')
                
                # è§£æç‰ˆæœ¬å­—ç¬¦ä¸²
                if isinstance(version_str, str):
                    version_parts = version_str.split('.')
                    if len(version_parts) == 2:
                        major, minor = int(version_parts[0]), int(version_parts[1])
                    else:
                        major, minor = 1, 2
                else:
                    major, minor = 1, 2
                
                header = laspy.LasHeader(point_format=point_format, version=f"{major}.{minor}")
                
                # è®¾ç½®ç¼©æ”¾å’Œåç§»
                header.offsets = [
                    header_info.get('x_offset', 0),
                    header_info.get('y_offset', 0),
                    header_info.get('z_offset', 0)
                ]
                header.scales = [
                    header_info.get('x_scale', 0.01),
                    header_info.get('y_scale', 0.01),
                    header_info.get('z_scale', 0.01)
                ]
                
                # æ¢å¤å…¶ä»–å¤´ä¿¡æ¯
                if 'system_identifier' in header_info:
                    header.system_identifier = header_info['system_identifier']
                if 'generating_software' in header_info:
                    header.generating_software = header_info['generating_software']
                
                # æ¢å¤ VLRs (Variable Length Records) - åŒ…å«åæ ‡ç³»ä¿¡æ¯
                if 'vlrs' in header_info and header_info['vlrs']:
                    for vlr_dict in header_info['vlrs']:
                        try:
                            vlr = laspy.VLR(
                                user_id=vlr_dict['user_id'],
                                record_id=vlr_dict['record_id'],
                                description=vlr_dict['description'],
                                record_data=vlr_dict.get('record_data', b'')
                            )
                            header.vlrs.append(vlr)
                        except Exception as e:
                            pl_module.print(f"    è­¦å‘Š: æ¢å¤ VLR å¤±è´¥: {e}")
                
                pl_module.print(f"    - ä½¿ç”¨åŸå§‹ LAS å¤´ä¿¡æ¯ (format {point_format}, version {major}.{minor})")
                
            else:
                # å¦‚æœæ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼
                pl_module.print("    è­¦å‘Š: å…ƒæ•°æ®ä¸­æ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼")
                header = laspy.LasHeader(point_format=3, version='1.2')
                header.offsets = xyz.min(axis=0)
                header.scales = np.array([0.001, 0.001, 0.001])
            
            # 2. åˆ›å»º LAS æ•°æ®
            las = laspy.LasData(header)
            
            # 3. è®¾ç½®åæ ‡ (laspy ä¼šè‡ªåŠ¨åº”ç”¨ scale å’Œ offset)
            las.x = xyz[:, 0]
            las.y = xyz[:, 1]
            las.z = xyz[:, 2]
            
            # 4. ğŸ”¥ ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤æ‰€æœ‰å¯ç”¨å±æ€§
            if 'dtype' in metadata:
                dtype = metadata['dtype']
                
                # è·å– bin æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä» metadata ä¸­çš„ _bin_pathï¼‰
                bin_path = metadata.get('_bin_path', None)
                
                if bin_path and Path(bin_path).exists():
                    pl_module.print(f"    - ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤å±æ€§: {Path(bin_path).name}")
                    
                    # ä½¿ç”¨ memmap åŠ è½½åŸå§‹æ•°æ®
                    point_data = np.memmap(bin_path, dtype=dtype, mode='r')
                    
                    # æ¢å¤å„ä¸ªå±æ€§ï¼ˆæ ¹æ® dtype ä¸­çš„å­—æ®µï¼‰
                    field_names = [name for name, _ in dtype]
                    
                    # å¼ºåº¦ (Intensity)
                    if 'intensity' in field_names:
                        las.intensity = point_data['intensity']
                        pl_module.print(f"      âœ“ æ¢å¤ intensity")
                    
                    # å›æ³¢ä¿¡æ¯ (Return Number, Number of Returns)
                    if 'return_number' in field_names:
                        las.return_number = point_data['return_number']
                        pl_module.print(f"      âœ“ æ¢å¤ return_number")
                    if 'number_of_returns' in field_names:
                        las.number_of_returns = point_data['number_of_returns']
                        pl_module.print(f"      âœ“ æ¢å¤ number_of_returns")
                    
                    # æ‰«æè§’åº¦ (Scan Angle)
                    if 'scan_angle_rank' in field_names:
                        las.scan_angle_rank = point_data['scan_angle_rank']
                        pl_module.print(f"      âœ“ æ¢å¤ scan_angle_rank")
                    elif 'scan_angle' in field_names:
                        las.scan_angle = point_data['scan_angle']
                        pl_module.print(f"      âœ“ æ¢å¤ scan_angle")
                    
                    # ç”¨æˆ·æ•°æ® (User Data)
                    if 'user_data' in field_names:
                        las.user_data = point_data['user_data']
                        pl_module.print(f"      âœ“ æ¢å¤ user_data")
                    
                    # ç‚¹æº ID (Point Source ID)
                    if 'point_source_id' in field_names:
                        las.point_source_id = point_data['point_source_id']
                        pl_module.print(f"      âœ“ æ¢å¤ point_source_id")
                    
                    # GPS æ—¶é—´ (GPS Time)
                    if 'gps_time' in field_names:
                        las.gps_time = point_data['gps_time']
                        pl_module.print(f"      âœ“ æ¢å¤ gps_time")
                    
                    # RGB é¢œè‰² (å¦‚æœ point_format æ”¯æŒ)
                    if header.point_format.id in [2, 3, 5, 7, 8, 10]:
                        if 'red' in field_names and 'green' in field_names and 'blue' in field_names:
                            las.red = point_data['red']
                            las.green = point_data['green']
                            las.blue = point_data['blue']
                            pl_module.print(f"      âœ“ æ¢å¤ RGB é¢œè‰²")
                        
                        # NIR (è¿‘çº¢å¤–) - å¦‚æœæ”¯æŒ
                        if 'nir' in field_names and header.point_format.id in [8, 10]:
                            las.nir = point_data['nir']
                            pl_module.print(f"      âœ“ æ¢å¤ NIR")
                    
                    # å…¶ä»–å¯èƒ½çš„å­—æ®µå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ 
                    
                else:
                    pl_module.print(f"    è­¦å‘Š: æœªæ‰¾åˆ°åŸå§‹ bin æ–‡ä»¶: {bin_path}")
                    pl_module.print(f"    åªä¿å­˜åæ ‡å’Œåˆ†ç±»æ ‡ç­¾")
            
            # 5. è®¾ç½®é¢„æµ‹çš„åˆ†ç±»æ ‡ç­¾ï¼ˆè¦†ç›–åŸå§‹åˆ†ç±»ï¼‰
            las.classification = classification
            pl_module.print(f"      âœ“ è®¾ç½®é¢„æµ‹åˆ†ç±»æ ‡ç­¾")
            
            # 6. å†™å…¥æ–‡ä»¶
            las.write(las_path)
            
        except Exception as e:
            pl_module.print(f"    é”™è¯¯: ä¿å­˜ LAS æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise