import torch
import numpy as np
import os
import glob
import pickle
import pytorch_lightning as pl
from pytorch_lightning.callbacks import BasePredictionWriter
from typing import List, Any, Dict, Optional
from pathlib import Path
from collections import defaultdict
import time
from pytorch_lightning.callbacks import Callback


# å¯¼å…¥ laspy (æ‚¨éœ€è¦ 'pip install laspy')
try:
    import laspy
except ImportError:
    print("è­¦å‘Š: 'laspy' åº“æœªå®‰è£…ã€‚PredictionWriter å°†æ— æ³•ä¿å­˜ .las æ–‡ä»¶ã€‚")
    print("è¯·è¿è¡Œ: pip install laspy")


# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

def create_reverse_class_mapping(class_mapping: Dict[int, int]) -> Dict[int, int]:
    """
    ä» class_mapping åˆ›å»º reverse_class_mapping
    
    Args:
        class_mapping: åŸå§‹æ ‡ç­¾ -> è¿ç»­æ ‡ç­¾çš„æ˜ å°„
                      ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
    
    Returns:
        reverse_class_mapping: è¿ç»­æ ‡ç­¾ -> åŸå§‹æ ‡ç­¾çš„æ˜ å°„
                              ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
    
    Example:
        >>> class_mapping = {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
        >>> reverse_mapping = create_reverse_class_mapping(class_mapping)
        >>> print(reverse_mapping)
        {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
    """
    return {v: k for k, v in class_mapping.items()}


class SemanticPredictLasWriter(BasePredictionWriter):
    """
    ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ PredictionWriter å›è°ƒ (é€‚é… bin+pkl æ•°æ®æ ¼å¼)
    
    é‡å‘½åè‡ª SegmentationWriterï¼Œä¸“ä¸º PointSuite çš„ bin+pkl æ•°æ®ç»“æ„è®¾è®¡ã€‚
    è´Ÿè´£å°†æ¨¡å‹é¢„æµ‹ç»“æœæµå¼å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œå¹¶åœ¨é¢„æµ‹ç»“æŸååˆå¹¶ã€æŠ•ç¥¨å¹¶ä¿å­˜ä¸º LAS æ–‡ä»¶ã€‚

    ä¸»è¦åŠŸèƒ½:
    1. æµå¼å†™å…¥: é˜²æ­¢å¤§è§„æ¨¡ç‚¹äº‘é¢„æµ‹æ—¶çš„ OOMã€‚
    2. æŠ•ç¥¨æœºåˆ¶: å¯¹é‡å é¢„æµ‹è¿›è¡Œ logits å¹³å‡æŠ•ç¥¨ã€‚
    3. å®Œæ•´æ€§æ¢å¤: ä»åŸå§‹ bin/pkl æ¢å¤åæ ‡å’Œå±æ€§ã€‚
    4. æ ¼å¼ä¿æŒ: ä¿ç•™åŸå§‹ LAS å¤´ä¿¡æ¯å’Œåæ ‡ç³»ã€‚
    """
    
    def __init__(self, 
                 output_dir: str, 
                 write_interval: str = "batch", 
                 num_classes: int = -1,
                 save_logits: bool = False,
                 reverse_class_mapping: Optional[Dict[int, int]] = None,
                 auto_infer_reverse_mapping: bool = True):
        super().__init__(write_interval)
        self.output_dir = output_dir
        self.temp_dir = os.path.join(self.output_dir, "temp_predictions")
        self.num_classes = num_classes
        self.save_logits = save_logits
        self.reverse_class_mapping = reverse_class_mapping
        self.auto_infer_reverse_mapping = auto_infer_reverse_mapping
        self._mapping_inferred = False
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)
    
    def on_predict_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """é¢„æµ‹å¼€å§‹å‰çš„åˆå§‹åŒ–å·¥ä½œï¼Œä¸»è¦æ˜¯æ¨æ–­ç±»åˆ«æ˜ å°„"""
        self._infer_class_mapping(trainer, pl_module)

    def write_on_batch_end(
        self, 
        trainer: 'pl.Trainer', 
        pl_module: 'pl.LightningModule', 
        prediction: Dict[str, torch.Tensor], 
        batch_indices: List[int], 
        batch: Any, 
        batch_idx: int, 
        dataloader_idx: int
    ):
        """æ¯ä¸ªæ‰¹æ¬¡ç»“æŸæ—¶ï¼Œå°†é¢„æµ‹ç»“æœå†™å…¥ä¸´æ—¶æ–‡ä»¶"""
        if not self._validate_prediction(prediction, batch_idx):
            return
        
        # 1. å‡†å¤‡æ•°æ®
        bin_files = prediction['bin_file']
        logits = prediction['logits'].cpu().float() # ç¡®ä¿ float32
        indices = prediction['indices'].cpu()
        
        bin_paths = prediction.get('bin_path', [None] * len(bin_files))
        pkl_paths = prediction.get('pkl_path', [None] * len(bin_files))
        
        # è·å– offset
        offsets = batch['offset'].cpu().numpy() if 'offset' in batch else [len(logits)]
        
        # 2. æŒ‰æ–‡ä»¶åˆ†ç»„å¹¶ä¿å­˜
        self._save_batch_predictions(
            bin_files, logits, indices, bin_paths, pkl_paths, offsets, batch_idx, pl_module
        )

    def on_predict_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """é¢„æµ‹ç»“æŸåçš„æ±‡æ€»å¤„ç†"""
        self._ensure_num_classes(pl_module)
        pl_module.print(f"\n[SemanticPredictLasWriter] é¢„æµ‹å®Œæˆï¼Œå¼€å§‹æ‹¼æ¥å’ŒæŠ•ç¥¨...")
        
        tmp_files = sorted(glob.glob(os.path.join(self.temp_dir, "*.pred.tmp")))
        if not tmp_files:
            pl_module.print("[SemanticPredictLasWriter] è­¦å‘Š: æœªæ‰¾åˆ°ä¸´æ—¶é¢„æµ‹æ–‡ä»¶")
            return
            
        # æŒ‰ bin æ–‡ä»¶åˆ†ç»„
        bin_file_groups = self._group_temp_files(tmp_files)
        pl_module.print(f"[SemanticPredictLasWriter] æ£€æµ‹åˆ° {len(bin_file_groups)} ä¸ªå”¯ä¸€ bin æ–‡ä»¶")
        
        try:
            for bin_basename, file_list in bin_file_groups.items():
                pl_module.print(f"\n[SemanticPredictLasWriter] å¤„ç† bin æ–‡ä»¶: {bin_basename} ({len(file_list)} ä¸ªæ‰¹æ¬¡)")
                try:
                    self._process_single_bin_file(bin_basename, file_list, trainer, pl_module)
                except Exception as e:
                    pl_module.print(f"!!! é”™è¯¯: å¤„ç† {bin_basename} æ—¶å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
        finally:
            self._cleanup_temp_files(tmp_files, pl_module)

    # ================= å†…éƒ¨è¾…åŠ©æ–¹æ³• =================

    def _infer_class_mapping(self, trainer, pl_module):
        """æ¨æ–­åå‘ç±»åˆ«æ˜ å°„"""
        if self.reverse_class_mapping is not None:
            pl_module.print(f"[SemanticPredictLasWriter] ä½¿ç”¨ç”¨æˆ·æä¾›çš„ reverse_class_mapping: {self.reverse_class_mapping}")
            return
        
        if not self.auto_infer_reverse_mapping:
            return
            
        # å°è¯•ä»æ¨¡å‹ checkpoint è·å–
        try:
            if hasattr(pl_module, 'hparams') and hasattr(pl_module.hparams, 'class_mapping'):
                mapping = pl_module.hparams.class_mapping
                if mapping:
                    self.reverse_class_mapping = {v: k for k, v in mapping.items()}
                    self._mapping_inferred = True
                    pl_module.print(f"[SemanticPredictLasWriter] ä»æ¨¡å‹ checkpoint åŠ è½½ reverse_class_mapping")
                    return
        except Exception:
            pass
            
        # å°è¯•ä» DataModule è·å–
        try:
            datamodule = trainer.datamodule
            if hasattr(datamodule, 'class_mapping') and datamodule.class_mapping:
                self.reverse_class_mapping = {v: k for k, v in datamodule.class_mapping.items()}
                self._mapping_inferred = True
                pl_module.print(f"[SemanticPredictLasWriter] ä» DataModule æ¨æ–­ reverse_class_mapping")
            else:
                pl_module.print(f"[SemanticPredictLasWriter] æœªæ‰¾åˆ° class_mappingï¼Œä½¿ç”¨è¿ç»­æ ‡ç­¾")
        except Exception as e:
            pl_module.print(f"[SemanticPredictLasWriter] è­¦å‘Š: æ— æ³•æ¨æ–­ reverse_class_mapping: {e}")

    def _validate_prediction(self, prediction, batch_idx):
        if 'logits' not in prediction or 'indices' not in prediction:
            print(f"è­¦å‘Š: predict_step å¿…é¡»è¿”å› 'logits' å’Œ 'indices'ã€‚è·³è¿‡æ‰¹æ¬¡ {batch_idx}")
            return False
        if 'bin_file' not in prediction or len(prediction['bin_file']) == 0:
            print(f"è­¦å‘Š: batch {batch_idx} ç¼ºå°‘ bin_file ä¿¡æ¯ï¼Œè·³è¿‡")
            return False
        return True

    def _save_batch_predictions(self, bin_files, logits, indices, bin_paths, pkl_paths, offsets, batch_idx, pl_module):
        """å°†ä¸€ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹æŒ‰æ–‡ä»¶æ‹†åˆ†å¹¶ä¿å­˜"""
        file_groups = defaultdict(lambda: {'logits': [], 'indices': [], 'bin_path': None, 'pkl_path': None})
        
        start_idx = 0
        for i, end_idx in enumerate(offsets):
            # è·å–æ–‡ä»¶å
            f = bin_files[i]
            bin_basename = (f if isinstance(f, str) else str(f))
            if bin_basename.endswith('.bin'):
                bin_basename = bin_basename[:-4]
            
            # åˆ‡ç‰‡
            file_groups[bin_basename]['logits'].append(logits[start_idx:end_idx])
            file_groups[bin_basename]['indices'].append(indices[start_idx:end_idx])
            
            # è·¯å¾„
            if file_groups[bin_basename]['bin_path'] is None:
                if isinstance(bin_paths, list) and i < len(bin_paths):
                    file_groups[bin_basename]['bin_path'] = bin_paths[i]
                if isinstance(pkl_paths, list) and i < len(pkl_paths):
                    file_groups[bin_basename]['pkl_path'] = pkl_paths[i]
            
            start_idx = end_idx
            
        # ä¿å­˜
        for bin_basename, data in file_groups.items():
            if not data['logits']: continue
            
            save_path = os.path.join(self.temp_dir, f"{bin_basename}_batch_{batch_idx}.pred.tmp")
            save_dict = {
                'logits': torch.cat(data['logits'], dim=0),
                'indices': torch.cat(data['indices'], dim=0),
                'bin_file': bin_basename,
            }
            if data['bin_path']: save_dict['bin_path'] = data['bin_path']
            if data['pkl_path']: save_dict['pkl_path'] = data['pkl_path']
            
            torch.save(save_dict, save_path)
            
        if batch_idx % 10 == 0:
            pl_module.print(f"[SemanticPredictLasWriter] Batch {batch_idx}: ä¿å­˜äº† {len(file_groups)} ä¸ªæ–‡ä»¶çš„é¢„æµ‹")

    def _ensure_num_classes(self, pl_module):
        if self.num_classes != -1: return
        
        try:
            if hasattr(pl_module, 'head'):
                if hasattr(pl_module.head, 'out_channels'):
                    self.num_classes = pl_module.head.out_channels
                elif hasattr(pl_module.head, 'num_classes'):
                    self.num_classes = pl_module.head.num_classes
            elif hasattr(pl_module, 'num_classes'):
                self.num_classes = pl_module.num_classes
            pl_module.print(f"[SemanticPredictLasWriter] ä»æ¨¡å‹æ¨æ–­ç±»åˆ«æ•°: {self.num_classes}")
        except Exception:
            print("é”™è¯¯: æ— æ³•ä»æ¨¡å‹æ¨æ–­ num_classesï¼Œè¯·æ˜¾å¼æŒ‡å®š")

    def _group_temp_files(self, tmp_files):
        groups = defaultdict(list)
        for f in tmp_files:
            basename = os.path.basename(f).split('_batch_')[0]
            groups[basename].append(f)
        return groups

    def _cleanup_temp_files(self, tmp_files, pl_module):
        pl_module.print(f"\n[SemanticPredictLasWriter] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for f in tmp_files:
            try:
                if os.path.exists(f): os.remove(f)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åˆ é™¤ {f}: {e}")
        
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                pl_module.print(f"[SemanticPredictLasWriter] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹")
        except Exception as e:
            pl_module.print(f"è­¦å‘Š: æ¸…ç†æ–‡ä»¶å¤¹å¤±è´¥: {e}")
        pl_module.print(f"[SemanticPredictLasWriter] æ‰€æœ‰é¢„æµ‹å·²ä¿å­˜åˆ° {self.output_dir}")
        pl_module.print("="*70)

    def _process_single_bin_file(self, bin_basename, tmp_files, trainer, pl_module):
        # 1. è·å–è·¯å¾„
        bin_path, pkl_path = self._get_file_paths(bin_basename, tmp_files, trainer, pl_module)
        if not bin_path or not pkl_path: return

        # 2. åŠ è½½å…ƒæ•°æ®å’Œç‚¹äº‘
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        point_data = np.memmap(bin_path, dtype=metadata['dtype'], mode='r')
        num_points = len(point_data)
        
        # 3. æŠ•ç¥¨
        final_preds, mean_logits, counts = self._perform_voting(tmp_files, num_points, pl_module)
        
        # 4. æ˜ å°„
        if self.reverse_class_mapping:
            final_preds = self._apply_mapping(final_preds, pl_module)
            
        # 5. ä¿å­˜ LAS
        xyz = np.stack([point_data['X'], point_data['Y'], point_data['Z']], axis=1).astype(np.float64)
        metadata['_bin_path'] = bin_path
        
        self._save_las_file(
            os.path.join(self.output_dir, f"{bin_basename}.las"),
            xyz, final_preds, metadata, pl_module
        )
        
        # 6. ä¿å­˜ Logits
        if self.save_logits:
            np.savez_compressed(
                os.path.join(self.output_dir, f"{bin_basename}_logits.npz"),
                logits=mean_logits.numpy(),
                predictions=final_preds,
                counts=counts.numpy()
            )

    def _get_file_paths(self, bin_basename, tmp_files, trainer, pl_module):
        # å°è¯•ä»ä¸´æ—¶æ–‡ä»¶è·å–
        if tmp_files:
            try:
                # æ˜¾å¼è®¾ç½® weights_only=False ä»¥æ¶ˆé™¤ FutureWarning
                # æˆ‘ä»¬éœ€è¦åŠ è½½åŒ…å«è·¯å¾„å­—ç¬¦ä¸²çš„å­—å…¸ï¼Œè¿™æ˜¯å®‰å…¨çš„ï¼ˆå› ä¸ºæ˜¯æˆ‘ä»¬åœ¨ write_on_batch_end ä¸­ç”Ÿæˆçš„ï¼‰
                data = torch.load(tmp_files[0], weights_only=False)
                if 'bin_path' in data and 'pkl_path' in data:
                    bp = data['bin_path']
                    pp = data['pkl_path']
                    return (str(bp[0]) if isinstance(bp, list) else str(bp)), \
                           (str(pp[0]) if isinstance(pp, list) else str(pp))
            except Exception:
                pass
        
        # åå¤‡æ–¹æ¡ˆ
        return self._find_bin_pkl_paths(bin_basename, trainer)

    def _perform_voting(self, tmp_files, num_points, pl_module):
        logits_sum = torch.zeros((num_points, self.num_classes), dtype=torch.float32)
        counts = torch.zeros(num_points, dtype=torch.int32)
        
        for f in tmp_files:
            try:
                # æ˜¾å¼è®¾ç½® weights_only=False ä»¥æ¶ˆé™¤ FutureWarning
                d = torch.load(f, weights_only=False)
                # ç¡®ä¿ float32
                logits_sum.index_add_(0, d['indices'].long(), d['logits'].float())
                counts.index_add_(0, d['indices'].long(), torch.ones(len(d['indices']), dtype=torch.int32))
            except Exception as e:
                pl_module.print(f"    è­¦å‘Š: åŠ è½½ {f} å¤±è´¥: {e}")
                
        # è®¡ç®—å¹³å‡
        mask = (counts == 0)
        counts[mask] = 1
        mean_logits = logits_sum / counts.unsqueeze(-1)
        
        # Argmax
        if mean_logits.ndim == 2 and mean_logits.size(1) > 1:
            preds = torch.argmax(mean_logits, dim=1).numpy().astype(np.uint8)
        else:
            preds = mean_logits.squeeze().numpy().astype(np.uint8)
            
        if mask.any():
            preds[mask.numpy()] = 0
            
        return preds, mean_logits, counts

    def _apply_mapping(self, preds, pl_module):
        pl_module.print(f"  - åº”ç”¨åå‘ç±»åˆ«æ˜ å°„")
        max_label = max(self.reverse_class_mapping.keys())
        mapping = np.arange(max_label + 1)
        for k, v in self.reverse_class_mapping.items():
            mapping[k] = v
        return mapping[preds]

    def _find_bin_pkl_paths(self, bin_basename: str, trainer: 'pl.Trainer') -> tuple:
        """
        æ ¹æ® bin_basename æŸ¥æ‰¾å¯¹åº”çš„ bin å’Œ pkl æ–‡ä»¶è·¯å¾„
        
        Args:
            bin_basename: bin æ–‡ä»¶çš„åŸºç¡€åç§°
            trainer: Trainer å¯¹è±¡
            
        Returns:
            (bin_path, pkl_path) å…ƒç»„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› (None, None)
        """
        try:
            dataset = trainer.predict_dataloaders.dataset
            
            # åœ¨ data_list ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            for sample_info in dataset.data_list:
                bin_path = Path(sample_info['bin_path'])
                if bin_path.stem == bin_basename:
                    pkl_path = Path(sample_info['pkl_path'])
                    return str(bin_path), str(pkl_path)
            
            # å¦‚æœåœ¨ data_list ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä» data_root æœç´¢
            data_root = Path(dataset.data_root) if not isinstance(dataset.data_root, (list, tuple)) else Path(dataset.data_root[0]).parent
            
            bin_path = data_root / f"{bin_basename}.bin"
            pkl_path = data_root / f"{bin_basename}.pkl"
            
            if bin_path.exists() and pkl_path.exists():
                return str(bin_path), str(pkl_path)
            
            return None, None
            
        except Exception as e:
            print(f"é”™è¯¯: æŸ¥æ‰¾ bin/pkl æ–‡ä»¶å¤±è´¥: {e}")
            return None, None
    
    def _save_las_file(
        self,
        las_path: str,
        xyz: np.ndarray,
        classification: np.ndarray,
        metadata: Dict[str, Any],
        pl_module: 'pl.LightningModule'
    ):
        """
        ä¿å­˜ç‚¹äº‘ä¸º .las æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹ LAS å¤´ä¿¡æ¯å’Œæ‰€æœ‰ç‚¹å±æ€§
        
        Args:
            las_path: è¾“å‡º .las æ–‡ä»¶è·¯å¾„
            xyz: [N, 3] ç‚¹åæ ‡
            classification: [N] åˆ†ç±»æ ‡ç­¾ï¼ˆé¢„æµ‹ç»“æœï¼‰
            metadata: ä» pkl æ–‡ä»¶åŠ è½½çš„å…ƒæ•°æ® (åŒ…å« header_info)
            pl_module: PyTorch Lightning Module
        """
        
        try:
            # 1. ä» metadata ä¸­æ¢å¤ LAS å¤´ä¿¡æ¯
            if 'header_info' in metadata:
                header_info = metadata['header_info']
                
                # åˆ›å»º LAS å¤´
                point_format = header_info.get('point_format', 3)
                version_str = header_info.get('version', '1.2')
                
                # è§£æç‰ˆæœ¬å­—ç¬¦ä¸²
                if isinstance(version_str, str):
                    version_parts = version_str.split('.')
                    if len(version_parts) == 2:
                        major, minor = int(version_parts[0]), int(version_parts[1])
                    else:
                        major, minor = 1, 2
                else:
                    major, minor = 1, 2
                
                header = laspy.LasHeader(point_format=point_format, version=f"{major}.{minor}")
                
                # è®¾ç½®ç¼©æ”¾å’Œåç§»
                header.offsets = [
                    header_info.get('x_offset', 0),
                    header_info.get('y_offset', 0),
                    header_info.get('z_offset', 0)
                ]
                header.scales = [
                    header_info.get('x_scale', 0.01),
                    header_info.get('y_scale', 0.01),
                    header_info.get('z_scale', 0.01)
                ]
                
                # æ¢å¤å…¶ä»–å¤´ä¿¡æ¯
                if 'system_identifier' in header_info:
                    header.system_identifier = header_info['system_identifier']
                if 'generating_software' in header_info:
                    header.generating_software = header_info['generating_software']
                
                # æ¢å¤ VLRs (Variable Length Records) - åŒ…å«åæ ‡ç³»ä¿¡æ¯
                if 'vlrs' in header_info and header_info['vlrs']:
                    for vlr_dict in header_info['vlrs']:
                        try:
                            vlr = laspy.VLR(
                                user_id=vlr_dict['user_id'],
                                record_id=vlr_dict['record_id'],
                                description=vlr_dict['description'],
                                record_data=vlr_dict.get('record_data', b'')
                            )
                            header.vlrs.append(vlr)
                        except Exception as e:
                            pl_module.print(f"    è­¦å‘Š: æ¢å¤ VLR å¤±è´¥: {e}")
                
                pl_module.print(f"    - ä½¿ç”¨åŸå§‹ LAS å¤´ä¿¡æ¯ (format {point_format}, version {major}.{minor})")
                
            else:
                # å¦‚æœæ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼
                pl_module.print("    è­¦å‘Š: å…ƒæ•°æ®ä¸­æ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼")
                header = laspy.LasHeader(point_format=3, version='1.2')
                header.offsets = xyz.min(axis=0)
                header.scales = np.array([0.001, 0.001, 0.001])
            
            # 2. åˆ›å»º LAS æ•°æ®
            las = laspy.LasData(header)
            
            # 3. è®¾ç½®åæ ‡ (laspy ä¼šè‡ªåŠ¨åº”ç”¨ scale å’Œ offset)
            las.x = xyz[:, 0]
            las.y = xyz[:, 1]
            las.z = xyz[:, 2]
            
            # 4. ğŸ”¥ ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤æ‰€æœ‰å¯ç”¨å±æ€§
            if 'dtype' in metadata:
                dtype = metadata['dtype']
                
                # è·å– bin æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä» metadata ä¸­çš„ _bin_pathï¼‰
                bin_path = metadata.get('_bin_path', None)
                
                if bin_path and Path(bin_path).exists():
                    pl_module.print(f"    - ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤å±æ€§: {Path(bin_path).name}")
                    
                    # ä½¿ç”¨ memmap åŠ è½½åŸå§‹æ•°æ®
                    point_data = np.memmap(bin_path, dtype=dtype, mode='r')
                    
                    # æ¢å¤å„ä¸ªå±æ€§ï¼ˆæ ¹æ® dtype ä¸­çš„å­—æ®µï¼‰
                    field_names = [name for name, _ in dtype]
                    
                    # å¼ºåº¦ (Intensity)
                    if 'intensity' in field_names:
                        las.intensity = point_data['intensity']
                        pl_module.print(f"      âœ“ æ¢å¤ intensity")
                    
                    # å›æ³¢ä¿¡æ¯ (Return Number, Number of Returns)
                    if 'return_number' in field_names:
                        las.return_number = point_data['return_number']
                        pl_module.print(f"      âœ“ æ¢å¤ return_number")
                    if 'number_of_returns' in field_names:
                        las.number_of_returns = point_data['number_of_returns']
                        pl_module.print(f"      âœ“ æ¢å¤ number_of_returns")
                    
                    # æ‰«æè§’åº¦ (Scan Angle)
                    if 'scan_angle_rank' in field_names:
                        las.scan_angle_rank = point_data['scan_angle_rank']
                        pl_module.print(f"      âœ“ æ¢å¤ scan_angle_rank")
                    elif 'scan_angle' in field_names:
                        las.scan_angle = point_data['scan_angle']
                        pl_module.print(f"      âœ“ æ¢å¤ scan_angle")
                    
                    # ç”¨æˆ·æ•°æ® (User Data)
                    if 'user_data' in field_names:
                        las.user_data = point_data['user_data']
                        pl_module.print(f"      âœ“ æ¢å¤ user_data")
                    
                    # ç‚¹æº ID (Point Source ID)
                    if 'point_source_id' in field_names:
                        las.point_source_id = point_data['point_source_id']
                        pl_module.print(f"      âœ“ æ¢å¤ point_source_id")
                    
                    # GPS æ—¶é—´ (GPS Time)
                    if 'gps_time' in field_names:
                        las.gps_time = point_data['gps_time']
                        pl_module.print(f"      âœ“ æ¢å¤ gps_time")
                    
                    # RGB é¢œè‰² (å¦‚æœ point_format æ”¯æŒ)
                    if header.point_format.id in [2, 3, 5, 7, 8, 10]:
                        if 'red' in field_names and 'green' in field_names and 'blue' in field_names:
                            las.red = point_data['red']
                            las.green = point_data['green']
                            las.blue = point_data['blue']
                            pl_module.print(f"      âœ“ æ¢å¤ RGB é¢œè‰²")
                        
                        # NIR (è¿‘çº¢å¤–) - å¦‚æœæ”¯æŒ
                        if 'nir' in field_names and header.point_format.id in [8, 10]:
                            las.nir = point_data['nir']
                            pl_module.print(f"      âœ“ æ¢å¤ NIR")
                    
                    # å…¶ä»–å¯èƒ½çš„å­—æ®µå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ 
                    
                else:
                    pl_module.print(f"    è­¦å‘Š: æœªæ‰¾åˆ°åŸå§‹ bin æ–‡ä»¶: {bin_path}")
                    pl_module.print(f"    åªä¿å­˜åæ ‡å’Œåˆ†ç±»æ ‡ç­¾")
            
            # 5. è®¾ç½®é¢„æµ‹çš„åˆ†ç±»æ ‡ç­¾ï¼ˆè¦†ç›–åŸå§‹åˆ†ç±»ï¼‰
            las.classification = classification
            pl_module.print(f"      âœ“ è®¾ç½®é¢„æµ‹åˆ†ç±»æ ‡ç­¾")
            
            # 6. å†™å…¥æ–‡ä»¶
            las.write(las_path)
            
        except Exception as e:
            pl_module.print(f"    é”™è¯¯: ä¿å­˜ LAS æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


class AutoEmptyCacheCallback(Callback):
    """
    è‡ªåŠ¨æ˜¾å­˜æ¸…ç†å›è°ƒå‡½æ•°
    
    é€»è¾‘ï¼š
    1. å®šæœŸæ¸…ç†ï¼šä½œä¸ºå…œåº•ã€‚
    2. æ™ºèƒ½æ£€æµ‹ï¼šä¸€æ—¦å‘ç°å½“å‰ Batch è€—æ—¶å¼‚å¸¸ï¼ˆç»å¯¹æˆ–ç›¸å¯¹ï¼‰ï¼Œç«‹å³æ¸…ç†ã€‚
    3. æ— å†·å´æœŸï¼šåªè¦æ…¢ï¼Œå°±å°è¯•æ•‘ï¼Œä¼˜å…ˆä¿è¯é€Ÿåº¦æ¢å¤ã€‚
    """
    def __init__(
        self, 
        slowdown_threshold: float = 3.0,   # ç›¸å¯¹é˜ˆå€¼
        absolute_threshold: float = None,  # ç»å¯¹é˜ˆå€¼ (ç§’)
        clear_interval: int = 0,           # å®šæœŸæ¸…ç†
        warmup_steps: int = 50,            # é¢„çƒ­æ­¥æ•°
        verbose: bool = True
    ):
        super().__init__()
        self.config = {
            'slowdown': slowdown_threshold,
            'absolute': absolute_threshold,
            'interval': clear_interval,
            'warmup': warmup_steps
        }
        self.verbose = verbose
        
        # çŠ¶æ€è¿½è¸ª
        self.states = {} 

    def _get_state(self, stage):
        if stage not in self.states:
            self.states[stage] = {
                'start_time': 0.0,
                'avg_time': 0.0
            }
        return self.states[stage]

    def _on_batch_start(self, stage):
        state = self._get_state(stage)
        state['start_time'] = time.time()

    def _on_batch_end(self, trainer, batch_idx, stage):
        state = self._get_state(stage)
        duration = time.time() - state['start_time']
        
        # Trainé˜¶æ®µç”¨ global_step, å…¶ä»–é˜¶æ®µç”¨ batch_idx (ä»…ç”¨äºæ—¥å¿—æ˜¾ç¤º)
        current_step = trainer.global_step if stage == 'train' else batch_idx
        
        should_clear = False
        reason = ""

        # =========================================================
        # æ ¸å¿ƒé€»è¾‘
        # =========================================================

        # 1. ä¼˜å…ˆæ£€æŸ¥å®šæœŸæ¸…ç†
        if self.config['interval'] > 0 and (batch_idx + 1) % self.config['interval'] == 0:
            should_clear = True
            reason = "periodic"
            
        # 2. æ™ºèƒ½æ£€æµ‹ (å¦‚æœæ²¡æœ‰è§¦å‘å®šæœŸæ¸…ç†)
        else:
            # åˆ¤æ–­æ˜¯å¦å·²é¢„çƒ­ (æœ‰å†å²å¹³å‡å€¼ æˆ– è¶…è¿‡é¢„çƒ­æ­¥æ•°)
            is_warmed_up = (state['avg_time'] > 0) or (batch_idx > self.config['warmup'])
            
            # A. ç›¸å¯¹æ£€æµ‹: æ¯”å¹³å‡å€¼æ…¢ N å€
            if (is_warmed_up and 
                state['avg_time'] > 0 and 
                duration > state['avg_time'] * self.config['slowdown']):
                should_clear = True
                reason = f"slowdown ({duration:.2f}s vs avg {state['avg_time']:.2f}s)"
            
            # B. ç»å¯¹æ£€æµ‹: è¶…è¿‡ N ç§’
            elif (self.config['absolute'] is not None and 
                  duration > self.config['absolute']):
                should_clear = True
                reason = f"absolute limit ({duration:.2f}s > {self.config['absolute']}s)"

        # =========================================================
        # æ‰§è¡ŒåŠ¨ä½œ
        # =========================================================
        if should_clear:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                if self.verbose:
                    stage_name = stage.upper()
                    step_info = f"global_step={current_step}" if stage == 'train' else f"batch={batch_idx}"
                    trainer.print(f"\n[AutoCache][{stage_name}] ğŸ§¹ Cleared at {step_info}. Reason: {reason}")

        # æ›´æ–°å¹³å‡å€¼ (EMA)
        # ç­–ç•¥ï¼šåªæœ‰åœ¨ã€æœªè§¦å‘æ¸…ç†ã€‘(å³è®¤ä¸ºæ˜¯æ­£å¸¸Batch) æ—¶æ‰æ›´æ–°å¹³å‡å€¼
        # è¿™æ ·å¯ä»¥é˜²æ­¢å¼‚å¸¸æ…¢çš„ Batch æ±¡æŸ“å¹³å‡å€¼ï¼Œä¿æŒæ£€æµ‹çš„æ•é”åº¦
        if not should_clear:
            if state['avg_time'] == 0:
                state['avg_time'] = duration
            else:
                # alpha = 0.05
                state['avg_time'] = state['avg_time'] * 0.95 + duration * 0.05

    # ================= é’©å­ç»‘å®š (ä¿æŒä¸å˜) =================
    
    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):
        self._on_batch_start('train')

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        self._on_batch_end(trainer, batch_idx, 'train')

    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):
        self._on_batch_start('val')

    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._on_batch_end(trainer, batch_idx, 'val')

    def on_test_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):
        self._on_batch_start('test')

    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._on_batch_end(trainer, batch_idx, 'test')

    def on_predict_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):
        self._on_batch_start('predict')

    def on_predict_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._on_batch_end(trainer, batch_idx, 'predict')