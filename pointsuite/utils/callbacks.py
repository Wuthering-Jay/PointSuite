import torch
import numpy as np
import os
import glob
import pickle
import pytorch_lightning as pl
from pytorch_lightning.callbacks import BasePredictionWriter
from typing import List, Any, Dict, Optional, Union
from pathlib import Path
from collections import defaultdict
import time
from pytorch_lightning.callbacks import Callback
import datetime

from .mapping import ClassMapping, ClassMappingInput, create_reverse_mapping
from .logger import log_warning, log_error, log_info, log_success, Colors


# å¯¼å…¥ laspy (æ‚¨éœ€è¦ 'pip install laspy')
try:
    import laspy
except ImportError:
    log_warning("'laspy' åº“æœªå®‰è£…ã€‚PredictionWriter å°†æ— æ³•ä¿å­˜ .las æ–‡ä»¶ã€‚")
    log_warning("è¯·è¿è¡Œ: pip install laspy")


# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

def create_reverse_class_mapping(class_mapping: ClassMappingInput) -> Optional[Dict[int, int]]:
    """
    ä» class_mapping åˆ›å»º reverse_class_mapping
    
    Args:
        class_mapping: ç±»åˆ«æ˜ å°„é…ç½®ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
            - None: è¿”å› None
            - Dict[int, int]: åŸå§‹æ ‡ç­¾ -> è¿ç»­æ ‡ç­¾çš„æ˜ å°„
            - List[int]: åŸå§‹ç±»åˆ«IDåˆ—è¡¨
    
    Returns:
        reverse_class_mapping: è¿ç»­æ ‡ç­¾ -> åŸå§‹æ ‡ç­¾çš„æ˜ å°„
                              ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
    
    Example:
        >>> class_mapping = {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
        >>> reverse_mapping = create_reverse_class_mapping(class_mapping)
        >>> print(reverse_mapping)
        {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
        
        >>> # ä¹Ÿæ”¯æŒåˆ—è¡¨å½¢å¼
        >>> class_mapping = [0, 1, 2, 6, 9]  # è‡ªåŠ¨æ˜ å°„ä¸º {0:0, 1:1, 2:2, 6:3, 9:4}
        >>> reverse_mapping = create_reverse_class_mapping(class_mapping)
        >>> print(reverse_mapping)
        {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
    """
    return create_reverse_mapping(class_mapping)


class SemanticPredictLasWriter_old(BasePredictionWriter):
    """
    ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ PredictionWriter å›è°ƒ (é€‚é… bin+pkl æ•°æ®æ ¼å¼)
    
    é‡å‘½åè‡ª SegmentationWriterï¼Œä¸“ä¸º PointSuite çš„ bin+pkl æ•°æ®ç»“æ„è®¾è®¡ã€‚
    è´Ÿè´£å°†æ¨¡å‹é¢„æµ‹ç»“æœæµå¼å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œå¹¶åœ¨é¢„æµ‹ç»“æŸååˆå¹¶ã€æŠ•ç¥¨å¹¶ä¿å­˜ä¸º LAS æ–‡ä»¶ã€‚

    ä¸»è¦åŠŸèƒ½:
    1. æµå¼å†™å…¥: é˜²æ­¢å¤§è§„æ¨¡ç‚¹äº‘é¢„æµ‹æ—¶çš„ OOMã€‚
    2. æŠ•ç¥¨æœºåˆ¶: å¯¹é‡å é¢„æµ‹è¿›è¡Œ logits å¹³å‡æŠ•ç¥¨ã€‚
    3. å®Œæ•´æ€§æ¢å¤: ä»åŸå§‹ bin/pkl æ¢å¤åæ ‡å’Œå±æ€§ã€‚
    4. æ ¼å¼ä¿æŒ: ä¿ç•™åŸå§‹ LAS å¤´ä¿¡æ¯å’Œåæ ‡ç³»ã€‚
    """
    
    def __init__(self, 
                 output_dir: str, 
                 write_interval: str = "batch", 
                 num_classes: int = -1,
                 save_logits: bool = False,
                 reverse_class_mapping: Optional[Dict[int, int]] = None,
                 auto_infer_reverse_mapping: bool = True):
        super().__init__(write_interval)
        self.output_dir = output_dir
        self.temp_dir = os.path.join(self.output_dir, "temp_predictions")
        self.num_classes = num_classes
        self.save_logits = save_logits
        self.reverse_class_mapping = reverse_class_mapping
        self.auto_infer_reverse_mapping = auto_infer_reverse_mapping
        self._mapping_inferred = False
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)
    
    def on_predict_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """é¢„æµ‹å¼€å§‹å‰çš„åˆå§‹åŒ–å·¥ä½œï¼Œä¸»è¦æ˜¯æ¨æ–­ç±»åˆ«æ˜ å°„"""
        self._infer_class_mapping(trainer, pl_module)

    def write_on_batch_end(
        self, 
        trainer: 'pl.Trainer', 
        pl_module: 'pl.LightningModule', 
        prediction: Dict[str, torch.Tensor], 
        batch_indices: List[int], 
        batch: Any, 
        batch_idx: int, 
        dataloader_idx: int
    ):
        """æ¯ä¸ªæ‰¹æ¬¡ç»“æŸæ—¶ï¼Œå°†é¢„æµ‹ç»“æœå†™å…¥ä¸´æ—¶æ–‡ä»¶"""
        if not self._validate_prediction(prediction, batch_idx):
            return
        
        # 1. å‡†å¤‡æ•°æ®
        bin_files = prediction['bin_file']
        logits = prediction['logits'].cpu().float() # ç¡®ä¿ float32
        indices = prediction['indices'].cpu()
        
        bin_paths = prediction.get('bin_path', [None] * len(bin_files))
        pkl_paths = prediction.get('pkl_path', [None] * len(bin_files))
        
        # è·å– offset
        offsets = batch['offset'].cpu().numpy() if 'offset' in batch else [len(logits)]
        
        # 2. æŒ‰æ–‡ä»¶åˆ†ç»„å¹¶ä¿å­˜
        self._save_batch_predictions(
            bin_files, logits, indices, bin_paths, pkl_paths, offsets, batch_idx, pl_module
        )

    def on_predict_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """é¢„æµ‹ç»“æŸåçš„æ±‡æ€»å¤„ç†"""
        self._ensure_num_classes(pl_module)
        pl_module.print(f"\n[SemanticPredictLasWriter] é¢„æµ‹å®Œæˆï¼Œå¼€å§‹æ‹¼æ¥å’ŒæŠ•ç¥¨...")
        
        tmp_files = sorted(glob.glob(os.path.join(self.temp_dir, "*.pred.tmp")))
        if not tmp_files:
            pl_module.print("[SemanticPredictLasWriter] è­¦å‘Š: æœªæ‰¾åˆ°ä¸´æ—¶é¢„æµ‹æ–‡ä»¶")
            return
            
        # æŒ‰ bin æ–‡ä»¶åˆ†ç»„
        bin_file_groups = self._group_temp_files(tmp_files)
        pl_module.print(f"[SemanticPredictLasWriter] æ£€æµ‹åˆ° {len(bin_file_groups)} ä¸ªå”¯ä¸€ bin æ–‡ä»¶:")
        for name in sorted(bin_file_groups.keys()):
            pl_module.print(f"  - {name}: {len(bin_file_groups[name])} ä¸ªæ‰¹æ¬¡")
        
        # è·Ÿè¸ªæˆåŠŸå’Œå¤±è´¥
        success_files = []
        failed_files = []
        
        try:
            for bin_basename, file_list in bin_file_groups.items():
                pl_module.print(f"\n[SemanticPredictLasWriter] å¤„ç† bin æ–‡ä»¶: {bin_basename} ({len(file_list)} ä¸ªæ‰¹æ¬¡)")
                try:
                    self._process_single_bin_file(bin_basename, file_list, trainer, pl_module)
                    success_files.append(bin_basename)
                except Exception as e:
                    pl_module.print(f"!!! é”™è¯¯: å¤„ç† {bin_basename} æ—¶å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    failed_files.append((bin_basename, str(e)))
        finally:
            self._cleanup_temp_files(tmp_files, pl_module)
        
        # è¾“å‡ºæ€»ç»“
        pl_module.print(f"\n[SemanticPredictLasWriter] å¤„ç†æ€»ç»“:")
        pl_module.print(f"  - æˆåŠŸ: {len(success_files)} ä¸ªæ–‡ä»¶")
        if failed_files:
            pl_module.print(f"  - å¤±è´¥: {len(failed_files)} ä¸ªæ–‡ä»¶")
            for name, err in failed_files:
                pl_module.print(f"    - {name}: {err}")

    # ================= å†…éƒ¨è¾…åŠ©æ–¹æ³• =================

    def _infer_class_mapping(self, trainer, pl_module):
        """æ¨æ–­åå‘ç±»åˆ«æ˜ å°„"""
        if self.reverse_class_mapping is not None:
            pl_module.print(f"[SemanticPredictLasWriter] ä½¿ç”¨ç”¨æˆ·æä¾›çš„ reverse_class_mapping: {self.reverse_class_mapping}")
            return
        
        if not self.auto_infer_reverse_mapping:
            return
            
        # å°è¯•ä»æ¨¡å‹ checkpoint è·å–
        try:
            if hasattr(pl_module, 'hparams') and hasattr(pl_module.hparams, 'class_mapping'):
                mapping = pl_module.hparams.class_mapping
                if mapping:
                    # ä½¿ç”¨ create_reverse_mapping å¤„ç† Dict æˆ– List
                    self.reverse_class_mapping = create_reverse_mapping(mapping)
                    self._mapping_inferred = True
                    pl_module.print(f"[SemanticPredictLasWriter] ä»æ¨¡å‹ checkpoint åŠ è½½ reverse_class_mapping")
                    return
        except Exception:
            pass
            
        # å°è¯•ä» DataModule è·å–
        try:
            datamodule = trainer.datamodule
            if hasattr(datamodule, 'class_mapping') and datamodule.class_mapping:
                # ä½¿ç”¨ create_reverse_mapping å¤„ç† Dict æˆ– List
                self.reverse_class_mapping = create_reverse_mapping(datamodule.class_mapping)
                self._mapping_inferred = True
                pl_module.print(f"[SemanticPredictLasWriter] ä» DataModule æ¨æ–­ reverse_class_mapping")
            else:
                pl_module.print(f"[SemanticPredictLasWriter] æœªæ‰¾åˆ° class_mappingï¼Œä½¿ç”¨è¿ç»­æ ‡ç­¾")
        except Exception as e:
            pl_module.print(f"[SemanticPredictLasWriter] è­¦å‘Š: æ— æ³•æ¨æ–­ reverse_class_mapping: {e}")

    def _validate_prediction(self, prediction, batch_idx):
        if 'logits' not in prediction or 'indices' not in prediction:
            log_warning(f"predict_step å¿…é¡»è¿”å› 'logits' å’Œ 'indices'ã€‚è·³è¿‡æ‰¹æ¬¡ {batch_idx}")
            return False
        if 'bin_file' not in prediction or len(prediction['bin_file']) == 0:
            log_warning(f"batch {batch_idx} ç¼ºå°‘ bin_file ä¿¡æ¯ï¼Œè·³è¿‡")
            return False
        return True

    def _save_batch_predictions(self, bin_files, logits, indices, bin_paths, pkl_paths, offsets, batch_idx, pl_module):
        """å°†ä¸€ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹æŒ‰æ–‡ä»¶æ‹†åˆ†å¹¶ä¿å­˜"""
        file_groups = defaultdict(lambda: {'logits': [], 'indices': [], 'bin_path': None, 'pkl_path': None})
        
        start_idx = 0
        for i, end_idx in enumerate(offsets):
            # è·å–æ–‡ä»¶å
            f = bin_files[i]
            bin_basename = (f if isinstance(f, str) else str(f))
            if bin_basename.endswith('.bin'):
                bin_basename = bin_basename[:-4]
            
            # åˆ‡ç‰‡
            file_groups[bin_basename]['logits'].append(logits[start_idx:end_idx])
            file_groups[bin_basename]['indices'].append(indices[start_idx:end_idx])
            
            # è·¯å¾„
            if file_groups[bin_basename]['bin_path'] is None:
                if isinstance(bin_paths, list) and i < len(bin_paths):
                    file_groups[bin_basename]['bin_path'] = bin_paths[i]
                if isinstance(pkl_paths, list) and i < len(pkl_paths):
                    file_groups[bin_basename]['pkl_path'] = pkl_paths[i]
            
            start_idx = end_idx
            
        # ä¿å­˜
        for bin_basename, data in file_groups.items():
            if not data['logits']: continue
            
            save_path = os.path.join(self.temp_dir, f"{bin_basename}_batch_{batch_idx}.pred.tmp")
            save_dict = {
                'logits': torch.cat(data['logits'], dim=0),
                'indices': torch.cat(data['indices'], dim=0),
                'bin_file': bin_basename,
            }
            if data['bin_path']: save_dict['bin_path'] = data['bin_path']
            if data['pkl_path']: save_dict['pkl_path'] = data['pkl_path']
            
            torch.save(save_dict, save_path)
            
        if batch_idx % 10 == 0:
            pl_module.print(f"[SemanticPredictLasWriter] Batch {batch_idx}: ä¿å­˜äº† {len(file_groups)} ä¸ªæ–‡ä»¶çš„é¢„æµ‹")

    def _ensure_num_classes(self, pl_module):
        if self.num_classes != -1: return
        
        try:
            if hasattr(pl_module, 'head'):
                if hasattr(pl_module.head, 'out_channels'):
                    self.num_classes = pl_module.head.out_channels
                elif hasattr(pl_module.head, 'num_classes'):
                    self.num_classes = pl_module.head.num_classes
            elif hasattr(pl_module, 'num_classes'):
                self.num_classes = pl_module.num_classes
            pl_module.print(f"[SemanticPredictLasWriter] ä»æ¨¡å‹æ¨æ–­ç±»åˆ«æ•°: {self.num_classes}")
        except Exception:
            log_error("æ— æ³•ä»æ¨¡å‹æ¨æ–­ num_classesï¼Œè¯·æ˜¾å¼æŒ‡å®š")

    def _group_temp_files(self, tmp_files):
        groups = defaultdict(list)
        for f in tmp_files:
            basename = os.path.basename(f).split('_batch_')[0]
            groups[basename].append(f)
        return groups

    def _cleanup_temp_files(self, tmp_files, pl_module):
        pl_module.print(f"\n[SemanticPredictLasWriter] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for f in tmp_files:
            try:
                if os.path.exists(f): os.remove(f)
            except Exception as e:
                log_warning(f"æ— æ³•åˆ é™¤ {f}: {e}")
        
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                pl_module.print(f"[SemanticPredictLasWriter] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹")
        except Exception as e:
            pl_module.print(f"è­¦å‘Š: æ¸…ç†æ–‡ä»¶å¤¹å¤±è´¥: {e}")
        pl_module.print(f"[SemanticPredictLasWriter] æ‰€æœ‰é¢„æµ‹å·²ä¿å­˜åˆ° {self.output_dir}")
        pl_module.print("="*70)

    def _process_single_bin_file(self, bin_basename, tmp_files, trainer, pl_module):
        # 1. è·å–è·¯å¾„
        bin_path, pkl_path = self._get_file_paths(bin_basename, tmp_files, trainer, pl_module)
        if not bin_path or not pkl_path: return

        # 2. åŠ è½½å…ƒæ•°æ®å’Œç‚¹äº‘
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        point_data = np.memmap(bin_path, dtype=metadata['dtype'], mode='r')
        num_points = len(point_data)
        
        # 3. æŠ•ç¥¨
        final_preds, mean_logits, counts = self._perform_voting(tmp_files, num_points, pl_module)
        
        # 4. æ˜ å°„
        if self.reverse_class_mapping:
            final_preds = self._apply_mapping(final_preds, pl_module)
            
        # 5. ä¿å­˜ LAS
        xyz = np.stack([point_data['X'], point_data['Y'], point_data['Z']], axis=1).astype(np.float64)
        metadata['_bin_path'] = bin_path
        
        self._save_las_file(
            os.path.join(self.output_dir, f"{bin_basename}.las"),
            xyz, final_preds, metadata, pl_module
        )
        
        # 6. ä¿å­˜ Logits
        if self.save_logits:
            np.savez_compressed(
                os.path.join(self.output_dir, f"{bin_basename}_logits.npz"),
                logits=mean_logits.numpy(),
                predictions=final_preds,
                counts=counts.numpy()
            )

    def _get_file_paths(self, bin_basename, tmp_files, trainer, pl_module):
        # å°è¯•ä»ä¸´æ—¶æ–‡ä»¶è·å–
        if tmp_files:
            try:
                # æ˜¾å¼è®¾ç½® weights_only=False ä»¥æ¶ˆé™¤ FutureWarning
                # æˆ‘ä»¬éœ€è¦åŠ è½½åŒ…å«è·¯å¾„å­—ç¬¦ä¸²çš„å­—å…¸ï¼Œè¿™æ˜¯å®‰å…¨çš„ï¼ˆå› ä¸ºæ˜¯æˆ‘ä»¬åœ¨ write_on_batch_end ä¸­ç”Ÿæˆçš„ï¼‰
                data = torch.load(tmp_files[0], weights_only=False)
                if 'bin_path' in data and 'pkl_path' in data:
                    bp = data['bin_path']
                    pp = data['pkl_path']
                    return (str(bp[0]) if isinstance(bp, list) else str(bp)), \
                           (str(pp[0]) if isinstance(pp, list) else str(pp))
            except Exception:
                pass
        
        # åå¤‡æ–¹æ¡ˆ
        return self._find_bin_pkl_paths(bin_basename, trainer)

    def _perform_voting(self, tmp_files, num_points, pl_module):
        logits_sum = torch.zeros((num_points, self.num_classes), dtype=torch.float32)
        counts = torch.zeros(num_points, dtype=torch.int32)
        
        for f in tmp_files:
            try:
                # æ˜¾å¼è®¾ç½® weights_only=False ä»¥æ¶ˆé™¤ FutureWarning
                d = torch.load(f, weights_only=False)
                # ç¡®ä¿ float32
                logits_sum.index_add_(0, d['indices'].long(), d['logits'].float())
                counts.index_add_(0, d['indices'].long(), torch.ones(len(d['indices']), dtype=torch.int32))
            except Exception as e:
                pl_module.print(f"    è­¦å‘Š: åŠ è½½ {f} å¤±è´¥: {e}")
                
        # è®¡ç®—å¹³å‡
        mask = (counts == 0)
        counts[mask] = 1
        mean_logits = logits_sum / counts.unsqueeze(-1)
        
        # Argmax
        if mean_logits.ndim == 2 and mean_logits.size(1) > 1:
            preds = torch.argmax(mean_logits, dim=1).numpy().astype(np.uint8)
        else:
            preds = mean_logits.squeeze().numpy().astype(np.uint8)
            
        if mask.any():
            preds[mask.numpy()] = 0
            
        return preds, mean_logits, counts

    def _apply_mapping(self, preds, pl_module):
        pl_module.print(f"  - åº”ç”¨åå‘ç±»åˆ«æ˜ å°„")
        max_label = max(self.reverse_class_mapping.keys())
        mapping = np.arange(max_label + 1)
        for k, v in self.reverse_class_mapping.items():
            mapping[k] = v
        return mapping[preds]

    def _find_bin_pkl_paths(self, bin_basename: str, trainer: 'pl.Trainer') -> tuple:
        """
        æ ¹æ® bin_basename æŸ¥æ‰¾å¯¹åº”çš„ bin å’Œ pkl æ–‡ä»¶è·¯å¾„
        
        Args:
            bin_basename: bin æ–‡ä»¶çš„åŸºç¡€åç§°
            trainer: Trainer å¯¹è±¡
            
        Returns:
            (bin_path, pkl_path) å…ƒç»„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› (None, None)
        """
        try:
            dataset = trainer.predict_dataloaders.dataset
            
            # åœ¨ data_list ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            for sample_info in dataset.data_list:
                bin_path = Path(sample_info['bin_path'])
                if bin_path.stem == bin_basename:
                    pkl_path = Path(sample_info['pkl_path'])
                    return str(bin_path), str(pkl_path)
            
            # å¦‚æœåœ¨ data_list ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä» data_root æœç´¢
            data_root = Path(dataset.data_root) if not isinstance(dataset.data_root, (list, tuple)) else Path(dataset.data_root[0]).parent
            
            bin_path = data_root / f"{bin_basename}.bin"
            pkl_path = data_root / f"{bin_basename}.pkl"
            
            if bin_path.exists() and pkl_path.exists():
                return str(bin_path), str(pkl_path)
            
            return None, None
            
        except Exception as e:
            log_error(f"æŸ¥æ‰¾ bin/pkl æ–‡ä»¶å¤±è´¥: {e}")
            return None, None
    
    def _save_las_file(
        self,
        las_path: str,
        xyz: np.ndarray,
        classification: np.ndarray,
        metadata: Dict[str, Any],
        pl_module: 'pl.LightningModule'
    ):
        """
        ä¿å­˜ç‚¹äº‘ä¸º .las æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹ LAS å¤´ä¿¡æ¯å’Œæ‰€æœ‰ç‚¹å±æ€§
        
        Args:
            las_path: è¾“å‡º .las æ–‡ä»¶è·¯å¾„
            xyz: [N, 3] ç‚¹åæ ‡
            classification: [N] åˆ†ç±»æ ‡ç­¾ï¼ˆé¢„æµ‹ç»“æœï¼‰
            metadata: ä» pkl æ–‡ä»¶åŠ è½½çš„å…ƒæ•°æ® (åŒ…å« header_info)
            pl_module: PyTorch Lightning Module
        """
        
        try:
            # 1. ä» metadata ä¸­æ¢å¤ LAS å¤´ä¿¡æ¯
            if 'header_info' in metadata:
                header_info = metadata['header_info']
                
                # åˆ›å»º LAS å¤´
                point_format = header_info.get('point_format', 3)
                version_str = header_info.get('version', '1.2')
                
                # è§£æç‰ˆæœ¬å­—ç¬¦ä¸²
                if isinstance(version_str, str):
                    version_parts = version_str.split('.')
                    if len(version_parts) == 2:
                        major, minor = int(version_parts[0]), int(version_parts[1])
                    else:
                        major, minor = 1, 2
                else:
                    major, minor = 1, 2
                
                header = laspy.LasHeader(point_format=point_format, version=f"{major}.{minor}")
                
                # è®¾ç½®ç¼©æ”¾å’Œåç§»
                header.offsets = [
                    header_info.get('x_offset', 0),
                    header_info.get('y_offset', 0),
                    header_info.get('z_offset', 0)
                ]
                header.scales = [
                    header_info.get('x_scale', 0.01),
                    header_info.get('y_scale', 0.01),
                    header_info.get('z_scale', 0.01)
                ]
                
                # æ¢å¤å…¶ä»–å¤´ä¿¡æ¯
                if 'system_identifier' in header_info:
                    header.system_identifier = header_info['system_identifier']
                if 'generating_software' in header_info:
                    header.generating_software = header_info['generating_software']
                
                # æ¢å¤ VLRs (Variable Length Records) - åŒ…å«åæ ‡ç³»ä¿¡æ¯
                if 'vlrs' in header_info and header_info['vlrs']:
                    for vlr_dict in header_info['vlrs']:
                        try:
                            vlr = laspy.VLR(
                                user_id=vlr_dict['user_id'],
                                record_id=vlr_dict['record_id'],
                                description=vlr_dict['description'],
                                record_data=vlr_dict.get('record_data', b'')
                            )
                            header.vlrs.append(vlr)
                        except Exception as e:
                            pl_module.print(f"    è­¦å‘Š: æ¢å¤ VLR å¤±è´¥: {e}")
                
                pl_module.print(f"    - ä½¿ç”¨åŸå§‹ LAS å¤´ä¿¡æ¯ (format {point_format}, version {major}.{minor})")
                
            else:
                # å¦‚æœæ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼
                pl_module.print("    è­¦å‘Š: å…ƒæ•°æ®ä¸­æ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼")
                header = laspy.LasHeader(point_format=3, version='1.2')
                header.offsets = xyz.min(axis=0)
                header.scales = np.array([0.001, 0.001, 0.001])
            
            # 2. åˆ›å»º LAS æ•°æ®
            las = laspy.LasData(header)
            
            # 3. è®¾ç½®åæ ‡ (laspy ä¼šè‡ªåŠ¨åº”ç”¨ scale å’Œ offset)
            las.x = xyz[:, 0]
            las.y = xyz[:, 1]
            las.z = xyz[:, 2]
            
            # 4. ğŸ”¥ ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤æ‰€æœ‰å¯ç”¨å±æ€§
            if 'dtype' in metadata:
                dtype = metadata['dtype']
                
                # è·å– bin æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä» metadata ä¸­çš„ _bin_pathï¼‰
                bin_path = metadata.get('_bin_path', None)
                
                if bin_path and Path(bin_path).exists():
                    pl_module.print(f"    - ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤å±æ€§: {Path(bin_path).name}")
                    
                    # ä½¿ç”¨ memmap åŠ è½½åŸå§‹æ•°æ®
                    point_data = np.memmap(bin_path, dtype=dtype, mode='r')
                    
                    # æ¢å¤å„ä¸ªå±æ€§ï¼ˆæ ¹æ® dtype ä¸­çš„å­—æ®µï¼‰
                    field_names = [name for name, _ in dtype]
                    
                    # å¼ºåº¦ (Intensity)
                    if 'intensity' in field_names:
                        las.intensity = point_data['intensity']
                        pl_module.print(f"      [OK] æ¢å¤ intensity")
                    
                    # å›æ³¢ä¿¡æ¯ (Return Number, Number of Returns)
                    if 'return_number' in field_names:
                        las.return_number = point_data['return_number']
                        pl_module.print(f"      [OK] æ¢å¤ return_number")
                    if 'number_of_returns' in field_names:
                        las.number_of_returns = point_data['number_of_returns']
                        pl_module.print(f"      [OK] æ¢å¤ number_of_returns")
                    
                    # æ‰«æè§’åº¦ (Scan Angle)
                    if 'scan_angle_rank' in field_names:
                        las.scan_angle_rank = point_data['scan_angle_rank']
                        pl_module.print(f"      [OK] æ¢å¤ scan_angle_rank")
                    elif 'scan_angle' in field_names:
                        las.scan_angle = point_data['scan_angle']
                        pl_module.print(f"      [OK] æ¢å¤ scan_angle")
                    
                    # ç”¨æˆ·æ•°æ® (User Data)
                    if 'user_data' in field_names:
                        las.user_data = point_data['user_data']
                        pl_module.print(f"      [OK] æ¢å¤ user_data")
                    
                    # ç‚¹æº ID (Point Source ID)
                    if 'point_source_id' in field_names:
                        las.point_source_id = point_data['point_source_id']
                        pl_module.print(f"      [OK] æ¢å¤ point_source_id")
                    
                    # GPS æ—¶é—´ (GPS Time)
                    if 'gps_time' in field_names:
                        las.gps_time = point_data['gps_time']
                        pl_module.print(f"      [OK] æ¢å¤ gps_time")
                    
                    # RGB é¢œè‰² (å¦‚æœ point_format æ”¯æŒ)
                    if header.point_format.id in [2, 3, 5, 7, 8, 10]:
                        if 'red' in field_names and 'green' in field_names and 'blue' in field_names:
                            las.red = point_data['red']
                            las.green = point_data['green']
                            las.blue = point_data['blue']
                            pl_module.print(f"      [OK] æ¢å¤ RGB é¢œè‰²")
                        
                        # NIR (è¿‘çº¢å¤–) - å¦‚æœæ”¯æŒ
                        if 'nir' in field_names and header.point_format.id in [8, 10]:
                            las.nir = point_data['nir']
                            pl_module.print(f"      [OK] æ¢å¤ NIR")
                    
                    # å…¶ä»–å¯èƒ½çš„å­—æ®µå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ 
                    
                else:
                    pl_module.print(f"    è­¦å‘Š: æœªæ‰¾åˆ°åŸå§‹ bin æ–‡ä»¶: {bin_path}")
                    pl_module.print(f"    åªä¿å­˜åæ ‡å’Œåˆ†ç±»æ ‡ç­¾")
            
            # 5. è®¾ç½®é¢„æµ‹çš„åˆ†ç±»æ ‡ç­¾ï¼ˆè¦†ç›–åŸå§‹åˆ†ç±»ï¼‰
            las.classification = classification
            pl_module.print(f"      [OK] è®¾ç½®é¢„æµ‹åˆ†ç±»æ ‡ç­¾")
            
            # 6. å†™å…¥æ–‡ä»¶
            las.write(las_path)
            
        except Exception as e:
            pl_module.print(f"    é”™è¯¯: ä¿å­˜ LAS æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise


class AutoEmptyCacheCallback(Callback):
    """
    è‡ªåŠ¨æ˜¾å­˜æ¸…ç†å›è°ƒå‡½æ•°
    
    é€»è¾‘ï¼š
    1. å®šæœŸæ¸…ç†ï¼šä½œä¸ºå…œåº•ã€‚
    2. æ™ºèƒ½æ£€æµ‹ï¼šä¸€æ—¦å‘ç°å½“å‰ Batch è€—æ—¶å¼‚å¸¸ï¼ˆç»å¯¹æˆ–ç›¸å¯¹ï¼‰ï¼Œç«‹å³æ¸…ç†ã€‚
    3. æ— å†·å´æœŸï¼šåªè¦æ…¢ï¼Œå°±å°è¯•æ•‘ï¼Œä¼˜å…ˆä¿è¯é€Ÿåº¦æ¢å¤ã€‚
    """
    def __init__(
        self, 
        slowdown_threshold: float = 3.0,   # ç›¸å¯¹é˜ˆå€¼
        absolute_threshold: float = None,  # ç»å¯¹é˜ˆå€¼ (ç§’)
        clear_interval: int = 0,           # å®šæœŸæ¸…ç†
        warmup_steps: int = 50,            # é¢„çƒ­æ­¥æ•°
        verbose: bool = True
    ):
        super().__init__()
        self.config = {
            'slowdown': slowdown_threshold,
            'absolute': absolute_threshold,
            'interval': clear_interval,
            'warmup': warmup_steps
        }
        self.verbose = verbose
        
        # çŠ¶æ€è¿½è¸ª
        self.states = {} 

    def _get_state(self, stage):
        if stage not in self.states:
            self.states[stage] = {
                'start_time': 0.0,
                'avg_time': 0.0
            }
        return self.states[stage]

    def _on_batch_start(self, stage):
        state = self._get_state(stage)
        state['start_time'] = time.time()

    def _on_batch_end(self, trainer, batch_idx, stage):
        state = self._get_state(stage)
        duration = time.time() - state['start_time']
        
        # Trainé˜¶æ®µç”¨ global_step, å…¶ä»–é˜¶æ®µç”¨ batch_idx (ä»…ç”¨äºæ—¥å¿—æ˜¾ç¤º)
        current_step = trainer.global_step if stage == 'train' else batch_idx
        
        should_clear = False
        reason = ""

        # =========================================================
        # æ ¸å¿ƒé€»è¾‘
        # =========================================================

        # 1. ä¼˜å…ˆæ£€æŸ¥å®šæœŸæ¸…ç†
        if self.config['interval'] > 0 and (batch_idx + 1) % self.config['interval'] == 0:
            should_clear = True
            reason = "periodic"
            
        # 2. æ™ºèƒ½æ£€æµ‹ (å¦‚æœæ²¡æœ‰è§¦å‘å®šæœŸæ¸…ç†)
        else:
            # åˆ¤æ–­æ˜¯å¦å·²é¢„çƒ­ (æœ‰å†å²å¹³å‡å€¼ æˆ– è¶…è¿‡é¢„çƒ­æ­¥æ•°)
            is_warmed_up = (state['avg_time'] > 0) or (batch_idx > self.config['warmup'])
            
            # A. ç›¸å¯¹æ£€æµ‹: æ¯”å¹³å‡å€¼æ…¢ N å€
            if (is_warmed_up and 
                state['avg_time'] > 0 and 
                duration > state['avg_time'] * self.config['slowdown']):
                should_clear = True
                reason = f"slowdown ({duration:.2f}s vs avg {state['avg_time']:.2f}s)"
            
            # B. ç»å¯¹æ£€æµ‹: è¶…è¿‡ N ç§’
            elif (self.config['absolute'] is not None and 
                  duration > self.config['absolute']):
                should_clear = True
                reason = f"absolute limit ({duration:.2f}s > {self.config['absolute']}s)"

        # =========================================================
        # æ‰§è¡ŒåŠ¨ä½œ
        # =========================================================
        if should_clear:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                if self.verbose:
                    stage_name = stage.upper()
                    step_info = f"global_step={current_step}" if stage == 'train' else f"batch={batch_idx}"
                    trainer.print(f"\n[AutoCache][{stage_name}] ğŸ§¹ Cleared at {step_info}. Reason: {reason}")

        # æ›´æ–°å¹³å‡å€¼ (EMA)
        # ç­–ç•¥ï¼šåªæœ‰åœ¨ã€æœªè§¦å‘æ¸…ç†ã€‘(å³è®¤ä¸ºæ˜¯æ­£å¸¸Batch) æ—¶æ‰æ›´æ–°å¹³å‡å€¼
        # è¿™æ ·å¯ä»¥é˜²æ­¢å¼‚å¸¸æ…¢çš„ Batch æ±¡æŸ“å¹³å‡å€¼ï¼Œä¿æŒæ£€æµ‹çš„æ•é”åº¦
        if not should_clear:
            if state['avg_time'] == 0:
                state['avg_time'] = duration
            else:
                # alpha = 0.05
                state['avg_time'] = state['avg_time'] * 0.95 + duration * 0.05

    # ================= é’©å­ç»‘å®š (ä¿æŒä¸å˜) =================
    
    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):
        self._on_batch_start('train')

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        self._on_batch_end(trainer, batch_idx, 'train')

    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):
        self._on_batch_start('val')

    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._on_batch_end(trainer, batch_idx, 'val')

    def on_test_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):
        self._on_batch_start('test')

    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._on_batch_end(trainer, batch_idx, 'test')

    def on_predict_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx=0):
        self._on_batch_start('predict')

    def on_predict_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._on_batch_end(trainer, batch_idx, 'predict')


class TextLoggingCallback(Callback):
    """
    æ–‡æœ¬æ—¥å¿—å›è°ƒ
    
    æ›¿ä»£ tqdm è¿›åº¦æ¡ï¼Œæä¾›ç»Ÿä¸€é£æ ¼çš„æ–‡æœ¬è¾“å‡º:
    - è®­ç»ƒ/éªŒè¯/æµ‹è¯•/é¢„æµ‹è¿›åº¦
    - é¢œè‰²åŒºåˆ†ä¸åŒé˜¶æ®µ
    - å…³é”®æŒ‡æ ‡æ˜¾ç¤º
    """
    
    def __init__(self, log_interval: int = 10):
        super().__init__()
        self.log_interval = log_interval
        self.stage_start_time = 0.0
        
        # å¯¼å…¥æ—¥å¿—å·¥å…·
        from .logger import (
            Colors, print_header, print_section, 
            format_points, log_info, log_warning
        )
        self.Colors = Colors
        self.format_points = format_points

    def _reset_timer(self):
        self.stage_start_time = time.time()

    def on_fit_start(self, trainer, pl_module):
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        from .logger import print_box
        print_box("å¼€å§‹è®­ç»ƒ", {
            "æœ€å¤§è½®æ•°": trainer.max_epochs,
            "è®­ç»ƒæ‰¹æ¬¡æ•°": trainer.num_training_batches,
        })

    def on_train_epoch_start(self, trainer, pl_module):
        self._reset_timer()
        display_epoch = trainer.current_epoch + 1
        max_epochs = trainer.max_epochs if trainer.max_epochs else "?"
        print(f"\n{self.Colors.BOLD}{self.Colors.GREEN}{'â”€' * 60}{self.Colors.RESET}")
        print(f"{self.Colors.BOLD}{self.Colors.GREEN}  Epoch {display_epoch}/{max_epochs}{self.Colors.RESET}")
        print(f"{self.Colors.BOLD}{self.Colors.GREEN}{'â”€' * 60}{self.Colors.RESET}")

    def on_validation_epoch_start(self, trainer, pl_module):
        self._reset_timer()
        print(f"\n{self.Colors.BOLD}{self.Colors.BLUE}  ğŸ“Š éªŒè¯ä¸­...{self.Colors.RESET}")
    
    def on_test_epoch_start(self, trainer, pl_module):
        self._reset_timer()
        print(f"\n{self.Colors.BOLD}{self.Colors.CYAN}  ğŸ§ª æµ‹è¯•ä¸­...{self.Colors.RESET}")

    def on_predict_epoch_start(self, trainer, pl_module):
        self._reset_timer()
        print(f"\n{self.Colors.BOLD}{self.Colors.MAGENTA}  ğŸ”® é¢„æµ‹ä¸­...{self.Colors.RESET}")

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        self._log_batch(trainer, pl_module, batch, batch_idx, stage="Train")

    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._log_batch(trainer, pl_module, batch, batch_idx, stage="Val")

    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._log_batch(trainer, pl_module, batch, batch_idx, stage="Test")

    def on_predict_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        self._log_batch(trainer, pl_module, batch, batch_idx, stage="Pred")

    def _log_batch(self, trainer, pl_module, batch, batch_idx, stage):
        # å§‹ç»ˆè¾“å‡ºç¬¬ä¸€ä¸ª batchï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        if batch_idx == 0 and stage != "Train":
            pass  # ç»§ç»­æ‰§è¡Œ
        elif (batch_idx + 1) % self.log_interval != 0:
            return

        # 1. åŸºæœ¬ä¿¡æ¯
        current_batch = batch_idx + 1
        
        # é˜¶æ®µé¢œè‰²
        stage_colors = {
            'Train': self.Colors.GREEN,
            'Val': self.Colors.BLUE,
            'Test': self.Colors.CYAN,
            'Pred': self.Colors.MAGENTA
        }
        stage_color = stage_colors.get(stage, self.Colors.WHITE)
        
        # Total Batches
        total_batches = 0
        if stage == "Train":
            total_batches = trainer.num_training_batches
            # å¦‚æœ PL è¿”å› infï¼Œå°è¯•æ‰‹åŠ¨è·å–
            if total_batches == float('inf'):
                try:
                    if trainer.train_dataloader:
                        total_batches = len(trainer.train_dataloader)
                except:
                    pass
        elif stage == "Val":
            total_batches = sum(trainer.num_val_batches) if isinstance(trainer.num_val_batches, list) else trainer.num_val_batches
        elif stage == "Test":
            total_batches = sum(trainer.num_test_batches) if isinstance(trainer.num_test_batches, list) else trainer.num_test_batches
        elif stage == "Pred":
            total_batches = sum(trainer.num_predict_batches) if isinstance(trainer.num_predict_batches, list) else trainer.num_predict_batches
            
        # Epoch Info (ä»…åœ¨ Train/Val é˜¶æ®µæ˜¾ç¤º)
        epoch_str = ""
        if stage in ("Train", "Val"):
            max_epochs = trainer.max_epochs if trainer.max_epochs is not None else "?"
            display_epoch = trainer.current_epoch + 1
            epoch_str = f"[{display_epoch}/{max_epochs}]"

        # 2. æ—¶é—´è®¡ç®—
        now = time.time()
        if self.stage_start_time == 0.0: 
            self.stage_start_time = now
        elapsed_sec = now - self.stage_start_time
        
        avg_time = 0.0
        remaining_str = "?"
        if current_batch > 0:
            avg_time = elapsed_sec / current_batch
            if isinstance(total_batches, (int, float)) and total_batches > 0 and total_batches >= current_batch:
                remaining_sec = avg_time * (total_batches - current_batch)
                remaining_str = str(datetime.timedelta(seconds=int(remaining_sec)))
        
        elapsed_str = str(datetime.timedelta(seconds=int(elapsed_sec)))

        # 3. æ‰¹æ¬¡æ•°æ®ä¿¡æ¯
        batch_size = 0
        num_points = 0
        if isinstance(batch, dict):
            batch_size = len(batch.get('offset', [])) if 'offset' in batch else 1
            coord = batch.get('coord', None)
            if coord is not None:
                num_points = coord.shape[0]
        
        pts_str = self.format_points(num_points)

        # 4. å­¦ä¹ ç‡
        lr_str = ""
        if stage == "Train" and trainer.optimizers:
            try:
                lr = trainer.optimizers[0].param_groups[0]['lr']
                lr_str = f"lr={self.Colors.YELLOW}{lr:.2e}{self.Colors.RESET}"
            except:
                pass

        # 5. æŒ‡æ ‡ (æŸå¤±)
        metrics_parts = []
        if stage != "Pred":
            # è®­ç»ƒé˜¶æ®µ
            if stage == "Train":
                loss_val = None
                if hasattr(trainer, 'live_loss'):
                    loss_val = trainer.live_loss
                elif "total_loss_step" in trainer.callback_metrics:
                    loss_val = trainer.callback_metrics["total_loss_step"].item()
                
                if loss_val is not None:
                    loss_color = self.Colors.RED if loss_val > 2.0 else (self.Colors.YELLOW if loss_val > 1.0 else self.Colors.GREEN)
                    metrics_parts.append(f"loss={loss_color}{loss_val:.4f}{self.Colors.RESET}")

                # å…¶ä»–æ­¥éª¤æŒ‡æ ‡
                for k, v in trainer.callback_metrics.items():
                    if k.endswith("_step") and k != "total_loss_step" and "val" not in k and "test" not in k:
                        name = k.replace("_step", "").replace("train_", "")
                        name_map = {
                            "ce_loss": "CE", "lovasz_loss": "LOV", 
                            "dice_loss": "DICE", "focal_loss": "FOCAL", "lac_loss": "LAC"
                        }
                        name = name_map.get(name, name)
                        val = v.item() if hasattr(v, 'item') else v
                        metrics_parts.append(f"{name}={val:.4f}")
            else:
                # Val/Test é˜¶æ®µ
                prefix = "val" if stage == "Val" else "test"
                for k, v in trainer.callback_metrics.items():
                    if k.startswith(prefix) and "loss" in k:
                        name = k.replace(f"{prefix}_", "")
                        val = v.item() if hasattr(v, 'item') else v
                        metrics_parts.append(f"{name}={val:.4f}")

        metrics_str = ", ".join(metrics_parts) if metrics_parts else ""

        # 6. æ„å»ºå¹¶æ‰“å°è¾“å‡º
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        parts = [
            f"{self.Colors.DIM}[{timestamp}]{self.Colors.RESET}",
            f"{stage_color}[{stage}]{self.Colors.RESET}",
            f"{self.Colors.DIM}{epoch_str}{self.Colors.RESET}" if epoch_str else "",
            f"[{current_batch}/{total_batches}]",
            f"{self.Colors.DIM}{elapsed_str}<{remaining_str}{self.Colors.RESET}",
            f"{self.Colors.DIM}{avg_time:.2f}s/it{self.Colors.RESET}",
        ]
        
        if lr_str:
            parts.append(lr_str)
        if metrics_str:
            parts.append(metrics_str)
        
        parts.append(f"bs={batch_size}")
        parts.append(f"pts={pts_str}")
        
        print(" ".join(filter(None, parts)))


# ============================================================================
# SemanticPredictLasWriter1: é€‚é… tile_las1.py é€»è¾‘ç´¢å¼•æ ¼å¼
# ============================================================================

class SemanticPredictLasWriter(BasePredictionWriter):
    """
    ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ PredictionWriter å›è°ƒ (é€‚é… tile_las1.py é€»è¾‘ç´¢å¼•æ ¼å¼)
    
    ä¸åŸç‰ˆ SemanticPredictLasWriter çš„åŒºåˆ«ï¼š
    1. æ”¯æŒ coord_offset æ¢å¤å…¨å±€åæ ‡
    2. æ”¯æŒ segment_id å’Œ loop_idx åŒºåˆ†ä¸åŒé‡‡æ ·è½®æ¬¡
    3. é€‚é…æ–°çš„ pkl å…ƒæ•°æ®ç»“æ„
    
    ä¸»è¦åŠŸèƒ½:
    1. æµå¼å†™å…¥: é˜²æ­¢å¤§è§„æ¨¡ç‚¹äº‘é¢„æµ‹æ—¶çš„ OOM
    2. æŠ•ç¥¨æœºåˆ¶: å¯¹é‡å é¢„æµ‹è¿›è¡Œ logits å¹³å‡æŠ•ç¥¨
    3. å®Œæ•´æ€§æ¢å¤: ä»åŸå§‹ bin/pkl æ¢å¤åæ ‡å’Œå±æ€§
    4. æ ¼å¼ä¿æŒ: ä¿ç•™åŸå§‹ LAS å¤´ä¿¡æ¯å’Œåæ ‡ç³»
    """
    
    def __init__(self, 
                 output_dir: str, 
                 write_interval: str = "batch", 
                 num_classes: int = -1,
                 save_logits: bool = False,
                 reverse_class_mapping: Optional[Dict[int, int]] = None,
                 auto_infer_reverse_mapping: bool = True):
        super().__init__(write_interval)
        self.output_dir = output_dir
        self.temp_dir = os.path.join(self.output_dir, "temp_predictions")
        self.num_classes = num_classes
        self.save_logits = save_logits
        self.reverse_class_mapping = reverse_class_mapping
        self.auto_infer_reverse_mapping = auto_infer_reverse_mapping
        self._mapping_inferred = False
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'total_batches': 0,
            'total_points': 0,
            'files_processed': set()
        }
    
    def on_predict_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """é¢„æµ‹å¼€å§‹å‰çš„åˆå§‹åŒ–å·¥ä½œ"""
        self._infer_class_mapping(trainer, pl_module)
        
        # ç¾åŒ–è¾“å‡º
        print()
        print("â•”" + "â•" * 68 + "â•—")
        print("â•‘" + " è¯­ä¹‰åˆ†å‰²é¢„æµ‹ (é€»è¾‘ç´¢å¼•æ ¼å¼)".center(58) + "â•‘")
        print("â• " + "â•" * 68 + "â•£")
        print(f"â•‘  è¾“å‡ºç›®å½•: {self.output_dir[:50]:<50} â•‘")
        print(f"â•‘  ç±»åˆ«æ•°é‡: {self.num_classes if self.num_classes > 0 else 'è‡ªåŠ¨æ¨æ–­':<50} â•‘")
        print(f"â•‘  ä¿å­˜ Logits: {'æ˜¯' if self.save_logits else 'å¦':<50} â•‘")
        print("â•š" + "â•" * 68 + "â•")
        print()

    def write_on_batch_end(
        self, 
        trainer: 'pl.Trainer', 
        pl_module: 'pl.LightningModule', 
        prediction: Dict[str, torch.Tensor], 
        batch_indices: List[int], 
        batch: Any, 
        batch_idx: int, 
        dataloader_idx: int
    ):
        """æ¯ä¸ªæ‰¹æ¬¡ç»“æŸæ—¶ï¼Œå°†é¢„æµ‹ç»“æœå†™å…¥ä¸´æ—¶æ–‡ä»¶"""
        if not self._validate_prediction(prediction, batch_idx):
            return
        
        # 1. å‡†å¤‡æ•°æ®
        bin_files = prediction['bin_file']
        logits = prediction['logits'].cpu().float()
        indices = prediction['indices'].cpu()
        
        bin_paths = prediction.get('bin_path', [None] * len(bin_files))
        pkl_paths = prediction.get('pkl_path', [None] * len(bin_files))
        
        # è·å– offset
        offsets = batch['offset'].cpu().numpy() if 'offset' in batch else [len(logits)]
        
        # 2. æŒ‰æ–‡ä»¶åˆ†ç»„å¹¶ä¿å­˜
        self._save_batch_predictions(
            bin_files, logits, indices, bin_paths, pkl_paths, 
            offsets, batch_idx, pl_module
        )
        
        # æ›´æ–°ç»Ÿè®¡
        self._stats['total_batches'] += 1
        self._stats['total_points'] += len(logits)
        for bf in bin_files:
            self._stats['files_processed'].add(bf if isinstance(bf, str) else str(bf))

    def on_predict_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """é¢„æµ‹ç»“æŸåçš„æ±‡æ€»å¤„ç†"""
        self._ensure_num_classes(pl_module)
        
        print()
        print("â•”" + "â•" * 68 + "â•—")
        print("â•‘" + " é¢„æµ‹å®Œæˆï¼Œå¼€å§‹æ‹¼æ¥å’ŒæŠ•ç¥¨...".center(53) + "â•‘")
        print("â•š" + "â•" * 68 + "â•")
        print()
        
        # ========== è°ƒè¯•ä¿¡æ¯ï¼šæ¯”è¾ƒé¢„æœŸæ–‡ä»¶å’Œå®é™…æ”¶åˆ°çš„æ–‡ä»¶ ==========
        print("  [è°ƒè¯•] æ•°æ®è¦†ç›–è¯Šæ–­:")
        print(f"     - é¢„æµ‹è¿‡ç¨‹ä¸­é‡åˆ°çš„æ–‡ä»¶: {len(self._stats['files_processed'])}")
        for f in sorted(self._stats['files_processed']):
            print(f"       â€¢ {f}")
        
        # è·å–æ•°æ®é›†ä¸­é¢„æœŸçš„æ‰€æœ‰æ–‡ä»¶
        try:
            dataset = trainer.predict_dataloaders.dataset
            expected_files = set()
            if hasattr(dataset, 'data_list'):
                for item in dataset.data_list:
                    bin_path = item.get('bin_path', '')
                    if bin_path:
                        expected_files.add(Path(bin_path).stem)
            print(f"     - æ•°æ®é›†ä¸­é¢„æœŸçš„æ–‡ä»¶: {len(expected_files)}")
            for f in sorted(expected_files):
                print(f"       â€¢ {f}")
            
            # æ£€æŸ¥é—æ¼
            missing = expected_files - self._stats['files_processed']
            extra = self._stats['files_processed'] - expected_files
            if missing:
                print(f"     [WARN] é—æ¼çš„æ–‡ä»¶ ({len(missing)}):")
                for f in sorted(missing):
                    print(f"       - {f}")
            if extra:
                print(f"     [WARN] é¢å¤–çš„æ–‡ä»¶ ({len(extra)}):")
                for f in sorted(extra):
                    print(f"       - {f}")
            if not missing and not extra:
                print(f"     [OK] æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†!")
        except Exception as e:
            print(f"     [WARN] æ— æ³•è·å–é¢„æœŸæ–‡ä»¶åˆ—è¡¨: {e}")
        print()
        # ========== è°ƒè¯•ä¿¡æ¯ç»“æŸ ==========
        
        tmp_files = sorted(glob.glob(os.path.join(self.temp_dir, "*.pred.tmp")))
        if not tmp_files:
            print("  [WARN] æœªæ‰¾åˆ°ä¸´æ—¶é¢„æµ‹æ–‡ä»¶")
            return
            
        # æŒ‰ bin æ–‡ä»¶åˆ†ç»„
        bin_file_groups = self._group_temp_files(tmp_files)
        print(f"  æ£€æµ‹åˆ° {len(bin_file_groups)} ä¸ªå”¯ä¸€ bin æ–‡ä»¶")
        
        try:
            for idx, (bin_basename, file_list) in enumerate(bin_file_groups.items(), 1):
                print()
                print(f"  â”Œâ”€ [{idx}/{len(bin_file_groups)}] å¤„ç†: {bin_basename}")
                print(f"  â”‚  æ‰¹æ¬¡æ•°: {len(file_list)}")
                try:
                    self._process_single_bin_file(bin_basename, file_list, trainer, pl_module)
                    print(f"  â””â”€ [OK] å®Œæˆ")
                except Exception as e:
                    print(f"  â””â”€ [ERROR] é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
        finally:
            self._cleanup_temp_files(tmp_files, pl_module)
        
        # æœ€ç»ˆç»Ÿè®¡
        print()
        print("â•”" + "â•" * 68 + "â•—")
        print("â•‘" + " é¢„æµ‹å®Œæˆ!".center(58) + "â•‘")
        print("â• " + "â•" * 68 + "â•£")
        print(f"â•‘  æ€»æ‰¹æ¬¡æ•°: {self._stats['total_batches']:<51} â•‘")
        print(f"â•‘  æ€»ç‚¹æ•°: {self._stats['total_points']:,}".ljust(55) + "â•‘")
        print(f"â•‘  è¾“å‡ºæ–‡ä»¶: {len(bin_file_groups)} ä¸ª LAS æ–‡ä»¶".ljust(55) + "â•‘")
        print(f"â•‘  è¾“å‡ºç›®å½•: {self.output_dir[:50]:<50} â•‘")
        print("â•š" + "â•" * 68 + "â•")
        print()

    # ================= å†…éƒ¨è¾…åŠ©æ–¹æ³• =================

    def _infer_class_mapping(self, trainer, pl_module):
        """æ¨æ–­åå‘ç±»åˆ«æ˜ å°„"""
        if self.reverse_class_mapping is not None:
            print(f"  â„¹ï¸  ä½¿ç”¨ç”¨æˆ·æä¾›çš„ reverse_class_mapping")
            return
        
        if not self.auto_infer_reverse_mapping:
            return
            
        # å°è¯•ä»æ¨¡å‹ checkpoint è·å–
        try:
            if hasattr(pl_module, 'hparams') and hasattr(pl_module.hparams, 'class_mapping'):
                mapping = pl_module.hparams.class_mapping
                if mapping:
                    # ä½¿ç”¨ create_reverse_mapping å¤„ç† Dict æˆ– List
                    self.reverse_class_mapping = create_reverse_mapping(mapping)
                    self._mapping_inferred = True
                    print(f"  â„¹ï¸  ä»æ¨¡å‹ checkpoint åŠ è½½ reverse_class_mapping")
                    return
        except Exception:
            pass
            
        # å°è¯•ä» DataModule è·å–
        try:
            datamodule = trainer.datamodule
            if hasattr(datamodule, 'class_mapping') and datamodule.class_mapping:
                # ä½¿ç”¨ create_reverse_mapping å¤„ç† Dict æˆ– List
                self.reverse_class_mapping = create_reverse_mapping(datamodule.class_mapping)
                self._mapping_inferred = True
                print(f"  â„¹ï¸  ä» DataModule æ¨æ–­ reverse_class_mapping")
            else:
                print(f"  â„¹ï¸  æœªæ‰¾åˆ° class_mappingï¼Œä½¿ç”¨è¿ç»­æ ‡ç­¾")
        except Exception as e:
            print(f"  [WARN] æ— æ³•æ¨æ–­ reverse_class_mapping: {e}")

    def _validate_prediction(self, prediction, batch_idx):
        if 'logits' not in prediction or 'indices' not in prediction:
            print(f"  [WARN] predict_step å¿…é¡»è¿”å› 'logits' å’Œ 'indices'ã€‚è·³è¿‡æ‰¹æ¬¡ {batch_idx}")
            return False
        if 'bin_file' not in prediction or len(prediction['bin_file']) == 0:
            print(f"  [WARN] batch {batch_idx} ç¼ºå°‘ bin_file ä¿¡æ¯ï¼Œè·³è¿‡")
            return False
        return True

    def _save_batch_predictions(self, bin_files, logits, indices, bin_paths, pkl_paths, 
                                 offsets, batch_idx, pl_module):
        """å°†ä¸€ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹æŒ‰æ–‡ä»¶æ‹†åˆ†å¹¶ä¿å­˜"""
        file_groups = defaultdict(lambda: {
            'logits': [], 'indices': [], 
            'bin_path': None, 'pkl_path': None
        })
        
        start_idx = 0
        for i, end_idx in enumerate(offsets):
            # è·å–æ–‡ä»¶å
            f = bin_files[i]
            bin_basename = (f if isinstance(f, str) else str(f))
            if bin_basename.endswith('.bin'):
                bin_basename = bin_basename[:-4]
            
            # åˆ‡ç‰‡
            file_groups[bin_basename]['logits'].append(logits[start_idx:end_idx])
            file_groups[bin_basename]['indices'].append(indices[start_idx:end_idx])
            
            # è·¯å¾„
            if file_groups[bin_basename]['bin_path'] is None:
                if isinstance(bin_paths, list) and i < len(bin_paths):
                    file_groups[bin_basename]['bin_path'] = bin_paths[i]
                if isinstance(pkl_paths, list) and i < len(pkl_paths):
                    file_groups[bin_basename]['pkl_path'] = pkl_paths[i]
            
            start_idx = end_idx
            
        # ä¿å­˜
        for bin_basename, data in file_groups.items():
            if not data['logits']: continue
            
            save_path = os.path.join(self.temp_dir, f"{bin_basename}_batch_{batch_idx}.pred.tmp")
            save_dict = {
                'logits': torch.cat(data['logits'], dim=0),
                'indices': torch.cat(data['indices'], dim=0),
                'bin_file': bin_basename,
            }
            if data['bin_path']: save_dict['bin_path'] = data['bin_path']
            if data['pkl_path']: save_dict['pkl_path'] = data['pkl_path']
            
            torch.save(save_dict, save_path)

    def _ensure_num_classes(self, pl_module):
        if self.num_classes != -1: return
        
        try:
            if hasattr(pl_module, 'head'):
                if hasattr(pl_module.head, 'out_channels'):
                    self.num_classes = pl_module.head.out_channels
                elif hasattr(pl_module.head, 'num_classes'):
                    self.num_classes = pl_module.head.num_classes
            elif hasattr(pl_module, 'num_classes'):
                self.num_classes = pl_module.num_classes
            print(f"  â„¹ï¸  ä»æ¨¡å‹æ¨æ–­ç±»åˆ«æ•°: {self.num_classes}")
        except Exception:
            print("  [ERROR] æ— æ³•ä»æ¨¡å‹æ¨æ–­ num_classesï¼Œè¯·æ˜¾å¼æŒ‡å®š")

    def _group_temp_files(self, tmp_files):
        groups = defaultdict(list)
        for f in tmp_files:
            basename = os.path.basename(f).split('_batch_')[0]
            groups[basename].append(f)
        return groups

    def _cleanup_temp_files(self, tmp_files, pl_module):
        print()
        print("  ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for f in tmp_files:
            try:
                if os.path.exists(f): os.remove(f)
            except Exception as e:
                print(f"  [WARN] æ— æ³•åˆ é™¤ {f}: {e}")
        
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                print(f"  [OK] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹")
        except Exception as e:
            print(f"  [WARN] æ¸…ç†æ–‡ä»¶å¤¹å¤±è´¥: {e}")

    def _process_single_bin_file(self, bin_basename, tmp_files, trainer, pl_module):
        """å¤„ç†å•ä¸ª bin æ–‡ä»¶çš„æ‰€æœ‰é¢„æµ‹ç»“æœ"""
        # 1. è·å–è·¯å¾„
        bin_path, pkl_path = self._get_file_paths(bin_basename, tmp_files, trainer, pl_module)
        if not bin_path or not pkl_path: 
            raise ValueError(f"æ— æ³•æ‰¾åˆ° bin/pkl æ–‡ä»¶: {bin_basename}")

        # 2. åŠ è½½å…ƒæ•°æ®å’Œç‚¹äº‘
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        point_data = np.memmap(bin_path, dtype=metadata['dtype'], mode='r')
        num_points = len(point_data)
        print(f"  â”‚  æ€»ç‚¹æ•°: {num_points:,}")
        
        # 3. æŠ•ç¥¨
        final_preds, mean_logits, counts = self._perform_voting(tmp_files, num_points, pl_module)
        
        # ç»Ÿè®¡æŠ•ç¥¨è¦†ç›–ç‡
        covered = (counts > 0).sum().item()
        coverage = covered / num_points * 100
        print(f"  â”‚  æŠ•ç¥¨è¦†ç›–ç‡: {coverage:.1f}% ({covered:,}/{num_points:,})")
        
        # 4. æ˜ å°„
        if self.reverse_class_mapping:
            final_preds = self._apply_mapping(final_preds, pl_module)
            
        # 5. ä¿å­˜ LAS
        xyz = np.stack([point_data['X'], point_data['Y'], point_data['Z']], axis=1).astype(np.float64)
        metadata['_bin_path'] = bin_path
        
        output_path = os.path.join(self.output_dir, f"{bin_basename}.las")
        self._save_las_file(output_path, xyz, final_preds, metadata, pl_module)
        print(f"  â”‚  ğŸ“„ å·²ä¿å­˜: {Path(output_path).name}")
        
        # 6. ä¿å­˜ Logits
        if self.save_logits:
            logits_path = os.path.join(self.output_dir, f"{bin_basename}_logits.npz")
            np.savez_compressed(
                logits_path,
                logits=mean_logits.numpy(),
                predictions=final_preds,
                counts=counts.numpy()
            )
            print(f"  â”‚  ğŸ“„ å·²ä¿å­˜ Logits: {Path(logits_path).name}")

    def _get_file_paths(self, bin_basename, tmp_files, trainer, pl_module):
        """è·å– bin å’Œ pkl æ–‡ä»¶è·¯å¾„"""
        # å°è¯•ä»ä¸´æ—¶æ–‡ä»¶è·å–
        if tmp_files:
            try:
                data = torch.load(tmp_files[0], weights_only=False)
                if 'bin_path' in data and 'pkl_path' in data:
                    bp = data['bin_path']
                    pp = data['pkl_path']
                    return (str(bp[0]) if isinstance(bp, list) else str(bp)), \
                           (str(pp[0]) if isinstance(pp, list) else str(pp))
            except Exception:
                pass
        
        # åå¤‡æ–¹æ¡ˆ
        return self._find_bin_pkl_paths(bin_basename, trainer)
    
    def _find_bin_pkl_paths(self, bin_basename: str, trainer: 'pl.Trainer') -> tuple:
        """æ ¹æ® bin_basename æŸ¥æ‰¾å¯¹åº”çš„ bin å’Œ pkl æ–‡ä»¶è·¯å¾„"""
        try:
            dataset = trainer.predict_dataloaders.dataset
            
            # åœ¨ data_list ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            for sample_info in dataset.data_list:
                bin_path = Path(sample_info['bin_path'])
                if bin_path.stem == bin_basename:
                    pkl_path = Path(sample_info['pkl_path'])
                    return str(bin_path), str(pkl_path)
            
            # å¦‚æœåœ¨ data_list ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä» data_root æœç´¢
            data_root = Path(dataset.data_root) if not isinstance(dataset.data_root, (list, tuple)) else Path(dataset.data_root[0]).parent
            
            bin_path = data_root / f"{bin_basename}.bin"
            pkl_path = data_root / f"{bin_basename}.pkl"
            
            if bin_path.exists() and pkl_path.exists():
                return str(bin_path), str(pkl_path)
            
            return None, None
            
        except Exception as e:
            print(f"  [ERROR] æŸ¥æ‰¾ bin/pkl æ–‡ä»¶å¤±è´¥: {e}")
            return None, None

    def _perform_voting(self, tmp_files, num_points, pl_module):
        """æ‰§è¡ŒæŠ•ç¥¨"""
        logits_sum = torch.zeros((num_points, self.num_classes), dtype=torch.float32)
        counts = torch.zeros(num_points, dtype=torch.int32)
        
        for f in tmp_files:
            try:
                d = torch.load(f, weights_only=False)
                logits_sum.index_add_(0, d['indices'].long(), d['logits'].float())
                counts.index_add_(0, d['indices'].long(), torch.ones(len(d['indices']), dtype=torch.int32))
            except Exception as e:
                print(f"  [WARN] åŠ è½½ {f} å¤±è´¥: {e}")
                
        # è®¡ç®—å¹³å‡
        mask = (counts == 0)
        counts_safe = counts.clone()
        counts_safe[mask] = 1
        mean_logits = logits_sum / counts_safe.unsqueeze(-1)
        
        # Argmax
        if mean_logits.ndim == 2 and mean_logits.size(1) > 1:
            preds = torch.argmax(mean_logits, dim=1).numpy().astype(np.uint8)
        else:
            preds = mean_logits.squeeze().numpy().astype(np.uint8)
            
        if mask.any():
            preds[mask.numpy()] = 0
            
        return preds, mean_logits, counts

    def _apply_mapping(self, preds, pl_module):
        """åº”ç”¨åå‘ç±»åˆ«æ˜ å°„"""
        print(f"  â”‚  åº”ç”¨åå‘ç±»åˆ«æ˜ å°„")
        max_label = max(self.reverse_class_mapping.keys())
        mapping = np.arange(max_label + 1)
        for k, v in self.reverse_class_mapping.items():
            mapping[k] = v
        return mapping[preds]

    def _save_las_file(self, las_path: str, xyz: np.ndarray, classification: np.ndarray,
                       metadata: Dict[str, Any], pl_module: 'pl.LightningModule'):
        """ä¿å­˜ç‚¹äº‘ä¸º .las æ–‡ä»¶"""
        try:
            # 1. ä» metadata ä¸­æ¢å¤ LAS å¤´ä¿¡æ¯
            if 'header_info' in metadata:
                header_info = metadata['header_info']
                
                point_format = header_info.get('point_format', 3)
                version_str = header_info.get('version', '1.2')
                
                if isinstance(version_str, str):
                    version_parts = version_str.split('.')
                    if len(version_parts) == 2:
                        major, minor = int(version_parts[0]), int(version_parts[1])
                    else:
                        major, minor = 1, 2
                else:
                    major, minor = 1, 2
                
                header = laspy.LasHeader(point_format=point_format, version=f"{major}.{minor}")
                
                header.offsets = [
                    header_info.get('x_offset', 0),
                    header_info.get('y_offset', 0),
                    header_info.get('z_offset', 0)
                ]
                header.scales = [
                    header_info.get('x_scale', 0.01),
                    header_info.get('y_scale', 0.01),
                    header_info.get('z_scale', 0.01)
                ]
                
                if 'system_identifier' in header_info:
                    header.system_identifier = header_info['system_identifier']
                if 'generating_software' in header_info:
                    header.generating_software = header_info['generating_software']
                
                # æ¢å¤ VLRs
                if 'vlrs' in header_info and header_info['vlrs']:
                    for vlr_dict in header_info['vlrs']:
                        try:
                            vlr = laspy.VLR(
                                user_id=vlr_dict['user_id'],
                                record_id=vlr_dict['record_id'],
                                description=vlr_dict['description'],
                                record_data=vlr_dict.get('record_data', b'')
                            )
                            header.vlrs.append(vlr)
                        except Exception as e:
                            pass
            else:
                header = laspy.LasHeader(point_format=3, version='1.2')
                header.offsets = xyz.min(axis=0)
                header.scales = np.array([0.001, 0.001, 0.001])
            
            # 2. åˆ›å»º LAS æ•°æ®
            las = laspy.LasData(header)
            
            # 3. è®¾ç½®åæ ‡
            las.x = xyz[:, 0]
            las.y = xyz[:, 1]
            las.z = xyz[:, 2]
            
            # 4. ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤å±æ€§
            if 'dtype' in metadata:
                dtype = metadata['dtype']
                bin_path = metadata.get('_bin_path', None)
                
                if bin_path and Path(bin_path).exists():
                    point_data = np.memmap(bin_path, dtype=dtype, mode='r')
                    field_names = [name for name, _ in dtype]
                    
                    if 'intensity' in field_names:
                        las.intensity = point_data['intensity']
                    if 'return_number' in field_names:
                        las.return_number = point_data['return_number']
                    if 'number_of_returns' in field_names:
                        las.number_of_returns = point_data['number_of_returns']
                    if 'scan_angle_rank' in field_names:
                        las.scan_angle_rank = point_data['scan_angle_rank']
                    elif 'scan_angle' in field_names:
                        las.scan_angle = point_data['scan_angle']
                    if 'user_data' in field_names:
                        las.user_data = point_data['user_data']
                    if 'point_source_id' in field_names:
                        las.point_source_id = point_data['point_source_id']
                    if 'gps_time' in field_names:
                        las.gps_time = point_data['gps_time']
                    
                    if header.point_format.id in [2, 3, 5, 7, 8, 10]:
                        if 'red' in field_names and 'green' in field_names and 'blue' in field_names:
                            las.red = point_data['red']
                            las.green = point_data['green']
                            las.blue = point_data['blue']
            
            # 5. è®¾ç½®é¢„æµ‹çš„åˆ†ç±»æ ‡ç­¾
            las.classification = classification
            
            # 6. å†™å…¥æ–‡ä»¶
            las.write(las_path)
            
        except Exception as e:
            print(f"  [ERROR] ä¿å­˜ LAS æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise