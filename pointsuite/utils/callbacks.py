import torch
import numpy as np
import os
import glob
import pickle
import pytorch_lightning as pl
from pytorch_lightning.callbacks import BasePredictionWriter
from typing import List, Any, Dict, Optional
from pathlib import Path
from collections import defaultdict

# å¯¼å…¥ laspy (æ‚¨éœ€è¦ 'pip install laspy')
try:
    import laspy
except ImportError:
    print("è­¦å‘Š: 'laspy' åº“æœªå®‰è£…ã€‚PredictionWriter å°†æ— æ³•ä¿å­˜ .las æ–‡ä»¶ã€‚")
    print("è¯·è¿è¡Œ: pip install laspy")


# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

def create_reverse_class_mapping(class_mapping: Dict[int, int]) -> Dict[int, int]:
    """
    ä» class_mapping åˆ›å»º reverse_class_mapping
    
    Args:
        class_mapping: åŸå§‹æ ‡ç­¾ -> è¿ç»­æ ‡ç­¾çš„æ˜ å°„
                      ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
    
    Returns:
        reverse_class_mapping: è¿ç»­æ ‡ç­¾ -> åŸå§‹æ ‡ç­¾çš„æ˜ å°„
                              ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
    
    Example:
        >>> class_mapping = {0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
        >>> reverse_mapping = create_reverse_class_mapping(class_mapping)
        >>> print(reverse_mapping)
        {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
    """
    return {v: k for k, v in class_mapping.items()}


class SegmentationWriter(BasePredictionWriter):
    """
    ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ PredictionWriter å›è°ƒ (é€‚é… bin+pkl æ•°æ®æ ¼å¼)

    ä¸“ä¸º PointSuite çš„ bin+pkl æ•°æ®ç»“æ„è®¾è®¡ï¼Œä¸ BinPklDataset å’Œ SemanticSegmentationTask ååŒå·¥ä½œã€‚

    æ•°æ®æµç¨‹:
    1. tile.py åœ¨åˆ†å‰² LAS æ–‡ä»¶æ—¶ï¼Œä¸ºæ¯ä¸ª segment ä¿å­˜æ–‡ä»¶å…³è”ä¿¡æ¯:
       - 'bin_file': bin æ–‡ä»¶å
       - 'bin_path': å®Œæ•´ bin æ–‡ä»¶è·¯å¾„
       - 'pkl_path': å®Œæ•´ pkl æ–‡ä»¶è·¯å¾„
       
    2. BinPklDataset (test split) åŠ è½½æ•°æ®æ—¶ï¼Œå°†æ–‡ä»¶ä¿¡æ¯åŠ å…¥ data å­—å…¸:
       {'coord', 'feat', 'indices', 'bin_file', 'bin_path', 'pkl_path', ...}
       
    3. SemanticSegmentationTask.predict_step è¿”å›æ—¶ï¼Œä¼ é€’æ–‡ä»¶ä¿¡æ¯:
       {'logits', 'indices', 'bin_file', 'bin_path', 'pkl_path', 'coord'}
       - 'logits': [N, C] æ¨¡å‹é¢„æµ‹çš„ç±»åˆ« logits
       - 'indices': [N] åŸå§‹ç‚¹ç´¢å¼•
       - 'bin_file': bin æ–‡ä»¶åï¼ˆç›´æ¥æ¥è‡ª datasetï¼‰
       - 'bin_path': bin æ–‡ä»¶å®Œæ•´è·¯å¾„
       - 'pkl_path': pkl æ–‡ä»¶å®Œæ•´è·¯å¾„
       - 'coord': [N, 3] ç‚¹åæ ‡ (ç”¨äºå¯è§†åŒ–)

    4. æœ¬å›è°ƒæ‰§è¡ŒæŠ•ç¥¨å¹¶ä¿å­˜:
       - ç›´æ¥ä½¿ç”¨ä¼ é€’çš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼Œæ— éœ€æ¨æ–­
       - å¯¹äºæ¯ä¸ª bin æ–‡ä»¶ï¼Œç´¯ç§¯æ‰€æœ‰ segment çš„é¢„æµ‹
       - ä½¿ç”¨ logits å¹³å‡è¿›è¡Œå¤šæ¬¡é¢„æµ‹æŠ•ç¥¨
       - ä»åŸå§‹ bin/pkl åŠ è½½å®Œæ•´ç‚¹äº‘å’Œ LAS å¤´ä¿¡æ¯
       - ä¿å­˜ä¸º .las æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹åæ ‡ç³»ç»Ÿå’Œç²¾åº¦

    å·¥ä½œæµç¨‹:
    1. write_on_batch_end: å°†æ¯ä¸ªæ‰¹æ¬¡çš„é¢„æµ‹æµå¼å†™å…¥ä¸´æ—¶æ–‡ä»¶ (.tmp)ï¼Œé˜²æ­¢ OOM
    2. on_predict_end: é¢„æµ‹ç»“æŸåè§¦å‘ï¼Œå¯¹æ¯ä¸ª bin æ–‡ä»¶æ‰§è¡Œ:
       a. æ”¶é›†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
       b. æŒ‰ bin æ–‡ä»¶åˆ†ç»„
       c. æ‰§è¡ŒæŠ•ç¥¨ç´¯ç§¯ (logits å¹³å‡)
       d. ä»åŸå§‹ bin/pkl åŠ è½½åæ ‡å’Œ LAS å¤´
       e. ä¿å­˜å®Œæ•´ç‚¹äº‘ä¸º .las æ–‡ä»¶
       f. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    
    æ³¨æ„: 
    - æ–‡ä»¶ä¿¡æ¯åœ¨æ•´ä¸ªæ•°æ®æµä¸­æ˜¾å¼ä¼ é€’ï¼Œé¿å…äº†æ¨æ–­çš„ä¸ç¡®å®šæ€§
    - å³ä½¿ batch åŒ…å«æ¥è‡ªå¤šä¸ª segment çš„ç‚¹ï¼Œå®ƒä»¬å¿…å®šæ¥è‡ªåŒä¸€ä¸ª bin æ–‡ä»¶
    """
    
    def __init__(self, 
                 output_dir: str, 
                 write_interval: str = "batch", 
                 num_classes: int = -1,
                 save_logits: bool = False,
                 reverse_class_mapping: Optional[Dict[int, int]] = None,
                 auto_infer_reverse_mapping: bool = True):
        """
        Args:
            output_dir (str): ä¿å­˜æœ€ç»ˆ .las æ–‡ä»¶çš„ç›®å½•
            write_interval (str): å¿…é¡»æ˜¯ "batch" æ‰èƒ½å®ç°æµå¼ä¼ è¾“
            num_classes (int): ç±»çš„æ•°é‡ï¼Œç”¨äºåˆ›å»ºæŠ•ç¥¨æ•°ç»„
                              å¦‚æœä¸º -1ï¼Œå°†ä» Task çš„ head.out_channels è‡ªåŠ¨æ¨æ–­
            save_logits (bool): æ˜¯å¦åŒæ—¶ä¿å­˜ logits åˆ° .npz æ–‡ä»¶ (ç”¨äºåå¤„ç†/é›†æˆ)
            reverse_class_mapping (Optional[Dict[int, int]]): å°†è¿ç»­æ ‡ç­¾æ˜ å°„å›åŸå§‹æ ‡ç­¾
                                  ä¾‹å¦‚: {0: 0, 1: 1, 2: 2, 3: 6, 4: 9}
                                  å¦‚æœä¸º None ä¸” auto_infer_reverse_mapping=Trueï¼Œ
                                  å°†å°è¯•ä» DataModule.class_mapping è‡ªåŠ¨æ„å»º
            auto_infer_reverse_mapping (bool): æ˜¯å¦è‡ªåŠ¨ä» DataModule æ¨æ–­ reverse_class_mapping
                                              é»˜è®¤ Trueã€‚å½“ reverse_class_mapping=None æ—¶ç”Ÿæ•ˆ
        """
        super().__init__(write_interval)
        self.output_dir = output_dir
        self.temp_dir = os.path.join(self.output_dir, "temp_predictions")
        self.num_classes = num_classes
        self.save_logits = save_logits
        self.reverse_class_mapping = reverse_class_mapping
        self.auto_infer_reverse_mapping = auto_infer_reverse_mapping
        self._mapping_inferred = False  # æ ‡è®°æ˜¯å¦å·²æ¨æ–­
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)
    
    def on_predict_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """
        é¢„æµ‹å¼€å§‹æ—¶ï¼Œå°è¯•è‡ªåŠ¨æ¨æ–­ reverse_class_mapping
        
        ä¼˜å…ˆçº§ï¼š
        1. ç”¨æˆ·æä¾›çš„ reverse_class_mappingï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        2. æ¨¡å‹ checkpoint ä¸­çš„ class_mappingï¼ˆä» hparams åŠ è½½ï¼‰
        3. DataModule ä¸­çš„ class_mapping
        """
        # å¦‚æœç”¨æˆ·å·²ç»æä¾›äº† reverse_class_mappingï¼Œè·³è¿‡æ¨æ–­
        if self.reverse_class_mapping is not None:
            pl_module.print(f"[SegmentationWriter] ä½¿ç”¨ç”¨æˆ·æä¾›çš„ reverse_class_mapping: {self.reverse_class_mapping}")
            return
        
        # å¦‚æœä¸éœ€è¦è‡ªåŠ¨æ¨æ–­ï¼Œè·³è¿‡
        if not self.auto_infer_reverse_mapping:
            return
        
        # ä¼˜å…ˆçº§ 1: å°è¯•ä»æ¨¡å‹ checkpoint è·å– class_mapping
        try:
            if hasattr(pl_module, 'hparams') and hasattr(pl_module.hparams, 'class_mapping'):
                class_mapping = pl_module.hparams.class_mapping
                if class_mapping is not None:
                    # æ„å»ºåå‘æ˜ å°„
                    self.reverse_class_mapping = {v: k for k, v in class_mapping.items()}
                    self._mapping_inferred = True
                    pl_module.print(f"[SegmentationWriter] è‡ªåŠ¨åŠ è½½ reverse_class_mapping ä»æ¨¡å‹ checkpoint:")
                    pl_module.print(f"  - class_mapping: {class_mapping}")
                    pl_module.print(f"  - reverse_class_mapping: {self.reverse_class_mapping}")
                    return
        except Exception as e:
            pl_module.print(f"[SegmentationWriter] æ— æ³•ä»æ¨¡å‹åŠ è½½ class_mapping: {e}")
        
        # ä¼˜å…ˆçº§ 2: å°è¯•ä» DataModule è·å– class_mapping
        try:
            datamodule = trainer.datamodule
            if hasattr(datamodule, 'class_mapping') and datamodule.class_mapping is not None:
                # æ„å»ºåå‘æ˜ å°„
                self.reverse_class_mapping = {v: k for k, v in datamodule.class_mapping.items()}
                self._mapping_inferred = True
                pl_module.print(f"[SegmentationWriter] è‡ªåŠ¨æ¨æ–­ reverse_class_mapping ä» DataModule:")
                pl_module.print(f"  - class_mapping: {datamodule.class_mapping}")
                pl_module.print(f"  - reverse_class_mapping: {self.reverse_class_mapping}")
            else:
                pl_module.print(f"[SegmentationWriter] æœªæ‰¾åˆ° class_mappingï¼Œé¢„æµ‹ç»“æœå°†ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„è¿ç»­æ ‡ç­¾")
        except Exception as e:
            pl_module.print(f"[SegmentationWriter] è­¦å‘Š: æ— æ³•æ¨æ–­ reverse_class_mapping: {e}")

    def write_on_batch_end(
        self, 
        trainer: 'pl.Trainer', 
        pl_module: 'pl.LightningModule', 
        prediction: Dict[str, torch.Tensor], 
        batch_indices: List[int], 
        batch: Any, 
        batch_idx: int, 
        dataloader_idx: int
    ):
        """ 
        åœ¨æ¯ä¸ªé¢„æµ‹æ‰¹æ¬¡ç»“æŸåï¼Œå°†ç»“æœæµå¼å†™å…¥ä¸´æ—¶æ–‡ä»¶
        
        prediction å­—å…¸åº”åŒ…å«:
        - 'logits': [N, C] æ¨¡å‹é¢„æµ‹
        - 'indices': [N] åŸå§‹ bin æ–‡ä»¶ä¸­çš„ç‚¹ç´¢å¼•
        - 'bin_file': bin æ–‡ä»¶åï¼ˆåˆ—è¡¨ï¼Œæ¯ä¸ªç‚¹å¯¹åº”çš„æ–‡ä»¶ï¼‰
        - 'coord': [N, 3] ç‚¹åæ ‡ (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
        """
        
        if 'logits' not in prediction or 'indices' not in prediction:
            print(f"è­¦å‘Š: predict_step å¿…é¡»è¿”å› 'logits' å’Œ 'indices'ã€‚è·³è¿‡æ‰¹æ¬¡ {batch_idx}")
            return
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ”¯æŒ batch å†…åŒ…å«å¤šä¸ª bin æ–‡ä»¶çš„ç‚¹
        # bin_file æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªç‚¹å¯¹åº”ä¸€ä¸ªæ–‡ä»¶å
        if 'bin_file' not in prediction or len(prediction['bin_file']) == 0:
            print(f"è­¦å‘Š: batch {batch_idx} ç¼ºå°‘ bin_file ä¿¡æ¯ï¼Œè·³è¿‡")
            return
        
        bin_files = prediction['bin_file']
        logits = prediction['logits'].cpu()  # [N, C]
        indices = prediction['indices'].cpu()  # [N]
        
        # è·å–å®Œæ•´è·¯å¾„ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        bin_paths = prediction.get('bin_path', [None] * len(bin_files))
        pkl_paths = prediction.get('pkl_path', [None] * len(bin_files))
        
        # æŒ‰ bin æ–‡ä»¶åˆ†ç»„ç‚¹
        # ä½¿ç”¨å­—å…¸è®°å½•æ¯ä¸ªæ–‡ä»¶çš„ç‚¹ï¼š{bin_basename: {'logits': [], 'indices': [], 'bin_path': str, 'pkl_path': str}}
        file_groups = defaultdict(lambda: {'logits': [], 'indices': [], 'bin_path': None, 'pkl_path': None})
        
        for i in range(len(bin_files)):
            # æå– bin_basename
            if isinstance(bin_files, list):
                bin_basename = bin_files[i] if isinstance(bin_files[i], str) else str(bin_files[i])
            else:
                bin_basename = str(bin_files[i].item()) if hasattr(bin_files[i], 'item') else str(bin_files[i])
            
            # å»æ‰å¯èƒ½çš„æ‰©å±•å
            if bin_basename.endswith('.bin'):
                bin_basename = bin_basename[:-4]
            
            # ç´¯ç§¯è¯¥ç‚¹çš„é¢„æµ‹
            file_groups[bin_basename]['logits'].append(logits[i])
            file_groups[bin_basename]['indices'].append(indices[i])
            
            # ä¿å­˜è·¯å¾„ä¿¡æ¯ï¼ˆæ‰€æœ‰æ¥è‡ªåŒä¸€æ–‡ä»¶çš„ç‚¹å…±äº«è·¯å¾„ï¼‰
            if file_groups[bin_basename]['bin_path'] is None:
                if isinstance(bin_paths, list) and i < len(bin_paths):
                    file_groups[bin_basename]['bin_path'] = bin_paths[i]
                if isinstance(pkl_paths, list) and i < len(pkl_paths):
                    file_groups[bin_basename]['pkl_path'] = pkl_paths[i]
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        for bin_basename, data in file_groups.items():
            # å †å è¯¥æ–‡ä»¶çš„æ‰€æœ‰ç‚¹
            file_logits = torch.stack(data['logits'], dim=0)  # [N_file, C]
            file_indices = torch.stack(data['indices'], dim=0)  # [N_file]
            
            # å®šä¹‰ä¸´æ—¶æ–‡ä»¶å
            tmp_filename = f"{bin_basename}_batch_{batch_idx}.pred.tmp"
            save_path = os.path.join(self.temp_dir, tmp_filename)
            
            # ä¿å­˜é¢„æµ‹ç»“æœåˆ°ç£ç›˜
            save_dict = {
                'logits': file_logits,
                'indices': file_indices,
                'bin_file': bin_basename,
            }
            
            # ä¿å­˜å®Œæ•´è·¯å¾„ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if data['bin_path'] is not None:
                save_dict['bin_path'] = data['bin_path']
            if data['pkl_path'] is not None:
                save_dict['pkl_path'] = data['pkl_path']
            
            torch.save(save_dict, save_path)
        
        # å¯é€‰: æ‰“å°è¿›åº¦
        if batch_idx % 10 == 0:
            pl_module.print(f"[SegmentationWriter] Batch {batch_idx}: ä¿å­˜äº† {len(file_groups)} ä¸ªæ–‡ä»¶çš„é¢„æµ‹")

    def on_predict_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule'):
        """
        åœ¨æ•´ä¸ªé¢„æµ‹ç»“æŸåè§¦å‘ï¼Œå¯¹æ‰€æœ‰ bin æ–‡ä»¶æ‰§è¡ŒæŠ•ç¥¨å’Œä¿å­˜
        
        æ­¤æ–¹æ³•ä¼š:
        1. æŒ‰ bin æ–‡ä»¶åˆ†ç»„æ‰€æœ‰ä¸´æ—¶é¢„æµ‹æ–‡ä»¶
        2. å¯¹æ¯ä¸ª bin æ–‡ä»¶æ‰§è¡ŒæŠ•ç¥¨ (logits å¹³å‡)
        3. ä»åŸå§‹ bin/pkl åŠ è½½å®Œæ•´ç‚¹äº‘æ•°æ®
        4. ä¿å­˜ä¸º .las æ–‡ä»¶
        5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        """
        
        # å¦‚æœ num_classes æœªæŒ‡å®šï¼Œä» Task æ¨æ–­
        if self.num_classes == -1:
            try:
                # å°è¯•å¤šç§æ–¹å¼æ¨æ–­ç±»åˆ«æ•°
                if hasattr(pl_module, 'head'):
                    if hasattr(pl_module.head, 'out_channels'):
                        self.num_classes = pl_module.head.out_channels
                    elif hasattr(pl_module.head, 'num_classes'):
                        self.num_classes = pl_module.head.num_classes
                    else:
                        raise AttributeError("head æ²¡æœ‰ out_channels æˆ– num_classes å±æ€§")
                elif hasattr(pl_module, 'num_classes'):
                    self.num_classes = pl_module.num_classes
                else:
                    raise AttributeError("æ— æ³•æ‰¾åˆ° num_classes")
                pl_module.print(f"[SegmentationWriter] ä»æ¨¡å‹æ¨æ–­ç±»åˆ«æ•°: {self.num_classes}")
            except Exception as e:
                print(f"é”™è¯¯: æ— æ³•ä»æ¨¡å‹æ¨æ–­ num_classes: {e}")
                print("è¯·åœ¨åˆå§‹åŒ– SegmentationWriter æ—¶æ˜¾å¼æŒ‡å®š num_classes")
                return

        pl_module.print(f"\n[SegmentationWriter] é¢„æµ‹å®Œæˆï¼Œå¼€å§‹æ‹¼æ¥å’ŒæŠ•ç¥¨...")
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        tmp_files = sorted(glob.glob(os.path.join(self.temp_dir, "*.pred.tmp")))
        
        if not tmp_files:
            pl_module.print("[SegmentationWriter] è­¦å‘Š: æœªæ‰¾åˆ°ä¸´æ—¶é¢„æµ‹æ–‡ä»¶")
            return
        
        pl_module.print(f"[SegmentationWriter] æ‰¾åˆ° {len(tmp_files)} ä¸ªä¸´æ—¶é¢„æµ‹æ–‡ä»¶")
        
        # 2. æŒ‰ bin æ–‡ä»¶åˆ†ç»„ä¸´æ—¶æ–‡ä»¶
        # æ–‡ä»¶åæ ¼å¼: {bin_basename}_batch_{batch_idx}.pred.tmp
        bin_file_groups = defaultdict(list)
        
        for tmp_file in tmp_files:
            filename = os.path.basename(tmp_file)
            # æå– bin_basename (å»é™¤ _batch_xxx.pred.tmp éƒ¨åˆ†)
            bin_basename = filename.split('_batch_')[0]
            bin_file_groups[bin_basename].append(tmp_file)
        
        pl_module.print(f"[SegmentationWriter] æ£€æµ‹åˆ° {len(bin_file_groups)} ä¸ªå”¯ä¸€ bin æ–‡ä»¶")
        
        # 3. å¯¹æ¯ä¸ª bin æ–‡ä»¶æ‰§è¡ŒæŠ•ç¥¨å’Œä¿å­˜
        for bin_basename, tmp_file_list in bin_file_groups.items():
            pl_module.print(f"\n[SegmentationWriter] å¤„ç† bin æ–‡ä»¶: {bin_basename} ({len(tmp_file_list)} ä¸ªæ‰¹æ¬¡)")
            
            try:
                self._process_single_bin_file(
                    bin_basename=bin_basename,
                    tmp_files=tmp_file_list,
                    trainer=trainer,
                    pl_module=pl_module
                )
            except Exception as e:
                pl_module.print(f"!!! é”™è¯¯: å¤„ç† {bin_basename} æ—¶å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # 4. æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        pl_module.print(f"\n[SegmentationWriter] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for tmp_file in tmp_files:
            try:
                os.remove(tmp_file)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {tmp_file}: {e}")
        
        pl_module.print(f"[SegmentationWriter] æ‰€æœ‰é¢„æµ‹å·²ä¿å­˜åˆ° {self.output_dir}")
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                pl_module.print(f"[SegmentationWriter] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹: {self.temp_dir}")
        except Exception as e:
            pl_module.print(f"[SegmentationWriter] è­¦å‘Š: æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹å¤±è´¥: {e}")
        
        pl_module.print("="*70)
    
    def _process_single_bin_file(
        self,
        bin_basename: str,
        tmp_files: List[str],
        trainer: 'pl.Trainer',
        pl_module: 'pl.LightningModule'
    ):
        """
        å¤„ç†å•ä¸ª bin æ–‡ä»¶çš„æ‰€æœ‰é¢„æµ‹æ‰¹æ¬¡
        
        æ‰§è¡Œæ­¥éª¤:
        1. ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½æ‰€æœ‰é¢„æµ‹å¹¶æ‰§è¡ŒæŠ•ç¥¨
        2. ä»åŸå§‹ bin/pkl æ–‡ä»¶åŠ è½½å®Œæ•´ç‚¹äº‘æ•°æ®
        3. åº”ç”¨ç±»åˆ«æ˜ å°„ (å¦‚æœæœ‰)
        4. ä¿å­˜ä¸º .las æ–‡ä»¶
        5. (å¯é€‰) ä¿å­˜ logits åˆ° .npz
        
        Args:
            bin_basename: bin æ–‡ä»¶çš„åŸºç¡€åç§° (ä¸å¸¦æ‰©å±•å)
            tmp_files: è¯¥ bin æ–‡ä»¶çš„æ‰€æœ‰ä¸´æ—¶é¢„æµ‹æ–‡ä»¶åˆ—è¡¨
            trainer: PyTorch Lightning Trainer
            pl_module: PyTorch Lightning Module
        """
        
        # 1. ğŸ”¥ ä¼˜å…ˆä»ä¸´æ—¶æ–‡ä»¶ä¸­è·å–å®Œæ•´è·¯å¾„ä¿¡æ¯
        bin_path, pkl_path = None, None
        
        # å°è¯•ä»ç¬¬ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¸­è¯»å–è·¯å¾„ä¿¡æ¯
        if len(tmp_files) > 0:
            try:
                first_tmp = torch.load(tmp_files[0])
                if 'bin_path' in first_tmp and 'pkl_path' in first_tmp:
                    # ä»ä¸´æ—¶æ–‡ä»¶ä¸­ç›´æ¥è·å–è·¯å¾„
                    bin_path_list = first_tmp['bin_path']
                    pkl_path_list = first_tmp['pkl_path']
                    
                    # å¤„ç†åˆ—è¡¨æƒ…å†µï¼ˆcollate_fn å¯èƒ½ä¿æŒä¸ºåˆ—è¡¨ï¼‰
                    if isinstance(bin_path_list, list):
                        bin_path = str(Path(bin_path_list[0]))
                        pkl_path = str(Path(pkl_path_list[0]))
                    else:
                        bin_path = str(Path(bin_path_list))
                        pkl_path = str(Path(pkl_path_list))
                    
                    pl_module.print(f"  - ä»ä¸´æ—¶æ–‡ä»¶è·å–è·¯å¾„ âœ“")
            except Exception as e:
                pl_module.print(f"  - ä»ä¸´æ—¶æ–‡ä»¶è·å–è·¯å¾„å¤±è´¥: {e}")
        
        # 2. å¦‚æœæ²¡æœ‰ä»ä¸´æ—¶æ–‡ä»¶è·å–åˆ°ï¼Œä½¿ç”¨æ—§çš„æŸ¥æ‰¾æ–¹æ³•ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        if bin_path is None or pkl_path is None:
            pl_module.print(f"  - ä½¿ç”¨åå¤‡æ–¹æ¡ˆæŸ¥æ‰¾æ–‡ä»¶...")
            bin_path, pkl_path = self._find_bin_pkl_paths(bin_basename, trainer)
        
        if bin_path is None or pkl_path is None:
            pl_module.print(f"é”™è¯¯: æ— æ³•æ‰¾åˆ° {bin_basename} å¯¹åº”çš„ bin/pkl æ–‡ä»¶")
            return
        
        pl_module.print(f"  - Bin æ–‡ä»¶: {bin_path}")
        pl_module.print(f"  - Pkl æ–‡ä»¶: {pkl_path}")
        
        # 2. ä» pkl åŠ è½½å…ƒæ•°æ®
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        
        # 3. ä½¿ç”¨ memmap åŠ è½½å®Œæ•´ç‚¹äº‘æ•°æ®
        point_data = np.memmap(bin_path, dtype=metadata['dtype'], mode='r')
        num_total_points = len(point_data)
        
        pl_module.print(f"  - æ€»ç‚¹æ•°: {num_total_points:,}")
        
        # 4. åˆ›å»ºæŠ•ç¥¨æ•°ç»„
        logits_sum = torch.zeros((num_total_points, self.num_classes), dtype=torch.float32)
        counts = torch.zeros(num_total_points, dtype=torch.int32)
        
        # 5. åŠ è½½æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶å¹¶ç´¯ç§¯æŠ•ç¥¨
        pl_module.print(f"  - åŠ è½½ {len(tmp_files)} ä¸ªé¢„æµ‹æ‰¹æ¬¡...")
        
        for tmp_file in tmp_files:
            try:
                pred_data = torch.load(tmp_file)
                indices = pred_data['indices']  # [N]
                logits = pred_data['logits']    # [N, C]
                
                # ç´¯ç§¯ logits
                logits_sum.index_add_(0, indices.long(), logits)
                counts.index_add_(0, indices.long(), torch.ones(len(indices), dtype=torch.int32))
                
            except Exception as e:
                pl_module.print(f"    è­¦å‘Š: åŠ è½½ {tmp_file} å¤±è´¥: {e}")
        
        # 6. è®¡ç®—å¹³å‡ logits å’Œæœ€ç»ˆé¢„æµ‹
        pl_module.print(f"  - è®¡ç®—æœ€ç»ˆé¢„æµ‹...")
        
        # å¤„ç†æœªè¢«é¢„æµ‹çš„ç‚¹
        unpredicted_mask = (counts == 0)
        counts[unpredicted_mask] = 1  # é¿å…é™¤ä»¥ 0
        
        # å¹³å‡ logits
        mean_logits = logits_sum / counts.unsqueeze(-1)
        
        # ğŸ”¥ æ–°å¢ï¼šæ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦ argmax
        # å¦‚æœ mean_logits æ˜¯ [N, C] çš„ logitsï¼Œåˆ™éœ€è¦ argmax
        # å¦‚æœ mean_logits æ˜¯ [N] çš„ labelsï¼Œåˆ™ç›´æ¥ä½¿ç”¨
        if mean_logits.ndim == 2 and mean_logits.size(1) > 1:
            # [N, C] logits -> argmax è·å–ç±»åˆ«
            final_preds = torch.argmax(mean_logits, dim=1).numpy().astype(np.uint8)
        elif mean_logits.ndim == 2 and mean_logits.size(1) == 1:
            # [N, 1] -> squeeze ä¸º [N]
            final_preds = mean_logits.squeeze(-1).numpy().astype(np.uint8)
        else:
            # [N] labels -> ç›´æ¥ä½¿ç”¨
            final_preds = mean_logits.numpy().astype(np.uint8)
        
        # å¯¹æœªé¢„æµ‹çš„ç‚¹èµ‹å€¼ (ä½¿ç”¨ 0 æˆ– ignore_label)
        if unpredicted_mask.any():
            num_unpredicted = unpredicted_mask.sum().item()
            pl_module.print(f"    è­¦å‘Š: {num_unpredicted} ä¸ªç‚¹æœªè¢«é¢„æµ‹ï¼Œå°†èµ‹äºˆæ ‡ç­¾ 0")
            final_preds[unpredicted_mask.numpy()] = 0
        
        # 7. åº”ç”¨åå‘ç±»åˆ«æ˜ å°„ (å¦‚æœæœ‰)
        if self.reverse_class_mapping is not None:
            pl_module.print(f"  - åº”ç”¨åå‘ç±»åˆ«æ˜ å°„...")
            final_preds_mapped = np.zeros_like(final_preds)
            for continuous_label, original_label in self.reverse_class_mapping.items():
                final_preds_mapped[final_preds == continuous_label] = original_label
            final_preds = final_preds_mapped
        
        # 8. æå–åæ ‡ (XYZ)
        xyz = np.stack([
            point_data['X'],
            point_data['Y'],
            point_data['Z']
        ], axis=1).astype(np.float64)
        
        # 9. ä¿å­˜ä¸º .las æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰åŸå§‹å±æ€§ï¼‰
        final_las_path = os.path.join(self.output_dir, f"{bin_basename}.las")
        
        try:
            # å°† bin_path æ·»åŠ åˆ° metadata ä¸­ï¼Œæ–¹ä¾¿ _save_las_file åŠ è½½åŸå§‹æ•°æ®
            metadata['_bin_path'] = bin_path
            
            self._save_las_file(
                las_path=final_las_path,
                xyz=xyz,
                classification=final_preds,
                metadata=metadata,
                pl_module=pl_module
            )
            pl_module.print(f"  âœ“ å·²ä¿å­˜åˆ°: {final_las_path}")
            
        except Exception as e:
            pl_module.print(f"  !!! é”™è¯¯: ä¿å­˜ .las æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # 10. (å¯é€‰) ä¿å­˜ logits
        if self.save_logits:
            logits_path = os.path.join(self.output_dir, f"{bin_basename}_logits.npz")
            np.savez_compressed(
                logits_path,
                logits=mean_logits.numpy(),
                predictions=final_preds,
                counts=counts.numpy()
            )
            pl_module.print(f"  âœ“ Logits å·²ä¿å­˜åˆ°: {logits_path}")
    
    def _find_bin_pkl_paths(self, bin_basename: str, trainer: 'pl.Trainer') -> tuple:
        """
        æ ¹æ® bin_basename æŸ¥æ‰¾å¯¹åº”çš„ bin å’Œ pkl æ–‡ä»¶è·¯å¾„
        
        Args:
            bin_basename: bin æ–‡ä»¶çš„åŸºç¡€åç§°
            trainer: Trainer å¯¹è±¡
            
        Returns:
            (bin_path, pkl_path) å…ƒç»„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› (None, None)
        """
        try:
            dataset = trainer.predict_dataloaders.dataset
            
            # åœ¨ data_list ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            for sample_info in dataset.data_list:
                bin_path = Path(sample_info['bin_path'])
                if bin_path.stem == bin_basename:
                    pkl_path = Path(sample_info['pkl_path'])
                    return str(bin_path), str(pkl_path)
            
            # å¦‚æœåœ¨ data_list ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä» data_root æœç´¢
            data_root = Path(dataset.data_root) if not isinstance(dataset.data_root, (list, tuple)) else Path(dataset.data_root[0]).parent
            
            bin_path = data_root / f"{bin_basename}.bin"
            pkl_path = data_root / f"{bin_basename}.pkl"
            
            if bin_path.exists() and pkl_path.exists():
                return str(bin_path), str(pkl_path)
            
            return None, None
            
        except Exception as e:
            print(f"é”™è¯¯: æŸ¥æ‰¾ bin/pkl æ–‡ä»¶å¤±è´¥: {e}")
            return None, None
    
    def _save_las_file(
        self,
        las_path: str,
        xyz: np.ndarray,
        classification: np.ndarray,
        metadata: Dict[str, Any],
        pl_module: 'pl.LightningModule'
    ):
        """
        ä¿å­˜ç‚¹äº‘ä¸º .las æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹ LAS å¤´ä¿¡æ¯å’Œæ‰€æœ‰ç‚¹å±æ€§
        
        Args:
            las_path: è¾“å‡º .las æ–‡ä»¶è·¯å¾„
            xyz: [N, 3] ç‚¹åæ ‡
            classification: [N] åˆ†ç±»æ ‡ç­¾ï¼ˆé¢„æµ‹ç»“æœï¼‰
            metadata: ä» pkl æ–‡ä»¶åŠ è½½çš„å…ƒæ•°æ® (åŒ…å« header_info)
            pl_module: PyTorch Lightning Module
        """
        
        try:
            # 1. ä» metadata ä¸­æ¢å¤ LAS å¤´ä¿¡æ¯
            if 'header_info' in metadata:
                header_info = metadata['header_info']
                
                # åˆ›å»º LAS å¤´
                point_format = header_info.get('point_format', 3)
                version_str = header_info.get('version', '1.2')
                
                # è§£æç‰ˆæœ¬å­—ç¬¦ä¸²
                if isinstance(version_str, str):
                    version_parts = version_str.split('.')
                    if len(version_parts) == 2:
                        major, minor = int(version_parts[0]), int(version_parts[1])
                    else:
                        major, minor = 1, 2
                else:
                    major, minor = 1, 2
                
                header = laspy.LasHeader(point_format=point_format, version=f"{major}.{minor}")
                
                # è®¾ç½®ç¼©æ”¾å’Œåç§»
                header.offsets = [
                    header_info.get('x_offset', 0),
                    header_info.get('y_offset', 0),
                    header_info.get('z_offset', 0)
                ]
                header.scales = [
                    header_info.get('x_scale', 0.01),
                    header_info.get('y_scale', 0.01),
                    header_info.get('z_scale', 0.01)
                ]
                
                # æ¢å¤å…¶ä»–å¤´ä¿¡æ¯
                if 'system_identifier' in header_info:
                    header.system_identifier = header_info['system_identifier']
                if 'generating_software' in header_info:
                    header.generating_software = header_info['generating_software']
                
                # æ¢å¤ VLRs (Variable Length Records) - åŒ…å«åæ ‡ç³»ä¿¡æ¯
                if 'vlrs' in header_info and header_info['vlrs']:
                    for vlr_dict in header_info['vlrs']:
                        try:
                            vlr = laspy.VLR(
                                user_id=vlr_dict['user_id'],
                                record_id=vlr_dict['record_id'],
                                description=vlr_dict['description'],
                                record_data=vlr_dict.get('record_data', b'')
                            )
                            header.vlrs.append(vlr)
                        except Exception as e:
                            pl_module.print(f"    è­¦å‘Š: æ¢å¤ VLR å¤±è´¥: {e}")
                
                pl_module.print(f"    - ä½¿ç”¨åŸå§‹ LAS å¤´ä¿¡æ¯ (format {point_format}, version {major}.{minor})")
                
            else:
                # å¦‚æœæ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼
                pl_module.print("    è­¦å‘Š: å…ƒæ•°æ®ä¸­æ²¡æœ‰ header_infoï¼Œä½¿ç”¨é»˜è®¤å€¼")
                header = laspy.LasHeader(point_format=3, version='1.2')
                header.offsets = xyz.min(axis=0)
                header.scales = np.array([0.001, 0.001, 0.001])
            
            # 2. åˆ›å»º LAS æ•°æ®
            las = laspy.LasData(header)
            
            # 3. è®¾ç½®åæ ‡ (laspy ä¼šè‡ªåŠ¨åº”ç”¨ scale å’Œ offset)
            las.x = xyz[:, 0]
            las.y = xyz[:, 1]
            las.z = xyz[:, 2]
            
            # 4. ğŸ”¥ ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤æ‰€æœ‰å¯ç”¨å±æ€§
            if 'dtype' in metadata:
                dtype = metadata['dtype']
                
                # è·å– bin æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä» metadata ä¸­çš„ _bin_pathï¼‰
                bin_path = metadata.get('_bin_path', None)
                
                if bin_path and Path(bin_path).exists():
                    pl_module.print(f"    - ä»åŸå§‹ bin æ–‡ä»¶æ¢å¤å±æ€§: {Path(bin_path).name}")
                    
                    # ä½¿ç”¨ memmap åŠ è½½åŸå§‹æ•°æ®
                    point_data = np.memmap(bin_path, dtype=dtype, mode='r')
                    
                    # æ¢å¤å„ä¸ªå±æ€§ï¼ˆæ ¹æ® dtype ä¸­çš„å­—æ®µï¼‰
                    field_names = [name for name, _ in dtype]
                    
                    # å¼ºåº¦ (Intensity)
                    if 'intensity' in field_names:
                        las.intensity = point_data['intensity']
                        pl_module.print(f"      âœ“ æ¢å¤ intensity")
                    
                    # å›æ³¢ä¿¡æ¯ (Return Number, Number of Returns)
                    if 'return_number' in field_names:
                        las.return_number = point_data['return_number']
                        pl_module.print(f"      âœ“ æ¢å¤ return_number")
                    if 'number_of_returns' in field_names:
                        las.number_of_returns = point_data['number_of_returns']
                        pl_module.print(f"      âœ“ æ¢å¤ number_of_returns")
                    
                    # æ‰«æè§’åº¦ (Scan Angle)
                    if 'scan_angle_rank' in field_names:
                        las.scan_angle_rank = point_data['scan_angle_rank']
                        pl_module.print(f"      âœ“ æ¢å¤ scan_angle_rank")
                    elif 'scan_angle' in field_names:
                        las.scan_angle = point_data['scan_angle']
                        pl_module.print(f"      âœ“ æ¢å¤ scan_angle")
                    
                    # ç”¨æˆ·æ•°æ® (User Data)
                    if 'user_data' in field_names:
                        las.user_data = point_data['user_data']
                        pl_module.print(f"      âœ“ æ¢å¤ user_data")
                    
                    # ç‚¹æº ID (Point Source ID)
                    if 'point_source_id' in field_names:
                        las.point_source_id = point_data['point_source_id']
                        pl_module.print(f"      âœ“ æ¢å¤ point_source_id")
                    
                    # GPS æ—¶é—´ (GPS Time)
                    if 'gps_time' in field_names:
                        las.gps_time = point_data['gps_time']
                        pl_module.print(f"      âœ“ æ¢å¤ gps_time")
                    
                    # RGB é¢œè‰² (å¦‚æœ point_format æ”¯æŒ)
                    if header.point_format.id in [2, 3, 5, 7, 8, 10]:
                        if 'red' in field_names and 'green' in field_names and 'blue' in field_names:
                            las.red = point_data['red']
                            las.green = point_data['green']
                            las.blue = point_data['blue']
                            pl_module.print(f"      âœ“ æ¢å¤ RGB é¢œè‰²")
                        
                        # NIR (è¿‘çº¢å¤–) - å¦‚æœæ”¯æŒ
                        if 'nir' in field_names and header.point_format.id in [8, 10]:
                            las.nir = point_data['nir']
                            pl_module.print(f"      âœ“ æ¢å¤ NIR")
                    
                    # å…¶ä»–å¯èƒ½çš„å­—æ®µå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ 
                    
                else:
                    pl_module.print(f"    è­¦å‘Š: æœªæ‰¾åˆ°åŸå§‹ bin æ–‡ä»¶: {bin_path}")
                    pl_module.print(f"    åªä¿å­˜åæ ‡å’Œåˆ†ç±»æ ‡ç­¾")
            
            # 5. è®¾ç½®é¢„æµ‹çš„åˆ†ç±»æ ‡ç­¾ï¼ˆè¦†ç›–åŸå§‹åˆ†ç±»ï¼‰
            las.classification = classification
            pl_module.print(f"      âœ“ è®¾ç½®é¢„æµ‹åˆ†ç±»æ ‡ç­¾")
            
            # 6. å†™å…¥æ–‡ä»¶
            las.write(las_path)
            
        except Exception as e:
            pl_module.print(f"    é”™è¯¯: ä¿å­˜ LAS æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise