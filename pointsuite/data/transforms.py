"""
3D Ã§Â‚Â¹Ã¤ÂºÂ‘Ã¦Â•Â°Ã¦ÂÂ®Ã¥Â¢ÂÃ¥Â¼ÂºÃ¥ÂÂ˜Ã¦ÂÂ¢Ã¦Â¨Â¡Ã¥ÂÂ—
"""

import random
import numbers
import scipy
import scipy.ndimage
import scipy.interpolate
import scipy.stats
import numpy as np
import torch
import copy
from collections.abc import Sequence, Mapping


# Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â” Ã©Â€ÂšÃ§Â”Â¨Ã¦Â“ÂÃ¤Â½Âœ Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”
# Ã§Â»Â„Ã¥ÂÂˆÃ¥Â¤ÂšÃ¤Â¸ÂªÃ¥ÂÂ˜Ã¦ÂÂ¢
class Compose:
    def __init__(self, transforms=None):
        if transforms is None:
            transforms = []
        self.transforms = [t for t in transforms if t is not None]

    def __call__(self, data_dict):
        for t in self.transforms:
            data_dict = t(data_dict)
            if data_dict is None:
                return None
        return data_dict


# Ã§Â´Â¢Ã¥Â¼Â•Ã¦Â“ÂÃ¤Â½Âœ
def index_operator(data_dict, index, duplicate=False):
    # Ã¥Â¯Â¹ "index_valid_keys" Ã¤Â¸Â­Ã§ÂšÂ„Ã©Â”Â®Ã¦Â‰Â§Ã¨Â¡ÂŒÃ§Â´Â¢Ã¥Â¼Â•Ã©Â€Â‰Ã¦Â‹Â©Ã¦Â“ÂÃ¤Â½Âœ
    # Ã¥ÂÂ¯Ã¥ÂœÂ¨Ã©Â…ÂÃ§Â½Â®Ã¤Â¸Â­Ã©Â€ÂšÃ¨Â¿Â‡ "Update" Ã¥ÂÂ˜Ã¦ÂÂ¢Ã¨Â‡ÂªÃ¥Â®ÂšÃ¤Â¹Â‰Ã¨Â¿Â™Ã¤ÂºÂ›Ã©Â”Â®
    # Ã¥Â¯Â¹ data_dict Ã¤Â¸Â­Ã§ÂšÂ„Ã©Â”Â®Ã¨Â¿Â›Ã¨Â¡ÂŒÃ§Â´Â¢Ã¥Â¼Â•Ã¦Â“ÂÃ¤Â½Âœ
    if "index_valid_keys" not in data_dict:
        data_dict["index_valid_keys"] = [
            "coord",
            "intensity",
            "echo",
            "h_norm",
            "color",
            "normal",
            "class",
            "instance",
            "indices",  # Ã°ÂŸÂ”Â¥ Ã¤Â¿Â®Ã¥Â¤ÂÃ¯Â¼ÂšÃ¥ÂœÂ¨test/predictÃ¦Â¨Â¡Ã¥Â¼ÂÃ¤Â¸Â‹Ã¯Â¼ÂŒindicesÃ©ÂœÂ€Ã¨Â¦ÂÃ¤Â¸ÂÃ¥Â…Â¶Ã¤Â»Â–Ã¥Â­Â—Ã¦Â®ÂµÃ¥ÂÂŒÃ¦Â­Â¥Ã¨Â¿Â‡Ã¦Â»Â¤
        ]
    if not duplicate:
        for key in data_dict["index_valid_keys"]:
            if key in data_dict:
                data_dict[key] = data_dict[key][index]
        return data_dict
    else:
        data_dict_ = dict()
        for key in data_dict.keys():
            if key in data_dict["index_valid_keys"]:
                data_dict_[key] = data_dict[key][index]
            else:
                data_dict_[key] = data_dict[key]
        return data_dict_


# Ã¦Â”Â¶Ã©Â›Â†Ã¦ÂŒÂ‡Ã¥Â®Âš key Ã§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂ offset Ã¥Â’ÂŒÃ§Â‰Â¹Ã¥Â¾ÂÃ¦Â‹Â¼Ã¦ÂÂ¥
class Collect(object):
    def __init__(self, keys, offset_key=None, feat_keys=None):
        """
        e.g. Collect(keys=[coord], feat_keys=[coord, color])
        """
        if offset_key is None:
            offset_key = dict(offset="coord")
        if feat_keys is None:
            feat_keys = dict(feat="coord")
        self.keys = keys
        self.offset_key = offset_key
        self.feat_keys = feat_keys

    def __call__(self, data_dict):
        if isinstance(data_dict, Sequence):
            return [self(item) for item in data_dict]
            
        data = dict()
        if isinstance(self.keys, str):
            self.keys = [self.keys]
        for key in self.keys:
            # ç›´æ¥ä¼ é€’æ•°æ®ï¼ˆå¯èƒ½æ˜¯ numpy æˆ– tensorï¼‰
            data[key] = data_dict[key]
        for key, value in self.offset_key.items():
            # offset åˆ›å»ºä¸º Tensor
            data[key] = torch.tensor([data_dict[value].shape[0]])
            
        # ğŸ”¥ å…¼å®¹ list ç±»å‹çš„ feat_keys (è‡ªåŠ¨è½¬æ¢ä¸º {'feat': list})
        feat_keys = self.feat_keys
        if isinstance(feat_keys, list):
            feat_keys = {'feat': feat_keys}
            
        for name, keys in feat_keys.items():
            name = name.replace("_keys", "")
            assert isinstance(keys, Sequence)
            # data[name] = torch.cat([data_dict[key].float() for key in keys], dim=1)
            tensors = []
            for key in keys:
                # Ã¥Â…ÂˆÃ¨Â½Â¬Ã¦ÂÂ¢Ã¤Â¸Âº TensorÃ¯Â¼ÂˆÃ¥Â¦Â‚Ã¦ÂÂœÃ¨Â¿Â˜Ã¤Â¸ÂÃ¦Â˜Â¯Ã¯Â¼Â‰
                if isinstance(data_dict[key], np.ndarray):
                    tensor = torch.from_numpy(data_dict[key]).float()
                else:
                    tensor = data_dict[key].float()
                
                if tensor.dim() == 1:  # Ã¥Â¦Â‚Ã¦ÂÂœÃ¦Â˜Â¯ [n]Ã¯Â¼ÂŒÃ¦Â‰Â©Ã¥Â±Â•Ã¦ÂˆÂ [n, 1]
                    tensor = tensor.unsqueeze(1)
                tensors.append(tensor)
            data[name] = torch.cat(tensors, dim=1)  # [n, c + m]Ã¯Â¼Âˆm Ã¦Â˜Â¯Ã©Â¢ÂÃ¥Â¤Â–Ã¦Â‹Â¼Ã¦ÂÂ¥Ã§ÂšÂ„ 1D Ã¥Â¼Â Ã©Â‡ÂÃ¦Â•Â°Ã©Â‡ÂÃ¯Â¼Â‰
        return data
    

# Ã¦Â›Â´Ã¦Â–Â°Ã¦ÂŒÂ‡Ã¥Â®ÂšÃ§ÂšÂ„Ã©Â”Â®
class Update(object):
    def __init__(self, keys_dict=None):
        if keys_dict is None:
            keys_dict = dict()
        self.keys_dict = keys_dict

    def __call__(self, data_dict):
        for key, value in self.keys_dict.items():
            data_dict[key] = value
        return data_dict
    
     
# Ã¥Â°Â†Ã¦Â•Â°Ã¦ÂÂ®Ã¨Â½Â¬Ã¦ÂÂ¢Ã¤Â¸ÂºÃ¥Â¼Â Ã©Â‡Â
class ToTensor(object):
    def __call__(self, data):
        if isinstance(data, torch.Tensor):
            return data
        elif isinstance(data, str):
            # note that str is also a kind of sequence, judgement should before sequence
            return data
        elif isinstance(data, int):
            return torch.LongTensor([data])
        elif isinstance(data, float):
            return torch.FloatTensor([data])
        elif isinstance(data, np.ndarray) and np.issubdtype(data.dtype, bool):
            return torch.from_numpy(data)
        elif isinstance(data, np.ndarray) and np.issubdtype(data.dtype, np.integer):
            return torch.from_numpy(data).long()
        elif isinstance(data, np.ndarray) and np.issubdtype(data.dtype, np.floating):
            return torch.from_numpy(data).float()
        elif isinstance(data, Mapping):
            result = {sub_key: self(item) for sub_key, item in data.items()}
            return result
        elif isinstance(data, Sequence):
            result = [self(item) for item in data]
            return result
        else:
            raise TypeError(f"type {type(data)} cannot be converted to tensor.")
        

# Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â” Ã¥ÂÂÃ¦Â Â‡Ã¥ÂÂ˜Ã¦ÂÂ¢ Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”
# Ã¥ÂÂÃ¦Â Â‡Ã¦Â Â‡Ã¥Â‡Â†Ã¥ÂŒÂ–Ã¯Â¼ÂŒÃ¥Â‡ÂÃ¥ÂÂ»Ã¨Â´Â¨Ã¥Â¿ÂƒÃ¥Â¹Â¶Ã§Â¼Â©Ã¦Â”Â¾Ã¥ÂˆÂ°Ã¥ÂÂ•Ã¤Â½ÂÃ§ÂÂƒ
class NormalizeCoord(object):
    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            # modified from pointnet2
            centroid = np.mean(data_dict["coord"], axis=0)
            data_dict["coord"] -= centroid
            m = np.max(np.sqrt(np.sum(data_dict["coord"] ** 2, axis=1)))
            data_dict["coord"] = data_dict["coord"] / m
        return data_dict
    

# Ã¥Â‡ÂÃ¥ÂÂ»Ã¥ÂÂ‡Ã¥Â€Â¼Ã©Â™Â¤Ã¤Â»Â¥Ã¦Â Â‡Ã¥Â‡Â†Ã¥Â·Â®Ã¯Â¼ÂˆÃ¦Â Â‡Ã¥Â‡Â†Ã¥ÂŒÂ–Ã¯Â¼Â‰
class StandardNormalize(object):
    def __init__(self, apply_z=True):
        self.apply_z = apply_z

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            mean = data_dict["coord"].mean(axis=0)
            std = data_dict["coord"].std(axis=0)
            if not self.apply_z:
                mean[2] = 0
                std[2] = 1
            # Ã©ÂÂ¿Ã¥Â…ÂÃ©Â™Â¤Ã¤Â»Â¥0
            std[std == 0] = 1
            data_dict["coord"] = (data_dict["coord"] - mean) / std
        return data_dict


# Ã¥Â‡ÂÃ¥ÂÂ»Ã¦ÂœÂ€Ã¥Â°ÂÃ¥Â€Â¼Ã©Â™Â¤Ã¤Â»Â¥Ã¦ÂœÂ€Ã¥Â¤Â§Ã¦ÂœÂ€Ã¥Â°ÂÃ¥Â€Â¼Ã¤Â¹Â‹Ã¥Â·Â®Ã¯Â¼ÂˆMinMaxÃ¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¯Â¼Â‰
class MinMaxNormalize(object):
    def __init__(self, apply_z=True):
        self.apply_z = apply_z

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            min_vals = data_dict["coord"].min(axis=0)
            max_vals = data_dict["coord"].max(axis=0)
            if not self.apply_z:
                min_vals[2] = 0
                max_vals[2] = 1
            # Ã¨Â®Â¡Ã§Â®Â—Ã¨ÂŒÂƒÃ¥Â›Â´Ã¯Â¼ÂŒÃ©ÂÂ¿Ã¥Â…ÂÃ©Â™Â¤Ã¤Â»Â¥0
            ranges = max_vals - min_vals
            ranges[ranges == 0] = 1
            data_dict["coord"] = (data_dict["coord"] - min_vals) / ranges
        return data_dict


# Ã¥ÂÂÃ¦Â Â‡Ã¥ÂÂÃ§Â§Â»Ã¯Â¼ÂˆÃ¦ÂœÂ€Ã¥Â°ÂÃ¥Â€Â¼Ã¯Â¼Â‰
class PositiveShift(object):
    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            coord_min = np.min(data_dict["coord"], 0)
            data_dict["coord"] -= coord_min
        return data_dict


# Ã¥ÂÂÃ¦Â Â‡Ã¥ÂÂÃ§Â§Â»Ã¯Â¼ÂˆÃ¤Â¸Â­Ã¥Â¿ÂƒÃ¯Â¼Â‰
class CenterShift(object):
    def __init__(self, apply_z=True):
        self.apply_z = apply_z

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            x_min, y_min, z_min = data_dict["coord"].min(axis=0)
            x_max, y_max, _ = data_dict["coord"].max(axis=0)
            if self.apply_z:
                shift = [(x_min + x_max) / 2, (y_min + y_max) / 2, z_min]
            else:
                shift = [(x_min + x_max) / 2, (y_min + y_max) / 2, 0]
            data_dict["coord"] -= shift
        return data_dict
    

# Ã¥ÂÂÃ¦Â Â‡Ã¥ÂÂÃ§Â§Â»Ã¯Â¼ÂˆÃ¨Â´Â¨Ã¥Â¿ÂƒÃ¯Â¼Â‰
class CentroidShift(object):
    def __init__(self, apply_z=True):
        self.apply_z = apply_z

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            centroid = np.mean(data_dict["coord"], axis=0)
            if not self.apply_z:
                centroid[2] = 0
            data_dict["coord"] -= centroid
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ¥ÂÂÃ§Â§Â»
class RandomShift(object):
    def __init__(self, shift=((-0.2, 0.2), (-0.2, 0.2), (0, 0))):
        self.shift = shift

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            shift_x = np.random.uniform(self.shift[0][0], self.shift[0][1])
            shift_y = np.random.uniform(self.shift[1][0], self.shift[1][1])
            shift_z = np.random.uniform(self.shift[2][0], self.shift[2][1])
            data_dict["coord"] += [shift_x, shift_y, shift_z]
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ¤Â¸Â¢Ã¥Â¼Âƒ
class RandomDropout(object):
    def __init__(self, dropout_ratio=0.2, p=0.5):
        """
        upright_axis: axis index among x,y,z, i.e. 2 for z
        """
        self.dropout_ratio = dropout_ratio
        self.dropout_application_ratio = p

    def __call__(self, data_dict):
        if random.random() < self.dropout_application_ratio:
            n = len(data_dict["coord"])
            idx = np.random.choice(n, int(n * (1 - self.dropout_ratio)), replace=False)
            if "sampled_index" in data_dict:
                # for ScanNet data efficient, we need to make sure labeled point is sampled.
                idx = np.unique(np.append(idx, data_dict["sampled_index"]))
                mask = np.zeros_like(data_dict["class"]).astype(bool)
                mask[data_dict["sampled_index"]] = True
                data_dict["sampled_index"] = np.where(mask[idx])[0]
            data_dict = index_operator(data_dict, idx)
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ¦Â—Â‹Ã¨Â½Â¬
class RandomRotate(object):
    def __init__(self, angle=None, center=None, axis="z", always_apply=False, p=0.5):
        self.angle = [-1, 1] if angle is None else angle
        self.axis = axis
        self.always_apply = always_apply
        self.p = p if not self.always_apply else 1
        self.center = center

    def __call__(self, data_dict):
        if random.random() > self.p:
            return data_dict
        angle = np.random.uniform(self.angle[0], self.angle[1]) * np.pi
        rot_cos, rot_sin = np.cos(angle), np.sin(angle)
        if self.axis == "x":
            rot_t = np.array([[1, 0, 0], [0, rot_cos, -rot_sin], [0, rot_sin, rot_cos]])
        elif self.axis == "y":
            rot_t = np.array([[rot_cos, 0, rot_sin], [0, 1, 0], [-rot_sin, 0, rot_cos]])
        elif self.axis == "z":
            rot_t = np.array([[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]])
        else:
            raise NotImplementedError
        if "coord" in data_dict.keys():
            if self.center is None:
                x_min, y_min, z_min = data_dict["coord"].min(axis=0)
                x_max, y_max, z_max = data_dict["coord"].max(axis=0)
                center = [(x_min + x_max) / 2, (y_min + y_max) / 2, (z_min + z_max) / 2]
            else:
                center = self.center
            data_dict["coord"] -= center
            data_dict["coord"] = np.dot(data_dict["coord"], np.transpose(rot_t))
            data_dict["coord"] += center
        if "normal" in data_dict.keys():
            data_dict["normal"] = np.dot(data_dict["normal"], np.transpose(rot_t))
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ¦Â—Â‹Ã¨Â½Â¬Ã¥ÂˆÂ°Ã§Â‰Â¹Ã¥Â®ÂšÃ¨Â§Â’Ã¥ÂºÂ¦
class RandomRotateTargetAngle(object):
    def __init__(
        self, angle=(1 / 2, 1, 3 / 2), center=None, axis="z", always_apply=False, p=0.75
    ):
        self.angle = angle
        self.axis = axis
        self.always_apply = always_apply
        self.p = p if not self.always_apply else 1
        self.center = center

    def __call__(self, data_dict):
        if random.random() > self.p:
            return data_dict
        angle = np.random.choice(self.angle) * np.pi
        rot_cos, rot_sin = np.cos(angle), np.sin(angle)
        if self.axis == "x":
            rot_t = np.array([[1, 0, 0], [0, rot_cos, -rot_sin], [0, rot_sin, rot_cos]])
        elif self.axis == "y":
            rot_t = np.array([[rot_cos, 0, rot_sin], [0, 1, 0], [-rot_sin, 0, rot_cos]])
        elif self.axis == "z":
            rot_t = np.array([[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]])
        else:
            raise NotImplementedError
        if "coord" in data_dict.keys():
            if self.center is None:
                x_min, y_min, z_min = data_dict["coord"].min(axis=0)
                x_max, y_max, z_max = data_dict["coord"].max(axis=0)
                center = [(x_min + x_max) / 2, (y_min + y_max) / 2, (z_min + z_max) / 2]
            else:
                center = self.center
            data_dict["coord"] -= center
            data_dict["coord"] = np.dot(data_dict["coord"], np.transpose(rot_t))
            data_dict["coord"] += center
        if "normal" in data_dict.keys():
            data_dict["normal"] = np.dot(data_dict["normal"], np.transpose(rot_t))
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ§Â¼Â©Ã¦Â”Â¾
class RandomScale(object):
    def __init__(self, scale=None, anisotropic=False):
        self.scale = scale if scale is not None else [0.95, 1.05]
        self.anisotropic = anisotropic

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            scale = np.random.uniform(
                self.scale[0], self.scale[1], 3 if self.anisotropic else 1
            )
            data_dict["coord"] *= scale
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ§Â¿Â»Ã¨Â½Â¬
class RandomFlip(object):
    def __init__(self, p=0.5):
        self.p = p

    def __call__(self, data_dict):
        if np.random.rand() < self.p:
            if "coord" in data_dict.keys():
                data_dict["coord"][:, 0] = -data_dict["coord"][:, 0]
            if "normal" in data_dict.keys():
                data_dict["normal"][:, 0] = -data_dict["normal"][:, 0]
        if np.random.rand() < self.p:
            if "coord" in data_dict.keys():
                data_dict["coord"][:, 1] = -data_dict["coord"][:, 1]
            if "normal" in data_dict.keys():
                data_dict["normal"][:, 1] = -data_dict["normal"][:, 1]
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ¦ÂŠÂ–Ã¥ÂŠÂ¨
class RandomJitter(object):
    def __init__(self, sigma=0.01, clip=0.05):
        assert clip > 0
        self.sigma = sigma
        self.clip = clip

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            jitter = np.clip(
                self.sigma * np.random.randn(data_dict["coord"].shape[0], 3),
                -self.clip,
                self.clip,
            )
            data_dict["coord"] += jitter
        return data_dict


# Ã©Â«Â˜Ã¦Â–Â¯Ã¦ÂŠÂ–Ã¥ÂŠÂ¨
class ClipGaussianJitter(object):
    def __init__(self, scalar=0.02, store_jitter=False):
        self.scalar = scalar
        self.mean = np.mean(3)
        self.cov = np.identity(3)
        self.quantile = 1.96
        self.store_jitter = store_jitter

    def __call__(self, data_dict):
        if "coord" in data_dict.keys():
            jitter = np.random.multivariate_normal(
                self.mean, self.cov, data_dict["coord"].shape[0]
            )
            jitter = self.scalar * np.clip(jitter / 1.96, -1, 1)
            data_dict["coord"] += jitter
            if self.store_jitter:
                data_dict["jitter"] = jitter
        return data_dict
    

# Ã©Â¡ÂºÃ¥ÂºÂÃ¦Â‰Â“Ã¤Â¹Â±
class ShufflePoint(object):
    def __call__(self, data_dict):
        assert "coord" in data_dict.keys()
        shuffle_index = np.arange(data_dict["coord"].shape[0])
        np.random.shuffle(shuffle_index)
        data_dict = index_operator(data_dict, shuffle_index)
        return data_dict


# Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â” Ã¥Â¼ÂºÃ¥ÂºÂ¦Ã¥ÂÂ˜Ã¦ÂÂ¢ Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”
# Intensity Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¦Â£Â€Ã¦ÂµÂ‹Ã¥Â¹Â¶Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
class AutoNormalizeIntensity(object):
    def __init__(self, target_range=(0, 1)):
        """
        Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¦Â£Â€Ã¦ÂµÂ‹ intensity Ã¤Â½ÂÃ¦Â•Â°Ã¥Â¹Â¶Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¥ÂˆÂ°Ã§Â›Â®Ã¦Â Â‡Ã¨ÂŒÂƒÃ¥Â›Â´
        
        Ã¦Â£Â€Ã¦ÂµÂ‹Ã©Â€Â»Ã¨Â¾Â‘Ã¯Â¼Âš
        - Ã¥Â¦Â‚Ã¦ÂÂœ max <= 1.0: Ã¨Â®Â¤Ã¤Â¸ÂºÃ¥Â·Â²Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¯Â¼ÂŒÃ¤Â¸ÂÃ¥Â¤Â„Ã§ÂÂ†
        - Ã¥Â¦Â‚Ã¦ÂÂœ max <= 255: Ã¨Â®Â¤Ã¤Â¸ÂºÃ¦Â˜Â¯ 8 Ã¤Â½ÂÃ¯Â¼ÂŒÃ©Â™Â¤Ã¤Â»Â¥ 255
        - Ã¥Â¦Â‚Ã¦ÂÂœ max <= 65535: Ã¨Â®Â¤Ã¤Â¸ÂºÃ¦Â˜Â¯ 16 Ã¤Â½ÂÃ¯Â¼ÂŒÃ©Â™Â¤Ã¤Â»Â¥ 65535
        - Ã¥ÂÂ¦Ã¥ÂˆÂ™: Ã¤Â½Â¿Ã§Â”Â¨Ã¥Â®ÂÃ©Â™Â…Ã§ÂšÂ„ max-min Ã¨ÂŒÂƒÃ¥Â›Â´Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
        
        Args:
            target_range: Ã§Â›Â®Ã¦Â Â‡Ã¨ÂŒÂƒÃ¥Â›Â´ (min, max)Ã¯Â¼ÂŒÃ©Â»Â˜Ã¨Â®Â¤ (0, 1)
        """
        self.target_range = target_range

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys():
            intensity = data_dict["intensity"].astype(np.float32)
            
            # Ã¦Â£Â€Ã¦ÂµÂ‹Ã¥Â½Â“Ã¥Â‰ÂÃ¨ÂŒÂƒÃ¥Â›Â´
            i_min = intensity.min()
            i_max = intensity.max()
            
            # Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¦Â£Â€Ã¦ÂµÂ‹Ã¤Â½ÂÃ¦Â•Â°Ã¥Â¹Â¶Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
            if i_max <= 1.0:
                # Ã¥Â·Â²Ã§Â»ÂÃ¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¨ÂƒÂ½Ã©ÂœÂ€Ã¨Â¦ÂÃ¨Â°ÂƒÃ¦Â•Â´Ã¨ÂŒÂƒÃ¥Â›Â´
                if self.target_range != (0, 1):
                    # Ã¤Â»Â [0, 1] Ã¦Â˜Â Ã¥Â°Â„Ã¥ÂˆÂ° target_range
                    target_min, target_max = self.target_range
                    intensity = intensity * (target_max - target_min) + target_min
            elif i_max <= 255:
                # 8 Ã¤Â½Â
                intensity = intensity / 255.0
                if self.target_range != (0, 1):
                    target_min, target_max = self.target_range
                    intensity = intensity * (target_max - target_min) + target_min
            elif i_max <= 65535:
                # 16 Ã¤Â½Â
                intensity = intensity / 65535.0
                if self.target_range != (0, 1):
                    target_min, target_max = self.target_range
                    intensity = intensity * (target_max - target_min) + target_min
            else:
                # Ã¦ÂœÂªÃ§ÂŸÂ¥Ã¨ÂŒÂƒÃ¥Â›Â´Ã¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨ min-max Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
                if i_max > i_min:
                    intensity = (intensity - i_min) / (i_max - i_min)
                    if self.target_range != (0, 1):
                        target_min, target_max = self.target_range
                        intensity = intensity * (target_max - target_min) + target_min
            
            data_dict["intensity"] = intensity
        return data_dict


# Intensity Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¯Â¼ÂˆÃ¦ÂŒÂ‡Ã¥Â®ÂšÃ¤Â½ÂÃ¦Â•Â°Ã¯Â¼Â‰
class NormalizeIntensity(object):
    def __init__(self, max_value=65535.0):
        """
        Ã¤Â½Â¿Ã§Â”Â¨Ã¦ÂŒÂ‡Ã¥Â®ÂšÃ§ÂšÂ„Ã¦ÂœÂ€Ã¥Â¤Â§Ã¥Â€Â¼Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ– intensity Ã¥ÂˆÂ° [0, 1]
        
        Args:
            max_value: Ã¦ÂœÂ€Ã¥Â¤Â§Ã¥ÂÂ¯Ã¨ÂƒÂ½Ã§ÂšÂ„Ã¥Â¼ÂºÃ¥ÂºÂ¦Ã¥Â€Â¼Ã¯Â¼ÂˆÃ¥Â¦Â‚ 65535 Ã¨Â¡Â¨Ã§Â¤Âº 16 Ã¤Â½ÂÃ¯Â¼ÂŒ255 Ã¨Â¡Â¨Ã§Â¤Âº 8 Ã¤Â½ÂÃ¯Â¼Â‰
        """
        self.max_value = max_value

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys():
            data_dict["intensity"] = data_dict["intensity"].astype(np.float32) / self.max_value
        return data_dict


# Intensity Ã©ÂšÂÃ¦ÂœÂºÃ§Â¼Â©Ã¦Â”Â¾
class RandomIntensityScale(object):
    def __init__(self, scale=(0.8, 1.2), p=0.95):
        """
        Randomly scale intensity values.
        
        Args:
            scale: (min_scale, max_scale) tuple
            p: Probability of applying the transform
        """
        self.scale = scale
        self.p = p

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys() and np.random.rand() < self.p:
            scale_factor = np.random.uniform(self.scale[0], self.scale[1])
            data_dict["intensity"] = np.clip(
                data_dict["intensity"] * scale_factor, 0, 1
            ).astype(data_dict["intensity"].dtype)
        return data_dict


# Intensity Ã©ÂšÂÃ¦ÂœÂºÃ¥ÂÂÃ§Â§Â»
class RandomIntensityShift(object):
    def __init__(self, shift=(-0.1, 0.1), p=0.95):
        """
        Randomly shift intensity values.
        
        Args:
            shift: (min_shift, max_shift) tuple for additive shift
            p: Probability of applying the transform
        """
        self.shift = shift
        self.p = p

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys() and np.random.rand() < self.p:
            shift_value = np.random.uniform(self.shift[0], self.shift[1])
            if data_dict["intensity"].dtype in [np.float32, np.float64]:
                # Normalized intensity [0, 1]
                data_dict["intensity"] = np.clip(
                    data_dict["intensity"] + shift_value, 0, 1
                )
            else:
                # Raw intensity (e.g., uint16)
                max_val = np.iinfo(data_dict["intensity"].dtype).max
                shift_value_scaled = shift_value * max_val
                data_dict["intensity"] = np.clip(
                    data_dict["intensity"] + shift_value_scaled, 0, max_val
                ).astype(data_dict["intensity"].dtype)
        return data_dict


# Intensity Ã©ÂšÂÃ¦ÂœÂºÃ¥Â™ÂªÃ¥Â£Â°
class RandomIntensityNoise(object):
    def __init__(self, sigma=0.01, p=0.5):
        """
        Add random Gaussian noise to intensity.
        
        Args:
            sigma: Standard deviation of Gaussian noise (for normalized intensity)
            p: Probability of applying the transform
        """
        self.sigma = sigma
        self.p = p

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys() and np.random.rand() < self.p:
            noise = np.random.normal(0, self.sigma, data_dict["intensity"].shape)
            if data_dict["intensity"].dtype in [np.float32, np.float64]:
                # Normalized intensity
                data_dict["intensity"] = np.clip(
                    data_dict["intensity"] + noise, 0, 1
                ).astype(data_dict["intensity"].dtype)
            else:
                # Raw intensity
                max_val = np.iinfo(data_dict["intensity"].dtype).max
                noise_scaled = noise * max_val
                data_dict["intensity"] = np.clip(
                    data_dict["intensity"] + noise_scaled, 0, max_val
                ).astype(data_dict["intensity"].dtype)
        return data_dict


# Intensity Ã©ÂšÂÃ¦ÂœÂºÃ¤Â¸Â¢Ã¥Â¼ÂƒÃ¯Â¼ÂˆÃ§Â½Â®Ã¤Â¸Âº0Ã¯Â¼Â‰
class RandomIntensityDrop(object):
    def __init__(self, drop_ratio=0.1, p=0.2):
        """
        Randomly drop (set to 0) a portion of intensity values.
        
        Args:
            drop_ratio: Ratio of points whose intensity will be set to 0
            p: Probability of applying the transform
        """
        self.drop_ratio = drop_ratio
        self.p = p

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys() and np.random.rand() < self.p:
            n = len(data_dict["intensity"])
            drop_mask = np.random.rand(n) < self.drop_ratio
            data_dict["intensity"][drop_mask] = 0
        return data_dict


# Intensity Gamma Ã¥ÂÂ˜Ã¦ÂÂ¢
class RandomIntensityGamma(object):
    def __init__(self, gamma_range=(0.8, 1.2), p=0.5):
        """
        Apply random gamma correction to intensity.
        
        Args:
            gamma_range: (min_gamma, max_gamma) range for gamma values
            p: Probability of applying the transform
        """
        self.gamma_range = gamma_range
        self.p = p

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys() and np.random.rand() < self.p:
            gamma = np.random.uniform(self.gamma_range[0], self.gamma_range[1])
            
            if data_dict["intensity"].dtype in [np.float32, np.float64]:
                # Normalized intensity [0, 1]
                data_dict["intensity"] = np.power(
                    data_dict["intensity"], gamma
                ).astype(data_dict["intensity"].dtype)
            else:
                # Raw intensity - normalize first, apply gamma, then denormalize
                max_val = np.iinfo(data_dict["intensity"].dtype).max
                normalized = data_dict["intensity"].astype(np.float32) / max_val
                gamma_corrected = np.power(normalized, gamma)
                data_dict["intensity"] = (gamma_corrected * max_val).astype(
                    data_dict["intensity"].dtype
                )
        return data_dict


# Intensity Ã¦Â Â‡Ã¥Â‡Â†Ã¥ÂŒÂ–Ã¯Â¼ÂˆÃ¥Â‡ÂÃ¥ÂÂ‡Ã¥Â€Â¼Ã©Â™Â¤Ã¦Â–Â¹Ã¥Â·Â®Ã¯Â¼Â‰
class StandardNormalizeIntensity(object):
    def __init__(self, mean=None, std=None):
        """
        Standardize intensity by subtracting mean and dividing by std.
        
        Args:
            mean: Mean value for normalization. If None, computed from data.
            std: Std value for normalization. If None, computed from data.
        """
        self.mean = mean
        self.std = std

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys():
            intensity = data_dict["intensity"].astype(np.float32)
            
            # Compute mean and std if not provided
            mean = self.mean if self.mean is not None else intensity.mean()
            std = self.std if self.std is not None else intensity.std()
            
            # Avoid division by zero
            if std == 0:
                std = 1.0
            
            data_dict["intensity"] = ((intensity - mean) / std).astype(np.float32)
        return data_dict


# Intensity MinMax Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
class MinMaxNormalizeIntensity(object):
    def __init__(self, min_val=None, max_val=None, target_range=(0, 1)):
        """
        MinMax normalize intensity to target range.
        
        Args:
            min_val: Minimum value for normalization. If None, computed from data.
            max_val: Maximum value for normalization. If None, computed from data.
            target_range: Target range (min, max) for normalized values.
        """
        self.min_val = min_val
        self.max_val = max_val
        self.target_range = target_range

    def __call__(self, data_dict):
        if "intensity" in data_dict.keys():
            intensity = data_dict["intensity"].astype(np.float32)
            
            # Compute min and max if not provided
            min_val = self.min_val if self.min_val is not None else intensity.min()
            max_val = self.max_val if self.max_val is not None else intensity.max()
            
            # Avoid division by zero
            if max_val == min_val:
                data_dict["intensity"] = np.full_like(
                    intensity, 
                    (self.target_range[0] + self.target_range[1]) / 2,
                    dtype=np.float32
                )
            else:
                # Normalize to [0, 1] first
                normalized = (intensity - min_val) / (max_val - min_val)
                # Scale to target range
                target_min, target_max = self.target_range
                data_dict["intensity"] = (
                    normalized * (target_max - target_min) + target_min
                ).astype(np.float32)
        return data_dict

    
# Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â” Ã©Â¢ÂœÃ¨Â‰Â²Ã¥ÂÂ˜Ã¦ÂÂ¢ Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”
# Ã©Â¢ÂœÃ¨Â‰Â²Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¦Â£Â€Ã¦ÂµÂ‹Ã¥Â¹Â¶Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
class AutoNormalizeColor(object):
    def __init__(self, target_range=(0, 255)):
        """
        Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¦Â£Â€Ã¦ÂµÂ‹ color Ã¤Â½ÂÃ¦Â•Â°Ã¥Â¹Â¶Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¥ÂˆÂ°Ã§Â›Â®Ã¦Â Â‡Ã¨ÂŒÂƒÃ¥Â›Â´
        
        Ã¦Â£Â€Ã¦ÂµÂ‹Ã©Â€Â»Ã¨Â¾Â‘Ã¯Â¼Âš
        - Ã¥Â¦Â‚Ã¦ÂÂœ max <= 1.0: Ã¨Â®Â¤Ã¤Â¸ÂºÃ¥Â·Â²Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¥ÂˆÂ° [0, 1]Ã¯Â¼ÂŒÃ¦Â˜Â Ã¥Â°Â„Ã¥ÂˆÂ° target_range
        - Ã¥Â¦Â‚Ã¦ÂÂœ max <= 255: Ã¨Â®Â¤Ã¤Â¸ÂºÃ¦Â˜Â¯ 8 Ã¤Â½ÂÃ¯Â¼ÂŒÃ¥Â·Â²Ã¥ÂœÂ¨Ã¦Â­Â£Ã§Â¡Â®Ã¨ÂŒÂƒÃ¥Â›Â´
        - Ã¥Â¦Â‚Ã¦ÂÂœ max <= 65535: Ã¨Â®Â¤Ã¤Â¸ÂºÃ¦Â˜Â¯ 16 Ã¤Â½ÂÃ¯Â¼ÂŒÃ¨Â½Â¬Ã¦ÂÂ¢Ã¥ÂˆÂ° 8 Ã¤Â½Â [0, 255]
        - Ã¥ÂÂ¦Ã¥ÂˆÂ™: Ã¤Â½Â¿Ã§Â”Â¨Ã¥Â®ÂÃ©Â™Â…Ã§ÂšÂ„ max-min Ã¨ÂŒÂƒÃ¥Â›Â´Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
        
        Ã¦Â³Â¨Ã¦Â„ÂÃ¯Â¼ÂšÃ¥Â¤Â§Ã©ÂƒÂ¨Ã¥ÂˆÂ†Ã©Â¢ÂœÃ¨Â‰Â²Ã¥Â¢ÂÃ¥Â¼ÂºÃ¯Â¼ÂˆChromaticJitter Ã§Â­Â‰Ã¯Â¼Â‰Ã¦ÂœÂŸÃ¦ÂœÂ› [0, 255] Ã¨ÂŒÂƒÃ¥Â›Â´
        
        Args:
            target_range: Ã§Â›Â®Ã¦Â Â‡Ã¨ÂŒÂƒÃ¥Â›Â´Ã¯Â¼ÂŒÃ©Â»Â˜Ã¨Â®Â¤ (0, 255) Ã§Â”Â¨Ã¤ÂºÂÃ©Â¢ÂœÃ¨Â‰Â²Ã¥Â¢ÂÃ¥Â¼Âº
        """
        self.target_range = target_range

    def __call__(self, data_dict):
        if "color" in data_dict.keys():
            color = data_dict["color"].astype(np.float32)
            
            # Ã¦Â£Â€Ã¦ÂµÂ‹Ã¥Â½Â“Ã¥Â‰ÂÃ¨ÂŒÂƒÃ¥Â›Â´Ã¯Â¼ÂˆÃ¤Â½Â¿Ã§Â”Â¨Ã¦Â‰Â€Ã¦ÂœÂ‰Ã©Â€ÂšÃ©ÂÂ“Ã§ÂšÂ„Ã¦ÂœÂ€Ã¥Â¤Â§Ã¥Â€Â¼Ã¯Â¼Â‰
            c_min = color.min()
            c_max = color.max()
            
            target_min, target_max = self.target_range
            
            # Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¦Â£Â€Ã¦ÂµÂ‹Ã¤Â½ÂÃ¦Â•Â°Ã¥Â¹Â¶Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
            if c_max <= 1.0:
                # Ã¥Â·Â²Ã§Â»ÂÃ¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¥ÂˆÂ° [0, 1]Ã¯Â¼ÂŒÃ¦Â˜Â Ã¥Â°Â„Ã¥ÂˆÂ° target_range
                color = color * (target_max - target_min) + target_min
            elif c_max <= 255:
                # 8 Ã¤Â½ÂÃ¯Â¼ÂŒÃ¥Â·Â²Ã¥ÂœÂ¨ [0, 255] Ã¨ÂŒÂƒÃ¥Â›Â´
                if self.target_range != (0, 255):
                    # Ã©ÂœÂ€Ã¨Â¦ÂÃ¦Â˜Â Ã¥Â°Â„Ã¥ÂˆÂ°Ã¥Â…Â¶Ã¤Â»Â–Ã¨ÂŒÂƒÃ¥Â›Â´
                    color = (color / 255.0) * (target_max - target_min) + target_min
            elif c_max <= 65535:
                # 16 Ã¤Â½ÂÃ¯Â¼ÂŒÃ¨Â½Â¬Ã¦ÂÂ¢Ã¥ÂˆÂ°Ã§Â›Â®Ã¦Â Â‡Ã¨ÂŒÂƒÃ¥Â›Â´
                color = (color / 65535.0) * (target_max - target_min) + target_min
            else:
                # Ã¦ÂœÂªÃ§ÂŸÂ¥Ã¨ÂŒÂƒÃ¥Â›Â´Ã¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨ min-max Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
                if c_max > c_min:
                    color = (color - c_min) / (c_max - c_min)
                    color = color * (target_max - target_min) + target_min
            
            data_dict["color"] = color
        return data_dict


# Ã©Â¢ÂœÃ¨Â‰Â²Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¯Â¼ÂˆÃ¦ÂŒÂ‡Ã¥Â®ÂšÃ¤Â½ÂÃ¦Â•Â°Ã¯Â¼Â‰
class NormalizeColor(object):
    def __init__(self, source_bits=16, target_range=(0, 255)):
        """
        Ã¤Â½Â¿Ã§Â”Â¨Ã¦ÂŒÂ‡Ã¥Â®ÂšÃ§ÂšÂ„Ã¤Â½ÂÃ¦Â•Â°Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ– color
        
        Args:
            source_bits: Ã¦ÂºÂÃ¦Â•Â°Ã¦ÂÂ®Ã¤Â½ÂÃ¦Â•Â°Ã¯Â¼Âˆ8 Ã¦ÂˆÂ– 16Ã¯Â¼Â‰
            target_range: Ã§Â›Â®Ã¦Â Â‡Ã¨ÂŒÂƒÃ¥Â›Â´Ã¯Â¼ÂŒÃ©Â»Â˜Ã¨Â®Â¤ (0, 255)
        """
        self.source_bits = source_bits
        self.target_range = target_range
        self.source_max = (2 ** source_bits) - 1

    def __call__(self, data_dict):
        if "color" in data_dict.keys():
            color = data_dict["color"].astype(np.float32)
            
            # Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¥ÂˆÂ° [0, 1]
            color = color / self.source_max
            
            # Ã¦Â˜Â Ã¥Â°Â„Ã¥ÂˆÂ°Ã§Â›Â®Ã¦Â Â‡Ã¨ÂŒÂƒÃ¥Â›Â´
            target_min, target_max = self.target_range
            color = color * (target_max - target_min) + target_min
            
            data_dict["color"] = color
        return data_dict


# Ã©Â¢ÂœÃ¨Â‰Â²Ã¥Â¯Â¹Ã¦Â¯Â”Ã¥ÂºÂ¦Ã¥Â¢ÂÃ¥Â¼Âº
class ChromaticAutoContrast(object):
    def __init__(self, p=0.2, blend_factor=None):
        self.p = p
        self.blend_factor = blend_factor

    def __call__(self, data_dict):
        if "color" in data_dict.keys() and np.random.rand() < self.p:
            lo = np.min(data_dict["color"], 0, keepdims=True)
            hi = np.max(data_dict["color"], 0, keepdims=True)
            scale = 255 / (hi - lo)
            contrast_feat = (data_dict["color"][:, :3] - lo) * scale
            blend_factor = (
                np.random.rand() if self.blend_factor is None else self.blend_factor
            )
            data_dict["color"][:, :3] = (1 - blend_factor) * data_dict["color"][
                :, :3
            ] + blend_factor * contrast_feat
        return data_dict


# Ã©Â¢ÂœÃ¨Â‰Â²Ã©ÂšÂÃ¦ÂœÂºÃ¥Â¹Â³Ã§Â§Â»
class ChromaticTranslation(object):
    def __init__(self, p=0.95, ratio=0.05):
        self.p = p
        self.ratio = ratio

    def __call__(self, data_dict):
        if "color" in data_dict.keys() and np.random.rand() < self.p:
            tr = (np.random.rand(1, 3) - 0.5) * 255 * 2 * self.ratio
            data_dict["color"][:, :3] = np.clip(tr + data_dict["color"][:, :3], 0, 255)
        return data_dict


# Ã©Â¢ÂœÃ¨Â‰Â²Ã©ÂšÂÃ¦ÂœÂºÃ¦ÂŠÂ–Ã¥ÂŠÂ¨
class ChromaticJitter(object):
    def __init__(self, p=0.95, std=0.005):
        self.p = p
        self.std = std

    def __call__(self, data_dict):
        if "color" in data_dict.keys() and np.random.rand() < self.p:
            noise = np.random.randn(data_dict["color"].shape[0], 3)
            noise *= self.std * 255
            data_dict["color"][:, :3] = np.clip(
                noise + data_dict["color"][:, :3], 0, 255
            )
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ©Â¢ÂœÃ¨Â‰Â²Ã§ÂÂ°Ã¥ÂºÂ¦Ã¥ÂŒÂ–
class RandomColorGrayScale(object):
    def __init__(self, p):
        self.p = p

    @staticmethod
    def rgb_to_grayscale(color, num_output_channels=1):
        if color.shape[-1] < 3:
            raise TypeError(
                "Input color should have at least 3 dimensions, but found {}".format(
                    color.shape[-1]
                )
            )

        if num_output_channels not in (1, 3):
            raise ValueError("num_output_channels should be either 1 or 3")

        r, g, b = color[..., 0], color[..., 1], color[..., 2]
        gray = (0.2989 * r + 0.587 * g + 0.114 * b).astype(color.dtype)
        gray = np.expand_dims(gray, axis=-1)

        if num_output_channels == 3:
            gray = np.broadcast_to(gray, color.shape)

        return gray

    def __call__(self, data_dict):
        if np.random.rand() < self.p:
            data_dict["color"] = self.rgb_to_grayscale(data_dict["color"], 3)
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ©Â¢ÂœÃ¨Â‰Â²Ã¦ÂŠÂ–Ã¥ÂŠÂ¨
class RandomColorJitter(object):
    """
    Random Color Jitter for 3D point cloud (refer torchvision)
    """

    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, p=0.95):
        self.brightness = self._check_input(brightness, "brightness")
        self.contrast = self._check_input(contrast, "contrast")
        self.saturation = self._check_input(saturation, "saturation")
        self.hue = self._check_input(
            hue, "hue", center=0, bound=(-0.5, 0.5), clip_first_on_zero=False
        )
        self.p = p

    @staticmethod
    def _check_input(
        value, name, center=1, bound=(0, float("inf")), clip_first_on_zero=True
    ):
        if isinstance(value, numbers.Number):
            if value < 0:
                raise ValueError(
                    "If {} is a single number, it must be non negative.".format(name)
                )
            value = [center - float(value), center + float(value)]
            if clip_first_on_zero:
                value[0] = max(value[0], 0.0)
        elif isinstance(value, (tuple, list)) and len(value) == 2:
            if not bound[0] <= value[0] <= value[1] <= bound[1]:
                raise ValueError("{} values should be between {}".format(name, bound))
        else:
            raise TypeError(
                "{} should be a single number or a list/tuple with length 2.".format(
                    name
                )
            )

        # if value is 0 or (1., 1.) for brightness/contrast/saturation
        # or (0., 0.) for hue, do nothing
        if value[0] == value[1] == center:
            value = None
        return value

    @staticmethod
    def blend(color1, color2, ratio):
        ratio = float(ratio)
        bound = 255.0
        return (
            (ratio * color1 + (1.0 - ratio) * color2)
            .clip(0, bound)
            .astype(color1.dtype)
        )

    @staticmethod
    def rgb2hsv(rgb):
        r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]
        maxc = np.max(rgb, axis=-1)
        minc = np.min(rgb, axis=-1)
        eqc = maxc == minc
        cr = maxc - minc
        s = cr / (np.ones_like(maxc) * eqc + maxc * (1 - eqc))
        cr_divisor = np.ones_like(maxc) * eqc + cr * (1 - eqc)
        rc = (maxc - r) / cr_divisor
        gc = (maxc - g) / cr_divisor
        bc = (maxc - b) / cr_divisor

        hr = (maxc == r) * (bc - gc)
        hg = ((maxc == g) & (maxc != r)) * (2.0 + rc - bc)
        hb = ((maxc != g) & (maxc != r)) * (4.0 + gc - rc)
        h = hr + hg + hb
        h = (h / 6.0 + 1.0) % 1.0
        return np.stack((h, s, maxc), axis=-1)

    @staticmethod
    def hsv2rgb(hsv):
        h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]
        i = np.floor(h * 6.0)
        f = (h * 6.0) - i
        i = i.astype(np.int32)

        p = np.clip((v * (1.0 - s)), 0.0, 1.0)
        q = np.clip((v * (1.0 - s * f)), 0.0, 1.0)
        t = np.clip((v * (1.0 - s * (1.0 - f))), 0.0, 1.0)
        i = i % 6
        mask = np.expand_dims(i, axis=-1) == np.arange(6)

        a1 = np.stack((v, q, p, p, t, v), axis=-1)
        a2 = np.stack((t, v, v, q, p, p), axis=-1)
        a3 = np.stack((p, p, t, v, v, q), axis=-1)
        a4 = np.stack((a1, a2, a3), axis=-1)

        return np.einsum("...na, ...nab -> ...nb", mask.astype(hsv.dtype), a4)

    def adjust_brightness(self, color, brightness_factor):
        if brightness_factor < 0:
            raise ValueError(
                "brightness_factor ({}) is not non-negative.".format(brightness_factor)
            )

        return self.blend(color, np.zeros_like(color), brightness_factor)

    def adjust_contrast(self, color, contrast_factor):
        if contrast_factor < 0:
            raise ValueError(
                "contrast_factor ({}) is not non-negative.".format(contrast_factor)
            )
        mean = np.mean(RandomColorGrayScale.rgb_to_grayscale(color))
        return self.blend(color, mean, contrast_factor)

    def adjust_saturation(self, color, saturation_factor):
        if saturation_factor < 0:
            raise ValueError(
                "saturation_factor ({}) is not non-negative.".format(saturation_factor)
            )
        gray = RandomColorGrayScale.rgb_to_grayscale(color)
        return self.blend(color, gray, saturation_factor)

    def adjust_hue(self, color, hue_factor):
        if not (-0.5 <= hue_factor <= 0.5):
            raise ValueError(
                "hue_factor ({}) is not in [-0.5, 0.5].".format(hue_factor)
            )
        orig_dtype = color.dtype
        hsv = self.rgb2hsv(color / 255.0)
        h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]
        h = (h + hue_factor) % 1.0
        hsv = np.stack((h, s, v), axis=-1)
        color_hue_adj = (self.hsv2rgb(hsv) * 255.0).astype(orig_dtype)
        return color_hue_adj

    @staticmethod
    def get_params(brightness, contrast, saturation, hue):
        fn_idx = torch.randperm(4)
        b = (
            None
            if brightness is None
            else np.random.uniform(brightness[0], brightness[1])
        )
        c = None if contrast is None else np.random.uniform(contrast[0], contrast[1])
        s = (
            None
            if saturation is None
            else np.random.uniform(saturation[0], saturation[1])
        )
        h = None if hue is None else np.random.uniform(hue[0], hue[1])
        return fn_idx, b, c, s, h

    def __call__(self, data_dict):
        (
            fn_idx,
            brightness_factor,
            contrast_factor,
            saturation_factor,
            hue_factor,
        ) = self.get_params(self.brightness, self.contrast, self.saturation, self.hue)

        for fn_id in fn_idx:
            if (
                fn_id == 0
                and brightness_factor is not None
                and np.random.rand() < self.p
            ):
                data_dict["color"] = self.adjust_brightness(
                    data_dict["color"], brightness_factor
                )
            elif (
                fn_id == 1 and contrast_factor is not None and np.random.rand() < self.p
            ):
                data_dict["color"] = self.adjust_contrast(
                    data_dict["color"], contrast_factor
                )
            elif (
                fn_id == 2
                and saturation_factor is not None
                and np.random.rand() < self.p
            ):
                data_dict["color"] = self.adjust_saturation(
                    data_dict["color"], saturation_factor
                )
            elif fn_id == 3 and hue_factor is not None and np.random.rand() < self.p:
                data_dict["color"] = self.adjust_hue(data_dict["color"], hue_factor)
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ©Â¢ÂœÃ¨Â‰Â²Ã©Â¥Â±Ã¥Â’ÂŒÃ¥ÂºÂ¦
class HueSaturationTranslation(object):
    @staticmethod
    def rgb_to_hsv(rgb):
        # Translated from source of colorsys.rgb_to_hsv
        # r,g,b should be a numpy arrays with values between 0 and 255
        # rgb_to_hsv returns an array of floats between 0.0 and 1.0.
        rgb = rgb.astype("float")
        hsv = np.zeros_like(rgb)
        # in case an RGBA array was passed, just copy the A channel
        hsv[..., 3:] = rgb[..., 3:]
        r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]
        maxc = np.max(rgb[..., :3], axis=-1)
        minc = np.min(rgb[..., :3], axis=-1)
        hsv[..., 2] = maxc
        mask = maxc != minc
        hsv[mask, 1] = (maxc - minc)[mask] / maxc[mask]
        rc = np.zeros_like(r)
        gc = np.zeros_like(g)
        bc = np.zeros_like(b)
        rc[mask] = (maxc - r)[mask] / (maxc - minc)[mask]
        gc[mask] = (maxc - g)[mask] / (maxc - minc)[mask]
        bc[mask] = (maxc - b)[mask] / (maxc - minc)[mask]
        hsv[..., 0] = np.select(
            [r == maxc, g == maxc], [bc - gc, 2.0 + rc - bc], default=4.0 + gc - rc
        )
        hsv[..., 0] = (hsv[..., 0] / 6.0) % 1.0
        return hsv

    @staticmethod
    def hsv_to_rgb(hsv):
        # Translated from source of colorsys.hsv_to_rgb
        # h,s should be a numpy arrays with values between 0.0 and 1.0
        # v should be a numpy array with values between 0.0 and 255.0
        # hsv_to_rgb returns an array of uints between 0 and 255.
        rgb = np.empty_like(hsv)
        rgb[..., 3:] = hsv[..., 3:]
        h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]
        i = (h * 6.0).astype("uint8")
        f = (h * 6.0) - i
        p = v * (1.0 - s)
        q = v * (1.0 - s * f)
        t = v * (1.0 - s * (1.0 - f))
        i = i % 6
        conditions = [s == 0.0, i == 1, i == 2, i == 3, i == 4, i == 5]
        rgb[..., 0] = np.select(conditions, [v, q, p, p, t, v], default=v)
        rgb[..., 1] = np.select(conditions, [v, v, v, q, p, p], default=t)
        rgb[..., 2] = np.select(conditions, [v, p, t, v, v, q], default=p)
        return rgb.astype("uint8")

    def __init__(self, hue_max=0.5, saturation_max=0.2):
        self.hue_max = hue_max
        self.saturation_max = saturation_max

    def __call__(self, data_dict):
        if "color" in data_dict.keys():
            # Assume color[:, :3] is rgb
            hsv = HueSaturationTranslation.rgb_to_hsv(data_dict["color"][:, :3])
            hue_val = (np.random.rand() - 0.5) * 2 * self.hue_max
            sat_ratio = 1 + (np.random.rand() - 0.5) * 2 * self.saturation_max
            hsv[..., 0] = np.remainder(hue_val + hsv[..., 0] + 1, 1)
            hsv[..., 1] = np.clip(sat_ratio * hsv[..., 1], 0, 1)
            data_dict["color"][:, :3] = np.clip(
                HueSaturationTranslation.hsv_to_rgb(hsv), 0, 255
            )
        return data_dict


# Ã©ÂšÂÃ¦ÂœÂºÃ©Â¢ÂœÃ¨Â‰Â²Ã¤Â¸Â¢Ã¥Â¼Âƒ
class RandomColorDrop(object):
    def __init__(self, p=0.2, color_augment=0.0):
        self.p = p
        self.color_augment = color_augment

    def __call__(self, data_dict):
        if "color" in data_dict.keys() and np.random.rand() < self.p:
            data_dict["color"] *= self.color_augment
        return data_dict

    def __repr__(self):
        return "RandomColorDrop(color_augment: {}, p: {})".format(
            self.color_augment, self.p
        )


# Ã¥Â¼Â¹Ã¦Â€Â§Ã¥Â¤Â±Ã§ÂœÂŸÃ¯Â¼ÂŒÃ¦Â¨Â¡Ã¦Â‹ÂŸÃ¨Â‡ÂªÃ§Â„Â¶Ã¥ÂÂ˜Ã¥Â½Â¢
class ElasticDistortion(object):
    def __init__(self, distortion_params=None):
        self.distortion_params = (
            [[0.2, 0.4], [0.8, 1.6]] if distortion_params is None else distortion_params
        )

    @staticmethod
    def elastic_distortion(coords, granularity, magnitude):
        """
        Apply elastic distortion on sparse coordinate space.
        pointcloud: numpy array of (number of points, at least 3 spatial dims)
        granularity: size of the noise grid (in same scale[m/cm] as the voxel grid)
        magnitude: noise multiplier
        """
        blurx = np.ones((3, 1, 1, 1)).astype("float32") / 3
        blury = np.ones((1, 3, 1, 1)).astype("float32") / 3
        blurz = np.ones((1, 1, 3, 1)).astype("float32") / 3
        coords_min = coords.min(0)

        # Create Gaussian noise tensor of the size given by granularity.
        noise_dim = ((coords - coords_min).max(0) // granularity).astype(int) + 3
        noise = np.random.randn(*noise_dim, 3).astype(np.float32)

        # Smoothing.
        for _ in range(2):
            noise = scipy.ndimage.filters.convolve(
                noise, blurx, mode="constant", cval=0
            )
            noise = scipy.ndimage.filters.convolve(
                noise, blury, mode="constant", cval=0
            )
            noise = scipy.ndimage.filters.convolve(
                noise, blurz, mode="constant", cval=0
            )

        # Trilinear interpolate noise filters for each spatial dimensions.
        ax = [
            np.linspace(d_min, d_max, d)
            for d_min, d_max, d in zip(
                coords_min - granularity,
                coords_min + granularity * (noise_dim - 2),
                noise_dim,
            )
        ]
        interp = scipy.interpolate.RegularGridInterpolator(
            ax, noise, bounds_error=False, fill_value=0
        )
        coords += interp(coords) * magnitude
        return coords

    def __call__(self, data_dict):
        if "coord" in data_dict.keys() and self.distortion_params is not None:
            if random.random() < 0.95:
                for granularity, magnitude in self.distortion_params:
                    data_dict["coord"] = self.elastic_distortion(
                        data_dict["coord"], granularity, magnitude
                    )
        return data_dict


# Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â” Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã¯Â¼Âˆh_normÃ¯Â¼Â‰Ã¥ÂÂ˜Ã¦ÂÂ¢ Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”
# Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¯Â¼ÂˆÃ¥ÂÂ¯Ã©Â€Â‰Ã¨Â£ÂÃ¥Â‰ÂªÃ¯Â¼Â‰
class AutoNormalizeHNorm(object):
    def __init__(self, clip_range=None):
        """
        Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ– h_normÃ¯Â¼ÂˆÃ¥ÂÂ¯Ã©Â€Â‰Ã¨Â£ÂÃ¥Â‰ÂªÃ¥Â¼Â‚Ã¥Â¸Â¸Ã¥Â€Â¼Ã¯Â¼Â‰
        
        Ã©Â»Â˜Ã¨Â®Â¤Ã¨Â¡ÂŒÃ¤Â¸ÂºÃ¯Â¼Âˆclip_range=NoneÃ¯Â¼Â‰Ã¯Â¼Âš
        - Ã¤Â¸ÂÃ¨Â£ÂÃ¥Â‰ÂªÃ¤Â»Â»Ã¤Â½Â•Ã¥Â€Â¼Ã¯Â¼ÂŒÃ¤Â¿ÂÃ§Â•Â™Ã¨Â´ÂŸÃ¥Â€Â¼Ã¥Â’ÂŒÃ¦ÂÂÃ¥Â¤Â§Ã¥Â€Â¼
        - Ã¨Â´ÂŸÃ¥Â€Â¼Ã¥ÂÂ¯Ã¨ÂƒÂ½Ã¤Â»Â£Ã¨Â¡Â¨Ã¥ÂœÂ°Ã¤Â¸Â‹Ã§Â»Â“Ã¦ÂÂ„Ã¯Â¼ÂˆÃ¥ÂœÂ°Ã¤Â¸Â‹Ã¥Â®Â¤Ã£Â€ÂÃ©ÂšÂ§Ã©ÂÂ“Ã£Â€ÂÃ¥ÂÂ‘Ã¦Â´ÂÃ¯Â¼Â‰
        - Ã¦ÂÂÃ¥Â¤Â§Ã¥Â€Â¼Ã¥ÂÂ¯Ã¨ÂƒÂ½Ã¤Â»Â£Ã¨Â¡Â¨Ã§ÂœÂŸÃ¥Â®ÂÃ©Â«Â˜Ã¥Â±Â‚Ã¥Â»ÂºÃ§Â­Â‘Ã¦ÂˆÂ–Ã¥Â™ÂªÃ¥Â£Â°
        - Ã¨Â®Â©Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥Â­Â¦Ã¤Â¹Â Ã¨Â¯Â†Ã¥ÂˆÂ«Ã¥Â’ÂŒÃ¥Â¤Â„Ã§ÂÂ†Ã¥Â¼Â‚Ã¥Â¸Â¸Ã¥Â€Â¼Ã¯Â¼ÂŒÃ¥Â¢ÂÃ¥Â¼ÂºÃ©Â²ÂÃ¦Â£Â’Ã¦Â€Â§
        
        Ã¥ÂÂ¯Ã©Â€Â‰Ã¨Â£ÂÃ¥Â‰ÂªÃ¯Â¼Âˆclip_range=(min, max)Ã¯Â¼Â‰Ã¯Â¼Âš
        - Ã¥Â¦Â‚ (0, 50) Ã¥Â°Â†Ã©Â«Â˜Ã§Â¨Â‹Ã©Â™ÂÃ¥ÂˆÂ¶Ã¥ÂœÂ¨ 0-50mÃ¯Â¼ÂˆÃ¦ÂÂ’Ã©Â™Â¤Ã¦Â˜ÂÃ¦Â˜Â¾Ã¥Â¼Â‚Ã¥Â¸Â¸Ã¥Â€Â¼Ã¯Â¼Â‰
        - Ã¥Â¦Â‚ (-5, 100) Ã¤Â¿ÂÃ§Â•Â™Ã¥ÂÂˆÃ§ÂÂ†Ã§ÂšÂ„Ã¥ÂœÂ°Ã¤Â¸Â‹Ã¥Â’ÂŒÃ©Â«Â˜Ã§Â©ÂºÃ¨ÂŒÂƒÃ¥Â›Â´
        
        Args:
            clip_range: Ã¨Â£ÂÃ¥Â‰ÂªÃ¨ÂŒÂƒÃ¥Â›Â´ (min, max)Ã¯Â¼ÂŒÃ©Â»Â˜Ã¨Â®Â¤ NoneÃ¯Â¼ÂˆÃ¤Â¸ÂÃ¨Â£ÂÃ¥Â‰ÂªÃ¯Â¼Â‰
                       None: Ã¤Â¸ÂÃ¨Â£ÂÃ¥Â‰ÂªÃ¯Â¼ÂŒÃ¤Â¿ÂÃ§Â•Â™Ã¦Â‰Â€Ã¦ÂœÂ‰Ã¥Â€Â¼Ã¯Â¼ÂˆÃ¦ÂÂ¨Ã¨ÂÂÃ¯Â¼Â‰
                       (min, max): Ã¨Â£ÂÃ¥Â‰ÂªÃ¥ÂˆÂ°Ã¦ÂŒÂ‡Ã¥Â®ÂšÃ¨ÂŒÂƒÃ¥Â›Â´
                       (None, max): Ã¥ÂÂªÃ¨Â£ÂÃ¥Â‰ÂªÃ¤Â¸ÂŠÃ§Â•ÂŒ
                       (min, None): Ã¥ÂÂªÃ¨Â£ÂÃ¥Â‰ÂªÃ¤Â¸Â‹Ã§Â•ÂŒ
        """
        self.clip_range = clip_range

    def __call__(self, data_dict):
        if "h_norm" in data_dict.keys():
            h_norm = data_dict["h_norm"].astype(np.float32)
            
            # Ã¥ÂÂ¯Ã©Â€Â‰Ã¨Â£ÂÃ¥Â‰ÂªÃ¥Â¼Â‚Ã¥Â¸Â¸Ã¥Â€Â¼
            if self.clip_range is not None:
                if self.clip_range[0] is not None:
                    h_norm = np.maximum(h_norm, self.clip_range[0])
                if self.clip_range[1] is not None:
                    h_norm = np.minimum(h_norm, self.clip_range[1])
            
            data_dict["h_norm"] = h_norm
        return data_dict


# Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã¦Â Â‡Ã¥Â‡Â†Ã¥ÂŒÂ–
class StandardNormalizeHNorm(object):
    def __init__(self, mean=None, std=None):
        """
        Ã¦Â Â‡Ã¥Â‡Â†Ã¥ÂŒÂ– h_normÃ¯Â¼ÂˆZ-score Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¯Â¼Â‰
        
        Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ©ÂœÂ€Ã¨Â¦ÂÃ©Â›Â¶Ã¥ÂÂ‡Ã¥Â€Â¼Ã£Â€ÂÃ¥ÂÂ•Ã¤Â½ÂÃ¦Â–Â¹Ã¥Â·Â®Ã¨Â¾Â“Ã¥Â…Â¥Ã§ÂšÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹
        
        Args:
            mean: Ã¥ÂÂ‡Ã¥Â€Â¼Ã¯Â¼ÂŒÃ¥Â¦Â‚Ã¦ÂÂœÃ¤Â¸Âº None Ã¥ÂˆÂ™Ã¤Â»ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¨Â®Â¡Ã§Â®Â—
            std: Ã¦Â Â‡Ã¥Â‡Â†Ã¥Â·Â®Ã¯Â¼ÂŒÃ¥Â¦Â‚Ã¦ÂÂœÃ¤Â¸Âº None Ã¥ÂˆÂ™Ã¤Â»ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¨Â®Â¡Ã§Â®Â—
        """
        self.mean = mean
        self.std = std

    def __call__(self, data_dict):
        if "h_norm" in data_dict.keys():
            h_norm = data_dict["h_norm"].astype(np.float32)
            
            mean = self.mean if self.mean is not None else h_norm.mean()
            std = self.std if self.std is not None else h_norm.std()
            
            # Ã©ÂÂ¿Ã¥Â…ÂÃ©Â™Â¤Ã©Â›Â¶
            if std == 0:
                std = 1.0
            
            data_dict["h_norm"] = ((h_norm - mean) / std).astype(np.float32)
        return data_dict


# Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã©ÂšÂÃ¦ÂœÂºÃ§Â¼Â©Ã¦Â”Â¾
class RandomHNormScale(object):
    def __init__(self, scale=(0.9, 1.1), p=0.5):
        """
        Ã©ÂšÂÃ¦ÂœÂºÃ§Â¼Â©Ã¦Â”Â¾ h_norm
        
        Ã¦Â¨Â¡Ã¦Â‹ÂŸÃ¤Â¸ÂÃ¥ÂÂŒÃ§ÂšÂ„Ã¥ÂœÂ°Ã©ÂÂ¢Ã¨Â¯Â†Ã¥ÂˆÂ«Ã§Â²Â¾Ã¥ÂºÂ¦Ã¦ÂˆÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã¦ÂµÂ‹Ã©Â‡ÂÃ¨Â¯Â¯Ã¥Â·Â®
        
        Args:
            scale: Ã§Â¼Â©Ã¦Â”Â¾Ã¨ÂŒÂƒÃ¥Â›Â´ (min_scale, max_scale)
            p: Ã¥ÂºÂ”Ã§Â”Â¨Ã¦Â¦Â‚Ã§ÂÂ‡
        """
        self.scale = scale
        self.p = p

    def __call__(self, data_dict):
        if "h_norm" in data_dict.keys() and np.random.rand() < self.p:
            scale_factor = np.random.uniform(self.scale[0], self.scale[1])
            data_dict["h_norm"] = (data_dict["h_norm"] * scale_factor).astype(
                data_dict["h_norm"].dtype
            )
        return data_dict


# Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã©ÂšÂÃ¦ÂœÂºÃ¥Â™ÂªÃ¥Â£Â°
class RandomHNormNoise(object):
    def __init__(self, sigma=0.1, p=0.5):
        """
        Ã¤Â¸Âº h_norm Ã¦Â·Â»Ã¥ÂŠÂ Ã©ÂšÂÃ¦ÂœÂºÃ©Â«Â˜Ã¦Â–Â¯Ã¥Â™ÂªÃ¥Â£Â°
        
        Ã¦Â¨Â¡Ã¦Â‹ÂŸÃ¥ÂœÂ°Ã©ÂÂ¢Ã©Â«Â˜Ã§Â¨Â‹Ã¤Â¼Â°Ã¨Â®Â¡Ã§ÂšÂ„Ã¥Â±Â€Ã©ÂƒÂ¨Ã¨Â¯Â¯Ã¥Â·Â®
        
        Args:
            sigma: Ã©Â«Â˜Ã¦Â–Â¯Ã¥Â™ÂªÃ¥Â£Â°Ã§ÂšÂ„Ã¦Â Â‡Ã¥Â‡Â†Ã¥Â·Â®Ã¯Â¼ÂˆÃ¥ÂÂ•Ã¤Â½ÂÃ¤Â¸Â h_norm Ã§Â›Â¸Ã¥ÂÂŒÃ¯Â¼ÂŒÃ©Â€ÂšÃ¥Â¸Â¸Ã¦Â˜Â¯Ã§Â±Â³Ã¯Â¼Â‰
            p: Ã¥ÂºÂ”Ã§Â”Â¨Ã¦Â¦Â‚Ã§ÂÂ‡
        """
        self.sigma = sigma
        self.p = p

    def __call__(self, data_dict):
        if "h_norm" in data_dict.keys() and np.random.rand() < self.p:
            noise = np.random.normal(0, self.sigma, data_dict["h_norm"].shape)
            data_dict["h_norm"] = (data_dict["h_norm"] + noise).astype(
                data_dict["h_norm"].dtype
            )
        return data_dict


# Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã¥Â¯Â¹Ã¦Â•Â°Ã¥ÂÂ˜Ã¦ÂÂ¢
class LogTransformHNorm(object):
    def __init__(self, epsilon=1e-6):
        """
        Ã¥Â¯Â¹ h_norm Ã¨Â¿Â›Ã¨Â¡ÂŒÃ¥Â¯Â¹Ã¦Â•Â°Ã¥ÂÂ˜Ã¦ÂÂ¢
        
        Ã§Â”Â¨Ã¤ÂºÂÃ¥Â¤Â„Ã§ÂÂ†Ã©Â«Â˜Ã¥ÂºÂ¦Ã¨ÂŒÂƒÃ¥Â›Â´Ã¥Â¾ÂˆÃ¥Â¤Â§Ã§ÂšÂ„Ã¥ÂœÂºÃ¦Â™Â¯Ã¯Â¼ÂˆÃ¥Â¦Â‚Ã¥Â»ÂºÃ§Â­Â‘Ã§Â‰Â©Ã¥Â’ÂŒÃ¥ÂœÂ°Ã©ÂÂ¢Ã¯Â¼Â‰
        Ã¤Â½Â¿Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥Â¯Â¹Ã¤Â¸ÂÃ¥ÂÂŒÃ©Â«Â˜Ã¥ÂºÂ¦Ã¥Â°ÂºÃ¥ÂºÂ¦Ã¦Â›Â´Ã¦Â•ÂÃ¦Â„ÂŸ
        
        Args:
            epsilon: Ã©ÂÂ¿Ã¥Â…Â log(0) Ã§ÂšÂ„Ã¥Â°ÂÃ¥Â¸Â¸Ã¦Â•Â°
        """
        self.epsilon = epsilon

    def __call__(self, data_dict):
        if "h_norm" in data_dict.keys():
            h_norm = data_dict["h_norm"].astype(np.float32)
            # Ã§Â¡Â®Ã¤Â¿ÂÃ©ÂÂÃ¨Â´ÂŸ
            h_norm = np.maximum(h_norm, 0)
            # Ã¥Â¯Â¹Ã¦Â•Â°Ã¥ÂÂ˜Ã¦ÂÂ¢
            data_dict["h_norm"] = np.log(h_norm + self.epsilon).astype(np.float32)
        return data_dict


# Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã©Â«Â˜Ã§Â¨Â‹Ã¥ÂˆÂ†Ã¦Â¡Â¶Ã§Â¼Â–Ã§Â Â
class BinHNorm(object):
    def __init__(self, bins=10, range=(0, 20)):
        """
        Ã¥Â°Â† h_norm Ã§Â¦Â»Ã¦Â•Â£Ã¥ÂŒÂ–Ã¤Â¸ÂºÃ¦Â¡Â¶Ã¯Â¼ÂˆbinsÃ¯Â¼Â‰
        
        Ã¥Â°Â†Ã¨Â¿ÂÃ§Â»Â­Ã§ÂšÂ„Ã©Â«Â˜Ã¥ÂºÂ¦Ã¥Â€Â¼Ã¨Â½Â¬Ã¦ÂÂ¢Ã¤Â¸ÂºÃ§Â¦Â»Ã¦Â•Â£Ã§ÂšÂ„Ã©Â«Â˜Ã¥ÂºÂ¦Ã§Â­Â‰Ã§ÂºÂ§
        
        Args:
            bins: Ã¦Â¡Â¶Ã§ÂšÂ„Ã¦Â•Â°Ã©Â‡Â
            range: Ã©Â«Â˜Ã¥ÂºÂ¦Ã¨ÂŒÂƒÃ¥Â›Â´ (min, max)
        """
        self.bins = bins
        self.range = range

    def __call__(self, data_dict):
        if "h_norm" in data_dict.keys():
            h_norm = data_dict["h_norm"].astype(np.float32)
            
            # Ã¤Â½Â¿Ã§Â”Â¨ numpy Ã§ÂšÂ„ digitize Ã¨Â¿Â›Ã¨Â¡ÂŒÃ¥ÂˆÂ†Ã¦Â¡Â¶
            bin_edges = np.linspace(self.range[0], self.range[1], self.bins + 1)
            binned = np.digitize(h_norm, bin_edges) - 1
            
            # Ã¨Â£ÂÃ¥Â‰ÂªÃ¥ÂˆÂ° [0, bins-1]
            binned = np.clip(binned, 0, self.bins - 1)
            
            # Ã¨Â½Â¬Ã¦ÂÂ¢Ã¤Â¸Âº floatÃ¯Â¼ÂˆÃ¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–Ã¥ÂˆÂ° [0, 1]Ã¯Â¼Â‰
            data_dict["h_norm"] = (binned / (self.bins - 1)).astype(np.float32)
        return data_dict


# Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â” Ã¥Â™ÂªÃ§Â‚Â¹Ã¦Â³Â¨Ã¥Â…Â¥Ã¥Â¢ÂÃ¥Â¼Âº Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”
# Ã¦Â·Â»Ã¥ÂŠÂ Ã¦ÂÂÃ§Â«Â¯Ã©Â«Â˜Ã¥ÂºÂ¦Ã¥Â™ÂªÃ§Â‚Â¹
class AddExtremeOutliers(object):
    def __init__(self, 
                 num_outliers=None, 
                 ratio=0.01,
                 height_range=(-10, 100),
                 height_mode='uniform',
                 intensity_range=(0, 1),
                 color_value=(128, 128, 128),
                 class_label=None,
                 p=0.5):
        """
        Ã¦Â·Â»Ã¥ÂŠÂ Ã¦ÂÂÃ§Â«Â¯Ã©Â«Â˜Ã¥ÂºÂ¦Ã¥Â™ÂªÃ§Â‚Â¹Ã¯Â¼ÂˆÃ¦Â¨Â¡Ã¦Â‹ÂŸÃ¥Â¤Â§Ã¦Â°Â”Ã¥Â™ÂªÃ¥Â£Â°Ã£Â€ÂÃ¥Â¤ÂšÃ¨Â·Â¯Ã¥Â¾Â„Ã¥ÂÂÃ¥Â°Â„Ã§Â­Â‰Ã¯Â¼Â‰
        
        Ã¥Â™ÂªÃ§Â‚Â¹Ã¦ÂÂ¥Ã¦ÂºÂÃ¦Â¨Â¡Ã¦Â‹ÂŸÃ¯Â¼Âš
        - Ã°ÂŸÂŒÂ©Ã¯Â¸Â Ã¥Â¤Â§Ã¦Â°Â”Ã¥Â™ÂªÃ¥Â£Â°Ã¯Â¼ÂšÃ©Â£ÂÃ©Â¸ÂŸÃ£Â€ÂÃ¤ÂºÂ‘Ã£Â€ÂÃ§ÂÂ°Ã¥Â°Â˜Ã¯Â¼ÂˆÃ©Â«Â˜Ã§Â©ÂºÃ¥Â™ÂªÃ§Â‚Â¹Ã¯Â¼Â‰
        - Ã°ÂŸÂ”Â» Ã¥ÂœÂ°Ã©ÂÂ¢Ã¥ÂÂÃ¥Â°Â„Ã¯Â¼ÂšÃ¦Â°Â´Ã©ÂÂ¢Ã£Â€ÂÃ§ÂÂ»Ã§Â’ÂƒÃ¥ÂÂÃ¥Â°Â„Ã¯Â¼ÂˆÃ¤Â½ÂÃ§Â©Âº/Ã¥ÂœÂ°Ã¤Â¸Â‹Ã¥Â™ÂªÃ§Â‚Â¹Ã¯Â¼Â‰
        - Ã°ÂŸÂ“Â¡ Ã¥Â¤ÂšÃ¨Â·Â¯Ã¥Â¾Â„Ã¥ÂÂÃ¥Â°Â„Ã¯Â¼ÂšÃ¥Â»ÂºÃ§Â­Â‘Ã§Â‰Â©Ã£Â€ÂÃ©Â‡Â‘Ã¥Â±ÂÃ¨Â¡Â¨Ã©ÂÂ¢Ã¥ÂÂÃ¥Â°Â„Ã¯Â¼ÂˆÃ©ÂšÂÃ¦ÂœÂºÃ©Â«Â˜Ã¥ÂºÂ¦Ã¯Â¼Â‰
        - Ã°ÂŸÂŒÂ³ Ã¦Â¤ÂÃ¨Â¢Â«Ã©ÂÂ®Ã¦ÂŒÂ¡Ã¯Â¼ÂšÃ¦Â Â‘Ã¥ÂÂ¶Ã©Â—Â´Ã©ÂšÂ™Ã§ÂšÂ„Ã¤Â¼ÂªÃ§Â‚Â¹Ã¯Â¼ÂˆÃ¤Â¸Â­Ã§Â­Â‰Ã©Â«Â˜Ã¥ÂºÂ¦Ã¯Â¼Â‰
        
        Ã¥Â™ÂªÃ§Â‚Â¹Ã¥Â±ÂÃ¦Â€Â§Ã¨Â®Â¾Ã§Â½Â®Ã§Â­Â–Ã§Â•Â¥Ã¯Â¼Âš
        - coord: Ã¥ÂœÂ¨Ã§ÂÂ°Ã¦ÂœÂ‰Ã§Â‚Â¹Ã¤ÂºÂ‘Ã§ÂšÂ„ XY Ã¨ÂŒÂƒÃ¥Â›Â´Ã¥Â†Â…Ã©ÂšÂÃ¦ÂœÂºÃ¥ÂˆÂ†Ã¥Â¸ÂƒÃ¯Â¼ÂŒZ Ã¤Â¸ÂºÃ¦ÂÂÃ§Â«Â¯Ã¥Â€Â¼
        - intensity: Ã©Â€ÂšÃ¥Â¸Â¸Ã¨Â¾ÂƒÃ¥Â¼Â±Ã¯Â¼ÂˆÃ¥Â¤Â§Ã¦Â°Â”Ã¥Â™ÂªÃ¥Â£Â°Ã¯Â¼Â‰Ã¦ÂˆÂ–Ã¥Â¾ÂˆÃ¥Â¼ÂºÃ¯Â¼ÂˆÃ¥ÂÂÃ¥Â°Â„Ã¯Â¼Â‰
        - color: Ã§ÂÂ°Ã¨Â‰Â²Ã¯Â¼ÂˆÃ¦ÂœÂªÃ§ÂŸÂ¥Ã¯Â¼Â‰Ã¦ÂˆÂ–Ã©ÂšÂÃ¦ÂœÂºÃ¨Â‰Â²
        - h_norm: Ã¦Â Â¹Ã¦ÂÂ® Z Ã¥Â’ÂŒÃ¥ÂœÂ°Ã©ÂÂ¢Ã©Â«Â˜Ã§Â¨Â‹Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼ÂˆÃ¦ÂˆÂ–Ã¨Â®Â¾Ã¤Â¸ÂºÃ¦ÂÂÃ§Â«Â¯Ã¥Â€Â¼Ã¯Â¼Â‰
        - class: Ã¥Â™ÂªÃ¥Â£Â°Ã§Â±Â»Ã¥ÂˆÂ«Ã¯Â¼ÂˆÃ¥ÂÂ¯Ã©Â…ÂÃ§Â½Â®Ã¯Â¼ÂŒÃ¥Â¦Â‚ 0=Ã¦ÂœÂªÃ¥ÂˆÂ†Ã§Â±Â»Ã¯Â¼Â‰
        
        Args:
            num_outliers: Ã¥Â›ÂºÃ¥Â®ÂšÃ¥Â™ÂªÃ§Â‚Â¹Ã¦Â•Â°Ã©Â‡ÂÃ¯Â¼ÂŒÃ¥Â¦Â‚Ã¦ÂÂœÃ¦ÂŒÂ‡Ã¥Â®ÂšÃ¥ÂˆÂ™Ã¥Â¿Â½Ã§Â•Â¥ ratio
            ratio: Ã¥Â™ÂªÃ§Â‚Â¹Ã¦Â•Â°Ã©Â‡ÂÃ¥ÂÂ Ã¦Â€Â»Ã§Â‚Â¹Ã¦Â•Â°Ã§ÂšÂ„Ã¦Â¯Â”Ã¤Â¾Â‹Ã¯Â¼ÂŒÃ©Â»Â˜Ã¨Â®Â¤ 0.01Ã¯Â¼Âˆ1%Ã¯Â¼Â‰
            height_range: Ã¥Â™ÂªÃ§Â‚Â¹Ã©Â«Â˜Ã¥ÂºÂ¦Ã¨ÂŒÂƒÃ¥Â›Â´ (z_min, z_max)Ã¯Â¼ÂŒÃ©Â»Â˜Ã¨Â®Â¤ (-10, 100) Ã§Â±Â³
                         Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ¥ÂÂŸÃ¥Â§Â‹ Z Ã¥ÂÂÃ¦Â Â‡Ã¯Â¼ÂŒÃ¤Â¸ÂÃ¦Â˜Â¯ h_norm
            height_mode: Ã©Â«Â˜Ã¥ÂºÂ¦Ã¥ÂˆÂ†Ã¥Â¸ÂƒÃ¦Â¨Â¡Ã¥Â¼Â
                - 'uniform': Ã¥ÂÂ‡Ã¥ÂŒÂ€Ã¥ÂˆÂ†Ã¥Â¸ÂƒÃ¥ÂœÂ¨ height_range
                - 'bimodal': Ã¥ÂÂŒÃ¥Â³Â°Ã¥ÂˆÂ†Ã¥Â¸ÂƒÃ¯Â¼ÂˆÃ©Â«Â˜Ã§Â©Âº+Ã¤Â½ÂÃ§Â©ÂºÃ¯Â¼Â‰
                - 'high': Ã¥ÂÂªÃ¥ÂœÂ¨Ã©Â«Â˜Ã§Â©ÂºÃ¯Â¼ÂˆÃ¦Â¨Â¡Ã¦Â‹ÂŸÃ©Â£ÂÃ©Â¸ÂŸÃ£Â€ÂÃ¤ÂºÂ‘Ã¯Â¼Â‰
                - 'low': Ã¥ÂÂªÃ¥ÂœÂ¨Ã¤Â½ÂÃ§Â©Âº/Ã¥ÂœÂ°Ã¤Â¸Â‹Ã¯Â¼ÂˆÃ¦Â¨Â¡Ã¦Â‹ÂŸÃ¥ÂÂÃ¥Â°Â„Ã¯Â¼Â‰
            intensity_range: Ã¥Â™ÂªÃ§Â‚Â¹Ã¥Â¼ÂºÃ¥ÂºÂ¦Ã¨ÂŒÂƒÃ¥Â›Â´ (min, max)Ã¯Â¼ÂŒÃ©Â»Â˜Ã¨Â®Â¤ (0, 1)
            color_value: Ã¥Â™ÂªÃ§Â‚Â¹Ã©Â¢ÂœÃ¨Â‰Â²
                - tuple (R, G, B): Ã¥Â›ÂºÃ¥Â®ÂšÃ©Â¢ÂœÃ¨Â‰Â²Ã¯Â¼ÂŒÃ¥Â¦Â‚ (128, 128, 128) Ã§ÂÂ°Ã¨Â‰Â²
                - 'random': Ã©ÂšÂÃ¦ÂœÂºÃ©Â¢ÂœÃ¨Â‰Â²
                - 'inherit': Ã¤Â»ÂÃ¦ÂœÂ€Ã¨Â¿Â‘Ã§ÂšÂ„Ã§ÂœÂŸÃ¥Â®ÂÃ§Â‚Â¹Ã§Â»Â§Ã¦Â‰Â¿Ã©Â¢ÂœÃ¨Â‰Â²
            class_label: Ã¥Â™ÂªÃ§Â‚Â¹Ã§ÂšÂ„Ã¥ÂˆÂ†Ã§Â±Â»Ã¦Â Â‡Ã§Â­Â¾
                - None: Ã¤Â»ÂÃ¦ÂœÂ€Ã¨Â¿Â‘Ã§ÂšÂ„Ã§ÂœÂŸÃ¥Â®ÂÃ§Â‚Â¹Ã§Â»Â§Ã¦Â‰Â¿
                - int: Ã¥Â›ÂºÃ¥Â®ÂšÃ¦Â Â‡Ã§Â­Â¾Ã¯Â¼ÂˆÃ¥Â¦Â‚ 0=Ã¦ÂœÂªÃ¥ÂˆÂ†Ã§Â±Â», -1=Ã¥Â™ÂªÃ¥Â£Â°Ã¯Â¼Â‰
                - 'ignore': Ã¤Â½Â¿Ã§Â”Â¨ ignore_labelÃ¯Â¼ÂˆÃ©Â€ÂšÃ¥Â¸Â¸Ã¦Â˜Â¯ -1Ã¯Â¼Â‰
            p: Ã¥ÂºÂ”Ã§Â”Â¨Ã¦Â¦Â‚Ã§ÂÂ‡
        """
        self.num_outliers = num_outliers
        self.ratio = ratio
        self.height_range = height_range
        self.height_mode = height_mode
        self.intensity_range = intensity_range
        self.color_value = color_value
        self.class_label = class_label
        self.p = p

    def __call__(self, data_dict):
        if np.random.rand() > self.p:
            return data_dict
        
        if "coord" not in data_dict:
            return data_dict
        
        coord = data_dict["coord"]
        n_points = len(coord)
        
        # Ã¨Â®Â¡Ã§Â®Â—Ã¥Â™ÂªÃ§Â‚Â¹Ã¦Â•Â°Ã©Â‡Â
        if self.num_outliers is not None:
            n_outliers = self.num_outliers
        else:
            n_outliers = max(1, int(n_points * self.ratio))
        
        # Ã¨ÂÂ·Ã¥ÂÂ–Ã¥ÂÂŸÃ¥Â§Â‹Ã§Â‚Â¹Ã¤ÂºÂ‘Ã§ÂšÂ„ XY Ã¨ÂŒÂƒÃ¥Â›Â´
        x_min, y_min, z_min = coord.min(axis=0)
        x_max, y_max, z_max = coord.max(axis=0)
        
        # Ã§Â”ÂŸÃ¦ÂˆÂÃ¥Â™ÂªÃ§Â‚Â¹Ã¥ÂÂÃ¦Â Â‡
        outlier_xy = np.random.rand(n_outliers, 2)
        outlier_xy[:, 0] = outlier_xy[:, 0] * (x_max - x_min) + x_min
        outlier_xy[:, 1] = outlier_xy[:, 1] * (y_max - y_min) + y_min
        
        # Ã¦Â Â¹Ã¦ÂÂ®Ã¦Â¨Â¡Ã¥Â¼ÂÃ§Â”ÂŸÃ¦ÂˆÂÃ©Â«Â˜Ã¥ÂºÂ¦
        if self.height_mode == 'uniform':
            # Ã¥ÂÂ‡Ã¥ÂŒÂ€Ã¥ÂˆÂ†Ã¥Â¸Âƒ
            outlier_z = np.random.uniform(
                self.height_range[0], self.height_range[1], n_outliers
            )
        elif self.height_mode == 'bimodal':
            # Ã¥ÂÂŒÃ¥Â³Â°Ã¥ÂˆÂ†Ã¥Â¸ÂƒÃ¯Â¼Âš50% Ã©Â«Â˜Ã§Â©ÂºÃ¯Â¼ÂŒ50% Ã¤Â½ÂÃ§Â©Âº
            n_high = n_outliers // 2
            n_low = n_outliers - n_high
            z_high = np.random.uniform(
                max(self.height_range[0], z_max), self.height_range[1], n_high
            )
            z_low = np.random.uniform(
                self.height_range[0], min(self.height_range[1], z_min), n_low
            )
            outlier_z = np.concatenate([z_high, z_low])
            np.random.shuffle(outlier_z)
        elif self.height_mode == 'high':
            # Ã¥ÂÂªÃ¥ÂœÂ¨Ã©Â«Â˜Ã§Â©Âº
            outlier_z = np.random.uniform(
                max(self.height_range[0], z_max), self.height_range[1], n_outliers
            )
        elif self.height_mode == 'low':
            # Ã¥ÂÂªÃ¥ÂœÂ¨Ã¤Â½ÂÃ§Â©Âº/Ã¥ÂœÂ°Ã¤Â¸Â‹
            outlier_z = np.random.uniform(
                self.height_range[0], min(self.height_range[1], z_min), n_outliers
            )
        else:
            raise ValueError(f"Unknown height_mode: {self.height_mode}")
        
        # Ã§Â»Â„Ã¥ÂÂˆÃ¥Â™ÂªÃ§Â‚Â¹Ã¥ÂÂÃ¦Â Â‡
        outlier_coord = np.column_stack([outlier_xy, outlier_z]).astype(coord.dtype)
        
        # Ã¦Â·Â»Ã¥ÂŠÂ Ã¥Â™ÂªÃ§Â‚Â¹Ã¥ÂˆÂ°Ã¥ÂÂÃ¦Â Â‡
        data_dict["coord"] = np.vstack([coord, outlier_coord])
        
        # Ã¥Â¤Â„Ã§ÂÂ†Ã¥Â…Â¶Ã¤Â»Â–Ã¥Â±ÂÃ¦Â€Â§
        # 1. Intensity
        if "intensity" in data_dict:
            outlier_intensity = np.random.uniform(
                self.intensity_range[0], self.intensity_range[1], n_outliers
            ).astype(data_dict["intensity"].dtype)
            data_dict["intensity"] = np.concatenate([
                data_dict["intensity"], outlier_intensity
            ])
        
        # 2. Color
        if "color" in data_dict:
            if self.color_value == 'random':
                # Ã©ÂšÂÃ¦ÂœÂºÃ©Â¢ÂœÃ¨Â‰Â²
                outlier_color = np.random.uniform(
                    0, 255, (n_outliers, 3)
                ).astype(data_dict["color"].dtype)
            elif self.color_value == 'inherit':
                # Ã¤Â»ÂÃ¦ÂœÂ€Ã¨Â¿Â‘Ã§ÂšÂ„Ã§ÂœÂŸÃ¥Â®ÂÃ§Â‚Â¹Ã§Â»Â§Ã¦Â‰Â¿Ã¯Â¼ÂˆÃ¤Â½Â¿Ã§Â”Â¨Ã§Â®Â€Ã¥ÂÂ•Ã§ÂšÂ„Ã©ÂšÂÃ¦ÂœÂºÃ©Â‡Â‡Ã¦Â Â·Ã¯Â¼Â‰
                random_indices = np.random.choice(n_points, n_outliers)
                outlier_color = data_dict["color"][random_indices].copy()
            else:
                # Ã¥Â›ÂºÃ¥Â®ÂšÃ©Â¢ÂœÃ¨Â‰Â²
                outlier_color = np.tile(
                    np.array(self.color_value, dtype=data_dict["color"].dtype),
                    (n_outliers, 1)
                )
            data_dict["color"] = np.vstack([data_dict["color"], outlier_color])
        
        # 3. h_norm
        if "h_norm" in data_dict:
            # Ã¨Â®Â¡Ã§Â®Â—Ã¥Â™ÂªÃ§Â‚Â¹Ã§ÂšÂ„ h_norm
            # Ã§Â®Â€Ã¥ÂŒÂ–Ã¯Â¼ÂšÃ¥ÂÂ‡Ã¨Â®Â¾Ã¥ÂœÂ°Ã©ÂÂ¢Ã©Â«Â˜Ã§Â¨Â‹Ã¤Â¸ÂºÃ¥ÂÂŸÃ¥Â§Â‹Ã§Â‚Â¹Ã¤ÂºÂ‘Ã§ÂšÂ„Ã¦ÂœÂ€Ã¥Â°Â Z
            ground_z = z_min
            outlier_h_norm = (outlier_z - ground_z).astype(data_dict["h_norm"].dtype)
            data_dict["h_norm"] = np.concatenate([
                data_dict["h_norm"], outlier_h_norm
            ])
        
        # 4. Normal
        if "normal" in data_dict:
            # Ã¥Â™ÂªÃ§Â‚Â¹Ã§ÂšÂ„Ã¦Â³Â•Ã¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂšÃ©ÂšÂÃ¦ÂœÂºÃ¦Â–Â¹Ã¥ÂÂ‘Ã¯Â¼ÂˆÃ¦Â¨Â¡Ã¦Â‹ÂŸÃ¥Â™ÂªÃ¥Â£Â°Ã¯Â¼Â‰
            outlier_normal = np.random.randn(n_outliers, 3).astype(
                data_dict["normal"].dtype
            )
            # Ã¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
            norms = np.linalg.norm(outlier_normal, axis=1, keepdims=True)
            outlier_normal = outlier_normal / (norms + 1e-8)
            data_dict["normal"] = np.vstack([data_dict["normal"], outlier_normal])
        
        # 5. Echo
        if "echo" in data_dict:
            # Ã¥Â™ÂªÃ§Â‚Â¹Ã©Â€ÂšÃ¥Â¸Â¸Ã¦Â˜Â¯Ã¥ÂÂ•Ã¦Â¬Â¡Ã¥Â›ÂÃ¦Â³Â¢
            outlier_echo = np.ones((n_outliers, 2), dtype=data_dict["echo"].dtype)
            # Ã¨Â®Â¾Ã¤Â¸ÂºÃ©Â¦Â–Ã¦Â¬Â¡Ã¤Â¸Â”Ã¦ÂœÂ«Ã¦Â¬Â¡Ã¥Â›ÂÃ¦Â³Â¢Ã¯Â¼ÂˆÃ¥ÂÂ•Ã¦Â¬Â¡Ã¥Â›ÂÃ¦Â³Â¢Ã§ÂšÂ„Ã§Â‰Â¹Ã¥Â¾ÂÃ¯Â¼Â‰
            data_dict["echo"] = np.vstack([data_dict["echo"], outlier_echo])
        
        # 6. Classification
        if "class" in data_dict:
            if self.class_label is None:
                # Ã¤Â»ÂÃ¦ÂœÂ€Ã¨Â¿Â‘Ã§ÂšÂ„Ã§ÂœÂŸÃ¥Â®ÂÃ§Â‚Â¹Ã§Â»Â§Ã¦Â‰Â¿Ã¯Â¼ÂˆÃ©ÂšÂÃ¦ÂœÂºÃ©Â‡Â‡Ã¦Â Â·Ã¯Â¼Â‰
                random_indices = np.random.choice(n_points, n_outliers)
                outlier_class = data_dict["class"][random_indices].copy()
            elif self.class_label == 'ignore':
                # Ã¤Â½Â¿Ã§Â”Â¨ ignore_labelÃ¯Â¼ÂˆÃ©Â€ÂšÃ¥Â¸Â¸Ã¥ÂœÂ¨ dataset Ã¤Â¸Â­Ã¥Â®ÂšÃ¤Â¹Â‰Ã¯Â¼Â‰
                outlier_class = np.full(n_outliers, -1, dtype=data_dict["class"].dtype)
            else:
                # Ã¥Â›ÂºÃ¥Â®ÂšÃ¦Â Â‡Ã§Â­Â¾
                outlier_class = np.full(
                    n_outliers, self.class_label, dtype=data_dict["class"].dtype
                )
            data_dict["class"] = np.concatenate([data_dict["class"], outlier_class])
        
        return data_dict


# Ã¦Â·Â»Ã¥ÂŠÂ Ã¥Â±Â€Ã©ÂƒÂ¨Ã¥Â™ÂªÃ§Â‚Â¹Ã§Â°Â‡
class AddLocalNoiseClusters(object):
    def __init__(self,
                 num_clusters=3,
                 points_per_cluster=(5, 20),
                 cluster_radius=2.0,
                 height_offset=(-5, 5),
                 intensity_range=(0, 1),
                 color_value='random',
                 class_label='ignore',
                 p=0.3):
        """
        Ã¦Â·Â»Ã¥ÂŠÂ Ã¥Â±Â€Ã©ÂƒÂ¨Ã¥Â™ÂªÃ§Â‚Â¹Ã§Â°Â‡Ã¯Â¼ÂˆÃ¦Â¨Â¡Ã¦Â‹ÂŸÃ¥Â±Â€Ã©ÂƒÂ¨Ã¦ÂµÂ‹Ã©Â‡ÂÃ¨Â¯Â¯Ã¥Â·Â®Ã£Â€ÂÃ¥Â¤ÂšÃ¨Â·Â¯Ã¥Â¾Â„Ã¥ÂÂÃ¥Â°Â„Ã§Â­Â‰Ã¯Â¼Â‰
        
        Ã¤Â¸Â AddExtremeOutliers Ã§ÂšÂ„Ã¥ÂŒÂºÃ¥ÂˆÂ«Ã¯Â¼Âš
        - AddExtremeOutliers: Ã¥Â…Â¨Ã¥Â±Â€Ã©ÂšÂÃ¦ÂœÂºÃ¥ÂˆÂ†Ã¥Â¸ÂƒÃ§ÂšÂ„Ã¦ÂÂÃ§Â«Â¯Ã¥Â™ÂªÃ§Â‚Â¹
        - AddLocalNoiseClusters: Ã¥Â±Â€Ã©ÂƒÂ¨Ã¨ÂÂšÃ©Â›Â†Ã§ÂšÂ„Ã¥Â™ÂªÃ§Â‚Â¹Ã§Â°Â‡Ã¯Â¼ÂˆÃ¦Â›Â´Ã§ÂœÂŸÃ¥Â®ÂÃ¯Â¼Â‰
        
        Ã¥ÂºÂ”Ã§Â”Â¨Ã¥ÂœÂºÃ¦Â™Â¯Ã¯Â¼Âš
        - Ã°ÂŸÂÂ¢ Ã¥Â»ÂºÃ§Â­Â‘Ã§Â‰Â©Ã§ÂÂ»Ã§Â’ÂƒÃ¥ÂÂÃ¥Â°Â„Ã¯Â¼ÂšÃ¤ÂºÂ§Ã§Â”ÂŸÃ¥Â±Â€Ã©ÂƒÂ¨Ã¨ÂÂšÃ©Â›Â†Ã§ÂšÂ„Ã¥ÂÂ‡Ã§Â‚Â¹
        - Ã°ÂŸÂŒÂ² Ã¦Â¤ÂÃ¨Â¢Â«Ã©ÂÂ®Ã¦ÂŒÂ¡Ã¯Â¼ÂšÃ¦Â Â‘Ã¥ÂÂ¶Ã©Â—Â´Ã©ÂšÂ™Ã¤ÂºÂ§Ã§Â”ÂŸÃ§ÂšÂ„Ã¥Â™ÂªÃ§Â‚Â¹Ã§Â°Â‡
        - Ã°ÂŸÂ“Â¡ Ã¥Â¤ÂšÃ¨Â·Â¯Ã¥Â¾Â„Ã¥Â¹Â²Ã¦Â‰Â°Ã¯Â¼ÂšÃ§Â‰Â¹Ã¥Â®ÂšÃ¤Â½ÂÃ§Â½Â®Ã§ÂšÂ„Ã§Â³Â»Ã§Â»ÂŸÃ¨Â¯Â¯Ã¥Â·Â®
        - Ã°ÂŸÂ’Â§ Ã¦Â°Â´Ã©ÂÂ¢Ã¥ÂÂÃ¥Â°Â„Ã¯Â¼ÂšÃ¦Â°Â´Ã¤Â½Â“Ã©Â™Â„Ã¨Â¿Â‘Ã§ÂšÂ„Ã¥Â™ÂªÃ§Â‚Â¹
        
        Args:
            num_clusters: Ã¥Â™ÂªÃ§Â‚Â¹Ã§Â°Â‡Ã§ÂšÂ„Ã¦Â•Â°Ã©Â‡Â
            points_per_cluster: Ã¦Â¯ÂÃ¤Â¸ÂªÃ§Â°Â‡Ã§ÂšÂ„Ã§Â‚Â¹Ã¦Â•Â°Ã¨ÂŒÂƒÃ¥Â›Â´ (min, max)
            cluster_radius: Ã§Â°Â‡Ã§ÂšÂ„Ã¥ÂÂŠÃ¥Â¾Â„Ã¯Â¼ÂˆÃ§Â±Â³Ã¯Â¼Â‰
            height_offset: Ã¥Â™ÂªÃ§Â‚Â¹Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ§Â°Â‡Ã¤Â¸Â­Ã¥Â¿ÂƒÃ§ÂšÂ„Ã©Â«Â˜Ã¥ÂºÂ¦Ã¥ÂÂÃ§Â§Â»Ã¨ÂŒÂƒÃ¥Â›Â´ (min, max)
            intensity_range: Ã¥Â™ÂªÃ§Â‚Â¹Ã¥Â¼ÂºÃ¥ÂºÂ¦Ã¨ÂŒÂƒÃ¥Â›Â´
            color_value: Ã¥Â™ÂªÃ§Â‚Â¹Ã©Â¢ÂœÃ¨Â‰Â²Ã¯Â¼Âˆ'random', 'inherit', Ã¦ÂˆÂ– RGB tupleÃ¯Â¼Â‰
            class_label: Ã¥Â™ÂªÃ§Â‚Â¹Ã¥ÂˆÂ†Ã§Â±Â»Ã¦Â Â‡Ã§Â­Â¾Ã¯Â¼ÂˆNone, int, 'ignore'Ã¯Â¼Â‰
            p: Ã¥ÂºÂ”Ã§Â”Â¨Ã¦Â¦Â‚Ã§ÂÂ‡
        """
        self.num_clusters = num_clusters
        self.points_per_cluster = points_per_cluster
        self.cluster_radius = cluster_radius
        self.height_offset = height_offset
        self.intensity_range = intensity_range
        self.color_value = color_value
        self.class_label = class_label
        self.p = p

    def __call__(self, data_dict):
        if np.random.rand() > self.p:
            return data_dict
        
        if "coord" not in data_dict:
            return data_dict
        
        coord = data_dict["coord"]
        n_points = len(coord)
        
        if n_points < 10:
            return data_dict
        
        # Ã©ÂšÂÃ¦ÂœÂºÃ©Â€Â‰Ã¦Â‹Â©Ã§Â°Â‡Ã¤Â¸Â­Ã¥Â¿ÂƒÃ¯Â¼ÂˆÃ¤Â»ÂÃ§ÂÂ°Ã¦ÂœÂ‰Ã§Â‚Â¹Ã¤Â¸Â­Ã©Â€Â‰Ã¦Â‹Â©Ã¯Â¼Â‰
        cluster_centers = coord[
            np.random.choice(n_points, min(self.num_clusters, n_points), replace=False)
        ]
        
        all_outlier_coords = []
        
        for center in cluster_centers:
            # Ã¦Â¯ÂÃ¤Â¸ÂªÃ§Â°Â‡Ã§ÂšÂ„Ã§Â‚Â¹Ã¦Â•Â°
            n_cluster = np.random.randint(
                self.points_per_cluster[0], self.points_per_cluster[1] + 1
            )
            
            # Ã¥ÂœÂ¨Ã§ÂÂƒÃ¥Â½Â¢Ã¥ÂŒÂºÃ¥ÂŸÂŸÃ¥Â†Â…Ã§Â”ÂŸÃ¦ÂˆÂÃ§Â‚Â¹
            # Ã¤Â½Â¿Ã§Â”Â¨Ã§ÂÂƒÃ¥ÂÂÃ¦Â Â‡Ã§Â³Â»Ã¯Â¼ÂšÃ¥ÂÂ‡Ã¥ÂŒÂ€Ã¥ÂˆÂ†Ã¥Â¸Âƒ
            theta = np.random.uniform(0, 2 * np.pi, n_cluster)
            phi = np.random.uniform(0, np.pi, n_cluster)
            r = np.random.uniform(0, self.cluster_radius, n_cluster)
            
            # Ã¨Â½Â¬Ã¦ÂÂ¢Ã¤Â¸ÂºÃ§Â¬Â›Ã¥ÂÂ¡Ã¥Â°Â”Ã¥ÂÂÃ¦Â Â‡
            x = center[0] + r * np.sin(phi) * np.cos(theta)
            y = center[1] + r * np.sin(phi) * np.sin(theta)
            z_base = center[2] + r * np.cos(phi)
            
            # Ã¦Â·Â»Ã¥ÂŠÂ Ã©Â«Â˜Ã¥ÂºÂ¦Ã¥ÂÂÃ§Â§Â»
            z_offset = np.random.uniform(
                self.height_offset[0], self.height_offset[1], n_cluster
            )
            z = z_base + z_offset
            
            cluster_coords = np.column_stack([x, y, z])
            all_outlier_coords.append(cluster_coords)
        
        if len(all_outlier_coords) == 0:
            return data_dict
        
        outlier_coord = np.vstack(all_outlier_coords).astype(coord.dtype)
        n_outliers = len(outlier_coord)
        
        # Ã¦Â·Â»Ã¥ÂŠÂ Ã¥Â™ÂªÃ§Â‚Â¹Ã¥ÂˆÂ°Ã¥ÂÂÃ¦Â Â‡
        data_dict["coord"] = np.vstack([coord, outlier_coord])
        
        # Ã¥Â¤Â„Ã§ÂÂ†Ã¥Â…Â¶Ã¤Â»Â–Ã¥Â±ÂÃ¦Â€Â§Ã¯Â¼ÂˆÃ¤Â¸Â AddExtremeOutliers Ã§Â±Â»Ã¤Â¼Â¼Ã¯Â¼Â‰
        if "intensity" in data_dict:
            outlier_intensity = np.random.uniform(
                self.intensity_range[0], self.intensity_range[1], n_outliers
            ).astype(data_dict["intensity"].dtype)
            data_dict["intensity"] = np.concatenate([
                data_dict["intensity"], outlier_intensity
            ])
        
        if "color" in data_dict:
            if self.color_value == 'random':
                outlier_color = np.random.uniform(
                    0, 255, (n_outliers, 3)
                ).astype(data_dict["color"].dtype)
            elif self.color_value == 'inherit':
                random_indices = np.random.choice(n_points, n_outliers)
                outlier_color = data_dict["color"][random_indices].copy()
            else:
                outlier_color = np.tile(
                    np.array(self.color_value, dtype=data_dict["color"].dtype),
                    (n_outliers, 1)
                )
            data_dict["color"] = np.vstack([data_dict["color"], outlier_color])
        
        if "h_norm" in data_dict:
            # Ã§Â®Â€Ã¥ÂŒÂ–Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼ÂšÃ¤Â½Â¿Ã§Â”Â¨Ã¥ÂÂŸÃ¥Â§Â‹Ã§Â‚Â¹Ã¤ÂºÂ‘Ã¦ÂœÂ€Ã¥Â°Â Z Ã¤Â½ÂœÃ¤Â¸ÂºÃ¥ÂœÂ°Ã©ÂÂ¢
            ground_z = coord[:, 2].min()
            outlier_h_norm = (outlier_coord[:, 2] - ground_z).astype(
                data_dict["h_norm"].dtype
            )
            data_dict["h_norm"] = np.concatenate([
                data_dict["h_norm"], outlier_h_norm
            ])
        
        if "normal" in data_dict:
            outlier_normal = np.random.randn(n_outliers, 3).astype(
                data_dict["normal"].dtype
            )
            norms = np.linalg.norm(outlier_normal, axis=1, keepdims=True)
            outlier_normal = outlier_normal / (norms + 1e-8)
            data_dict["normal"] = np.vstack([data_dict["normal"], outlier_normal])
        
        if "echo" in data_dict:
            outlier_echo = np.ones((n_outliers, 2), dtype=data_dict["echo"].dtype)
            data_dict["echo"] = np.vstack([data_dict["echo"], outlier_echo])
        
        if "class" in data_dict:
            if self.class_label is None:
                random_indices = np.random.choice(n_points, n_outliers)
                outlier_class = data_dict["class"][random_indices].copy()
            elif self.class_label == 'ignore':
                outlier_class = np.full(n_outliers, -1, dtype=data_dict["class"].dtype)
            else:
                outlier_class = np.full(
                    n_outliers, self.class_label, dtype=data_dict["class"].dtype
                )
            data_dict["class"] = np.concatenate([data_dict["class"], outlier_class])
        
        return data_dict
    

# Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â” Grid Sample Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”Ã¢Â€Â”
class GridSample(object):
    def __init__(
        self,
        grid_size=0.05,
        hash_type="fnv",
        mode="train",
        return_inverse=False,
        return_grid_coord=False,
        return_min_coord=False,
        return_displacement=False,
        project_displacement=False,
        max_test_loops=30,
    ):
        self.grid_size = grid_size
        self.hash = self.fnv_hash_vec if hash_type == "fnv" else self.ravel_hash_vec
        assert mode in ["train", "test"]
        self.mode = mode
        self.return_inverse = return_inverse
        self.return_grid_coord = return_grid_coord
        self.return_min_coord = return_min_coord
        self.return_displacement = return_displacement
        self.project_displacement = project_displacement

    def __call__(self, data_dict):
        assert "coord" in data_dict.keys()
        # Ã¨Â®Â¡Ã§Â®Â—Ã¨Â§Â„Ã¥ÂˆÂ™Ã¥ÂŒÂ–Ã¥ÂÂÃ¦Â Â‡
        self.grid_size=self.grid_size
        scaled_coord = data_dict["coord"] / np.array(self.grid_size)
        grid_coord = np.floor(scaled_coord).astype(int)
        # Ã¨Â®Â¡Ã§Â®Â—Ã¦ÂœÂ€Ã¥Â°ÂÃ§Â½Â‘Ã¦Â Â¼Ã¥ÂÂÃ¦Â Â‡Ã¯Â¼ÂŒÃ¥Â½Â’Ã¤Â¸Â€Ã¥ÂŒÂ–
        min_coord = grid_coord.min(0)
        grid_coord -= min_coord
        scaled_coord -= min_coord
        min_coord = min_coord * np.array(self.grid_size)
        # Ã¨ÂÂ·Ã¥ÂÂ–Ã¨Â§Â„Ã¥ÂˆÂ™Ã¥ÂÂÃ¦Â Â‡Ã¥Â“ÂˆÃ¥Â¸ÂŒÃ¥Â€Â¼Ã¥Â¹Â¶Ã¦ÂÂ’Ã¥ÂºÂ
        key = self.hash(grid_coord)
        idx_sort = np.argsort(key)
        key_sort = key[idx_sort]
        # Ã¨Â®Â¡Ã§Â®Â—Ã§Â½Â‘Ã¦Â Â¼Ã§Â´Â¢Ã¥Â¼Â•Ã¥Â’ÂŒÃ§Â‚Â¹Ã¦Â•Â°Ã§Â»ÂŸÃ¨Â®Â¡
        _, inverse, count = np.unique(key_sort, return_inverse=True, return_counts=True)
        if self.mode == "train":  # train mode
            # Ã¦Â Â¼Ã§Â½Â‘Ã¤Â¸Â­Ã©ÂšÂÃ¦ÂœÂºÃ©Â‡Â‡Ã¦Â Â·
            idx_select = (
                np.cumsum(np.insert(count, 0, 0)[0:-1])
                + np.random.randint(0, count.max(), count.size) % count
            )
            idx_unique = idx_sort[idx_select]
            if "sampled_index" in data_dict:
                # for ScanNet data efficient, we need to make sure labeled point is sampled.
                idx_unique = np.unique(
                    np.append(idx_unique, data_dict["sampled_index"])
                )
                mask = np.zeros_like(data_dict["segment"]).astype(bool)
                mask[data_dict["sampled_index"]] = True
                data_dict["sampled_index"] = np.where(mask[idx_unique])[0]
            data_dict = index_operator(data_dict, idx_unique)
            # Ã¨Â‹Â¥Ã©ÂœÂ€Ã¨Â¿Â”Ã¥Â›ÂÃ©Â€Â†Ã§Â´Â¢Ã¥Â¼Â• return_inverseÃ¯Â¼ÂŒÃ¨Â®Â°Ã¥Â½Â•Ã¦Â¯ÂÃ¤Â¸ÂªÃ§Â‚Â¹Ã¥ÂœÂ¨Ã¥ÂÂŸÃ¥Â§Â‹Ã¦Â•Â°Ã¦ÂÂ®Ã¤Â¸Â­Ã§ÂšÂ„Ã¥Â½Â’Ã¥Â±Â
            if self.return_inverse:
                data_dict["inverse"] = np.zeros_like(inverse)
                data_dict["inverse"][idx_sort] = inverse
            # Ã¨Â®Â°Ã¥Â½Â•Ã§Â½Â‘Ã¦Â Â¼Ã¥ÂÂÃ¦Â Â‡Ã¥Â’ÂŒÃ¦ÂœÂ€Ã¥Â°ÂÃ¥ÂÂÃ¦Â Â‡
            if self.return_grid_coord:
                data_dict["grid_coord"] = grid_coord[idx_unique]
                data_dict["index_valid_keys"].append("grid_coord")
            if self.return_min_coord:
                data_dict["min_coord"] = min_coord.reshape([1, 3])
            # Ã§Â‚Â¹Ã¥ÂœÂ¨Ã§Â½Â‘Ã¦Â Â¼Ã¥Â†Â…Ã§ÂšÂ„Ã¤Â½ÂÃ§Â½Â®Ã¥Â’ÂŒÃ¦Â³Â•Ã§ÂºÂ¿Ã¤Â¸ÂŠÃ§ÂšÂ„Ã¨Â·ÂÃ§Â¦Â»
            if self.return_displacement:
                displacement = (
                    scaled_coord - grid_coord - 0.5
                )  # [0, 1] -> [-0.5, 0.5] displacement to center
                if self.project_displacement:
                    displacement = np.sum(
                        displacement * data_dict["normal"], axis=-1, keepdims=True
                    )
                data_dict["displacement"] = displacement[idx_unique]
                data_dict["index_valid_keys"].append("displacement")
            return data_dict

        elif self.mode == "test":  # test mode
            data_part_list = []
            # Ã¥Â¾ÂªÃ§ÂÂ¯Ã©Â‡Â‡Ã¦Â Â·Ã¯Â¼ÂŒÃ©ÂÂ¿Ã¥Â…ÂÃ©ÂÂ—Ã¦Â¼Â
            for i in range(count.max()):
                idx_select = np.cumsum(np.insert(count, 0, 0)[0:-1]) + i % count
                idx_part = idx_sort[idx_select]
                data_part = index_operator(data_dict, idx_part, duplicate=True)
                data_part["index"] = idx_part
                if self.return_inverse:
                    data_part["inverse"] = np.zeros_like(inverse)
                    data_part["inverse"][idx_sort] = inverse
                if self.return_grid_coord:
                    data_part["grid_coord"] = grid_coord[idx_part]
                    data_dict["index_valid_keys"].append("grid_coord")
                if self.return_min_coord:
                    data_part["min_coord"] = min_coord.reshape([1, 3])
                if self.return_displacement:
                    displacement = (
                        scaled_coord - grid_coord - 0.5
                    )  # [0, 1] -> [-0.5, 0.5] displacement to center
                    if self.project_displacement:
                        displacement = np.sum(
                            displacement * data_dict["normal"], axis=-1, keepdims=True
                        )
                    data_dict["displacement"] = displacement[idx_part]
                    data_dict["index_valid_keys"].append("displacement")
                data_part_list.append(data_part)
            return data_part_list
        
        else:
            raise NotImplementedError

    @staticmethod
    # Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ¨ÂŒÂƒÃ¥Â›Â´Ã¥Â·Â²Ã§ÂŸÂ¥Ã¯Â¼ÂŒÃ¥Â¯Â†Ã©Â›Â†Ã¦Â Â¼Ã§Â½Â‘
    def ravel_hash_vec(arr):
        """
        Ravel the coordinates after subtracting the min coordinates.
        """
        assert arr.ndim == 2
        arr = arr.copy()
        arr -= arr.min(0)
        arr = arr.astype(np.uint64, copy=False)
        arr_max = arr.max(0).astype(np.uint64) + 1

        keys = np.zeros(arr.shape[0], dtype=np.uint64)
        # Fortran style indexing
        for j in range(arr.shape[1] - 1):
            keys += arr[:, j]
            keys *= arr_max[j + 1]
        keys += arr[:, -1]
        return keys

    @staticmethod
    # Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ¨ÂŒÂƒÃ¥Â›Â´Ã¦ÂœÂªÃ§ÂŸÂ¥Ã¦ÂˆÂ–Ã¨Â¾ÂƒÃ¥Â¤Â§Ã¯Â¼ÂŒÃ§Â¨Â€Ã§Â–ÂÃ¦Â Â¼Ã§Â½Â‘
    def fnv_hash_vec(arr):
        """
        FNV64-1A
        """
        assert arr.ndim == 2
        # Floor first for negative coordinates
        arr = arr.copy()
        arr = arr.astype(np.uint64, copy=False)
        hashed_arr = np.uint64(14695981039346656037) * np.ones(
            arr.shape[0], dtype=np.uint64
        )
        for j in range(arr.shape[1]):
            hashed_arr *= np.uint64(1099511628211)
            hashed_arr = np.bitwise_xor(hashed_arr, arr[:, j])
        return hashed_arr
