"""
ç”¨äºåŠ è½½ bin+pkl é€»è¾‘ç´¢å¼•æ ¼å¼ç‚¹äº‘æ•°æ®çš„æ•°æ®é›† (å¯¹åº” tile_las1.py)

æœ¬æ¨¡å—å®ç°äº†æ–°çš„ bin+pkl æ•°æ®æ ¼å¼çš„æ•°æ®é›†ç±»ï¼Œæ”¯æŒï¼š
1. å…¨é‡æ¨¡å¼ (full): åŠ è½½æ‰€æœ‰åŸå§‹ç‚¹
2. ä½“ç´ æ¨¡å¼ (voxel): ä½¿ç”¨ä½“ç´ åŒ–ç´¢å¼•è¿›è¡Œé‡‡æ ·
   - train/val: ä»æ¯ä¸ªä½“ç´ éšæœºå– 1 ä¸ªç‚¹
   - test/predict: ä½¿ç”¨æ¨¡è¿ç®—é‡‡æ ·ç¡®ä¿å…¨è¦†ç›–

æ•°æ®ç»“æ„ (tile_las1.py ç”Ÿæˆ):
- .bin æ–‡ä»¶ï¼šä»¥ç»“æ„åŒ– numpy æ•°ç»„æ ¼å¼åŒ…å«æ‰€æœ‰ç‚¹æ•°æ®
- .pkl æ–‡ä»¶ï¼šåŒ…å«å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
    - segments: åˆ†å—ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«ï¼š
        - indices: ç‚¹ç´¢å¼•
        - sort_idx: ä½“ç´ åŒ–æ’åºç´¢å¼•
        - voxel_counts: æ¯ä¸ªä½“ç´ çš„ç‚¹æ•°
        - num_voxels: ä½“ç´ æ•°é‡
        - max_voxel_density: æœ€å¤§ä½“ç´ ç‚¹æ•°
    - header_info: åŸå§‹ LAS æ–‡ä»¶å¤´
    - grid_size: ä½“ç´ åŒ–ç½‘æ ¼å¤§å°
"""
import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from numba import jit, prange

from .dataset_base import DatasetBase


# ============================================================================
# Numba åŠ é€Ÿé‡‡æ ·å‡½æ•°
# ============================================================================

@jit(nopython=True, cache=True)
def _voxel_random_sample_numba(sort_idx: np.ndarray, 
                                voxel_counts: np.ndarray,
                                cumsum: np.ndarray) -> np.ndarray:
    """
    Numba åŠ é€Ÿçš„éšæœºä½“ç´ é‡‡æ ·
    ä»æ¯ä¸ªä½“ç´ ä¸­éšæœºé‡‡æ · 1 ä¸ªç‚¹
    """
    n_voxels = len(voxel_counts)
    sampled = np.empty(n_voxels, dtype=np.int32)
    
    for i in range(n_voxels):
        voxel_count = voxel_counts[i]
        start_pos = cumsum[i]
        # ä½¿ç”¨ numpy éšæœºæ•°
        random_offset = np.random.randint(0, voxel_count)
        sampled[i] = sort_idx[start_pos + random_offset]
    
    return sampled


@jit(nopython=True, cache=True)
def _voxel_modulo_sample_numba(sort_idx: np.ndarray,
                                voxel_counts: np.ndarray,
                                cumsum: np.ndarray,
                                loop_idx: int,
                                points_per_loop: int) -> np.ndarray:
    """
    Numba åŠ é€Ÿçš„æ¨¡è¿ç®—ä½“ç´ é‡‡æ ·
    """
    n_voxels = len(voxel_counts)
    total_points = n_voxels * points_per_loop
    sampled = np.empty(total_points, dtype=np.int32)
    
    idx = 0
    for i in range(n_voxels):
        voxel_count = voxel_counts[i]
        start_pos = cumsum[i]
        
        for p in range(points_per_loop):
            logical_idx = loop_idx * points_per_loop + p
            local_idx = logical_idx % voxel_count
            sampled[idx] = sort_idx[start_pos + local_idx]
            idx += 1
    
    return sampled


@jit(nopython=True, parallel=True, cache=True)
def _voxel_random_sample_parallel(sort_idx: np.ndarray, 
                                   voxel_counts: np.ndarray,
                                   cumsum: np.ndarray,
                                   random_offsets: np.ndarray) -> np.ndarray:
    """
    Numba å¹¶è¡ŒåŠ é€Ÿçš„éšæœºä½“ç´ é‡‡æ ·
    æ³¨æ„ï¼šéœ€è¦é¢„ç”Ÿæˆéšæœºæ•°ä»¥é¿å…å¹¶è¡Œéšæœºæ•°é—®é¢˜
    """
    n_voxels = len(voxel_counts)
    sampled = np.empty(n_voxels, dtype=np.int32)
    
    for i in prange(n_voxels):
        voxel_count = voxel_counts[i]
        start_pos = cumsum[i]
        random_offset = random_offsets[i] % voxel_count
        sampled[i] = sort_idx[start_pos + random_offset]
    
    return sampled


@jit(nopython=True, parallel=True, cache=True)
def _voxel_modulo_sample_parallel(sort_idx: np.ndarray,
                                   voxel_counts: np.ndarray,
                                   cumsum: np.ndarray,
                                   loop_idx: int,
                                   points_per_loop: int) -> np.ndarray:
    """
    Numba å¹¶è¡ŒåŠ é€Ÿçš„æ¨¡è¿ç®—ä½“ç´ é‡‡æ ·
    """
    n_voxels = len(voxel_counts)
    total_points = n_voxels * points_per_loop
    sampled = np.empty(total_points, dtype=np.int32)
    
    for i in prange(n_voxels):
        voxel_count = voxel_counts[i]
        start_pos = cumsum[i]
        base_idx = i * points_per_loop
        
        for p in range(points_per_loop):
            logical_idx = loop_idx * points_per_loop + p
            local_idx = logical_idx % voxel_count
            sampled[base_idx + p] = sort_idx[start_pos + local_idx]
    
    return sampled


class BinPklDataset1(DatasetBase):
    """
    bin+pkl é€»è¾‘ç´¢å¼•æ ¼å¼ç‚¹äº‘æ•°æ®çš„æ•°æ®é›†ç±» (å¯¹åº” tile_las1.py)
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - full: å…¨é‡æ¨¡å¼ï¼ŒåŠ è½½æ‰€æœ‰åŸå§‹ç‚¹
    - voxel: ä½“ç´ æ¨¡å¼ï¼ŒåŸºäºä½“ç´ åŒ–ç´¢å¼•é‡‡æ ·
      - train/val: æ¯ä¸ªä½“ç´ éšæœºå– 1 ä¸ªç‚¹ï¼Œæ•°æ®é›†é•¿åº¦ = åˆ†å—æ•°
      - test/predict: æ¨¡è¿ç®—é‡‡æ ·ç¡®ä¿å…¨è¦†ç›–ï¼Œæ•°æ®é›†é•¿åº¦ = sum(actual_loops)
    """
    
    def __init__(
        self,
        data_root,
        split='train',
        assets=None,
        transform=None,
        ignore_label=-1,
        loop=1,
        cache_data=False,
        class_mapping=None,
        h_norm_grid=1.0,
        mode='voxel',
        max_loops: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ– BinPklDataset1
        
        å‚æ•°ï¼š
            data_root: åŒ…å« bin+pkl æ–‡ä»¶çš„æ ¹ç›®å½•ï¼Œæˆ–å•ä¸ª pkl æ–‡ä»¶è·¯å¾„ï¼Œ
                      æˆ– pkl æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            split: æ•°æ®é›†åˆ’åˆ†ï¼ˆ'train'ã€'val'ã€'test'ã€'predict'ï¼‰
            assets: è¦åŠ è½½çš„æ•°æ®å±æ€§åˆ—è¡¨ï¼ˆé»˜è®¤ï¼š['coord', 'intensity', 'classification']ï¼‰
            transform: è¦åº”ç”¨çš„æ•°æ®å˜æ¢
            ignore_label: åœ¨è®­ç»ƒä¸­å¿½ç•¥çš„æ ‡ç­¾
            loop: éå†æ•°æ®é›†çš„æ¬¡æ•°ï¼ˆç”¨äºè®­ç»ƒï¼‰
            cache_data: æ˜¯å¦åœ¨å†…å­˜ä¸­ç¼“å­˜åŠ è½½çš„æ•°æ®
            class_mapping: å°†åŸå§‹ç±»åˆ«æ ‡ç­¾æ˜ å°„åˆ°è¿ç»­æ ‡ç­¾çš„å­—å…¸
            h_norm_grid: è®¡ç®—å½’ä¸€åŒ–é«˜ç¨‹æ—¶ä½¿ç”¨çš„æ …æ ¼åˆ†è¾¨ç‡ï¼ˆç±³ï¼‰
            mode: é‡‡æ ·æ¨¡å¼
                - 'full': å…¨é‡æ¨¡å¼ï¼ŒåŠ è½½æ‰€æœ‰åŸå§‹ç‚¹
                - 'voxel': ä½“ç´ æ¨¡å¼ï¼ŒåŸºäºä½“ç´ åŒ–ç´¢å¼•é‡‡æ ·
            max_loops: ä½“ç´ æ¨¡å¼ä¸‹çš„æœ€å¤§é‡‡æ ·è½®æ¬¡ (ä»… test/predict ç”Ÿæ•ˆ)
                - None: æŒ‰ä½“ç´ å†…æœ€å¤§ç‚¹æ•°è¿›è¡Œé‡‡æ ·
                - è®¾ç½®å€¼: é™åˆ¶æœ€å¤§è½®æ•°ï¼Œç¡®ä¿åœ¨ max_loops è½®å†…é‡‡å®Œæ‰€æœ‰ç‚¹
        """
        # å¦‚æœæœªæŒ‡å®šï¼Œåˆ™è®¾ç½®é»˜è®¤èµ„äº§
        if assets is None:
            assets = ['coord', 'classification']
        
        # åˆå§‹åŒ–å…ƒæ•°æ®ç¼“å­˜
        self._metadata_cache = {}
        self._mmap_cache = {}  # memmap ç¼“å­˜
        self.h_norm_grid = h_norm_grid
        self.mode = mode
        self.max_loops = max_loops
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            data_root=data_root,
            split=split,
            assets=assets,
            transform=transform,
            ignore_label=ignore_label,
            loop=loop,
            cache_data=cache_data,
            class_mapping=class_mapping
        )
    
    def _load_data_list(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ‰€æœ‰æ•°æ®æ ·æœ¬çš„åˆ—è¡¨
        
        æ ¹æ®æ¨¡å¼å’Œ split ç”Ÿæˆä¸åŒçš„æ•°æ®åˆ—è¡¨ï¼š
        - full æ¨¡å¼: æ¯ä¸ª segment ä¸€ä¸ªæ ·æœ¬
        - voxel æ¨¡å¼:
          - train/val: æ¯ä¸ª segment ä¸€ä¸ªæ ·æœ¬ï¼ˆéšæœºé‡‡æ ·ï¼‰
          - test/predict: æ¯ä¸ª segment çš„æ¯ä¸ª loop ä¸€ä¸ªæ ·æœ¬
        """
        data_list = []
        
        # å¤„ç†ä¸åŒçš„ data_root ç±»å‹
        pkl_files = []
        
        if isinstance(self.data_root, (list, tuple)):
            pkl_files = [Path(p) for p in self.data_root]
            print(f"ä» {len(pkl_files)} ä¸ªæŒ‡å®šçš„ pkl æ–‡ä»¶åŠ è½½")
        elif self.data_root.is_file() and self.data_root.suffix == '.pkl':
            pkl_files = [self.data_root]
            print(f"ä»å•ä¸ª pkl æ–‡ä»¶åŠ è½½: {self.data_root.name}")
        else:
            pkl_files = sorted(self.data_root.glob('*.pkl'))
            if len(pkl_files) == 0:
                raise ValueError(f"åœ¨ {self.data_root} ä¸­æœªæ‰¾åˆ° pkl æ–‡ä»¶")
            print(f"åœ¨ç›®å½•ä¸­æ‰¾åˆ° {len(pkl_files)} ä¸ª pkl æ–‡ä»¶")
        
        total_segments = 0
        total_samples = 0
        
        for pkl_path in pkl_files:
            if not pkl_path.exists():
                print(f"è­¦å‘Š: {pkl_path} æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
                continue
                
            bin_path = pkl_path.with_suffix('.bin')
            
            if not bin_path.exists():
                print(f"è­¦å‘Š: {bin_path.name} æœªæ‰¾åˆ°ï¼Œè·³è¿‡ {pkl_path.name}")
                continue
            
            # åŠ è½½ pkl å…ƒæ•°æ®å¹¶ç¼“å­˜
            with open(pkl_path, 'rb') as f:
                metadata = pickle.load(f)
            
            pkl_key = str(pkl_path)
            self._metadata_cache[pkl_key] = metadata
            
            grid_size = metadata.get('grid_size', None)
            
            # å¤„ç†æ¯ä¸ª segment
            for segment_info in metadata['segments']:
                segment_id = segment_info['segment_id']
                num_points = segment_info['num_points']
                total_segments += 1
                
                # è·å–ä½“ç´ ä¿¡æ¯
                voxel_counts = segment_info.get('voxel_counts', None)
                max_voxel_count = int(voxel_counts.max()) if voxel_counts is not None and len(voxel_counts) > 0 else 1
                num_voxels = len(voxel_counts) if voxel_counts is not None else 0
                
                # è®¡ç®—è¾¹ç•Œä¿¡æ¯
                bounds = segment_info.get('bounds', {})
                if not bounds:
                    bounds = {
                        'x_min': segment_info.get('x_min', 0),
                        'x_max': segment_info.get('x_max', 0),
                        'y_min': segment_info.get('y_min', 0),
                        'y_max': segment_info.get('y_max', 0),
                        'z_min': segment_info.get('z_min', 0),
                        'z_max': segment_info.get('z_max', 0),
                    }
                
                # æ ¹æ®æ¨¡å¼å’Œ split å†³å®šå¦‚ä½•ç”Ÿæˆæ ·æœ¬
                if self.mode == 'full':
                    # å…¨é‡æ¨¡å¼ï¼šæ¯ä¸ª segment ä¸€ä¸ªæ ·æœ¬
                    data_list.append({
                        'bin_path': str(bin_path),
                        'pkl_path': str(pkl_path),
                        'segment_id': segment_id,
                        'num_points': num_points,
                        'num_voxels': num_voxels,
                        'max_voxel_count': max_voxel_count,
                        'file_name': bin_path.stem,
                        'bounds': bounds,
                        'loop_idx': None,  # å…¨é‡æ¨¡å¼æ—  loop
                        'points_per_loop': None,
                    })
                    total_samples += 1
                    
                elif self.mode == 'voxel':
                    if self.split in ['train', 'val']:
                        # train/val: æ¯ä¸ª segment ä¸€ä¸ªæ ·æœ¬ï¼Œéšæœºé‡‡æ ·
                        data_list.append({
                            'bin_path': str(bin_path),
                            'pkl_path': str(pkl_path),
                            'segment_id': segment_id,
                            'num_points': num_points,
                            'num_voxels': num_voxels,
                            'max_voxel_count': max_voxel_count,
                            'file_name': bin_path.stem,
                            'bounds': bounds,
                            'loop_idx': None,  # train/val æ—¶ä¸º Noneï¼Œè¡¨ç¤ºéšæœºé‡‡æ ·
                            'points_per_loop': 1,
                        })
                        total_samples += 1
                        
                    else:
                        # test/predict: æ¯ä¸ª loop ä¸€ä¸ªæ ·æœ¬ï¼Œç¡®ä¿å…¨è¦†ç›–
                        if voxel_counts is None or num_voxels == 0:
                            # æ— ä½“ç´ åŒ–ä¿¡æ¯ï¼Œå•ä¸ªæ ·æœ¬
                            actual_loops = 1
                            points_per_loop = num_points
                        else:
                            # è®¡ç®—å®é™…è½®æ•°å’Œæ¯è½®é‡‡æ ·ç‚¹æ•°
                            actual_loops, points_per_loop = self._compute_sampling_params(
                                max_voxel_count, self.max_loops
                            )
                        
                        for loop_idx in range(actual_loops):
                            data_list.append({
                                'bin_path': str(bin_path),
                                'pkl_path': str(pkl_path),
                                'segment_id': segment_id,
                                'num_points': num_points,
                                'num_voxels': num_voxels,
                                'max_voxel_count': max_voxel_count,
                                'file_name': bin_path.stem,
                                'bounds': bounds,
                                'loop_idx': loop_idx,
                                'points_per_loop': points_per_loop,
                                'actual_loops': actual_loops,
                            })
                            total_samples += 1
                else:
                    raise ValueError(f"æœªçŸ¥æ¨¡å¼: {self.mode}")
        
        print(f"ä» {len(pkl_files)} ä¸ªæ–‡ä»¶åŠ è½½äº† {total_segments} ä¸ª segments, "
              f"å…± {total_samples} ä¸ªæ ·æœ¬ (mode={self.mode}, split={self.split})")
        
        return data_list
    
    def _compute_sampling_params(self, max_voxel_count: int, max_loops: Optional[int]) -> Tuple[int, int]:
        """
        è®¡ç®—é‡‡æ ·å‚æ•°ï¼šå®é™…è½®æ•°å’Œæ¯è½®é‡‡æ ·ç‚¹æ•°
        
        Args:
            max_voxel_count: ä½“ç´ å†…æœ€å¤§ç‚¹æ•°
            max_loops: æœ€å¤§é‡‡æ ·è½®æ¬¡é™åˆ¶
            
        Returns:
            (actual_loops, points_per_loop)
        """
        if max_loops is None:
            # æœªè®¾ç½® max_loopsï¼šæŒ‰æœ€å¤§ä½“ç´ ç‚¹æ•°é‡‡æ ·ï¼Œæ¯è½®é‡‡ 1 ä¸ªç‚¹
            return max_voxel_count, 1
        elif max_voxel_count <= max_loops:
            # æœ€å¤§ç‚¹æ•° <= max_loopsï¼šæŒ‰å®é™…æœ€å¤§ç‚¹æ•°é‡‡æ ·ï¼Œæ¯è½®é‡‡ 1 ä¸ªç‚¹
            return max_voxel_count, 1
        else:
            # æœ€å¤§ç‚¹æ•° > max_loopsï¼šé™åˆ¶è½®æ•°ï¼Œæ¯è½®é‡‡å¤šä¸ªç‚¹
            points_per_loop = int(np.ceil(max_voxel_count / max_loops))
            return max_loops, points_per_loop
    
    def _get_metadata(self, pkl_path: str) -> dict:
        """è·å–ç¼“å­˜çš„å…ƒæ•°æ®"""
        if pkl_path not in self._metadata_cache:
            with open(pkl_path, 'rb') as f:
                self._metadata_cache[pkl_path] = pickle.load(f)
        return self._metadata_cache[pkl_path]
    
    def _get_mmap(self, bin_path: str, dtype) -> np.ndarray:
        """è·å–ç¼“å­˜çš„ memmap"""
        if bin_path not in self._mmap_cache:
            self._mmap_cache[bin_path] = np.memmap(bin_path, dtype=dtype, mode='r')
        return self._mmap_cache[bin_path]
    
    def _voxel_random_sample(self, segment_info: dict, mmap_data: np.ndarray) -> np.ndarray:
        """
        ä»æ¯ä¸ªä½“ç´ ä¸­éšæœºé‡‡æ · 1 ä¸ªç‚¹ (ç”¨äº train/val)
        ä½¿ç”¨ Numba åŠ é€Ÿ
        
        æ³¨æ„ï¼šä¸ºç¡®ä¿æ¯æ¬¡è°ƒç”¨çš„éšæœºæ€§ï¼ˆä¸åŒ epochã€ä¸åŒ workerï¼‰ï¼Œ
        ä½¿ç”¨åŸºäºæ—¶é—´å’Œå¯¹è±¡ id çš„ç†µæºç”Ÿæˆéšæœºæ•°ï¼Œé¿å…å—å…¨å±€ç§å­å½±å“ã€‚
        
        Args:
            segment_info: segment å…ƒæ•°æ®
            mmap_data: å†…å­˜æ˜ å°„çš„ bin æ•°æ®
            
        Returns:
            é‡‡æ ·åçš„ç»“æ„åŒ–æ•°ç»„
        """
        indices = segment_info['indices']
        sort_idx = segment_info.get('sort_idx', None)
        voxel_counts = segment_info.get('voxel_counts', None)
        
        # å¦‚æœæ²¡æœ‰ä½“ç´ åŒ–ä¿¡æ¯ï¼Œè¿”å›å…¨éƒ¨æ•°æ®
        if sort_idx is None or voxel_counts is None:
            return mmap_data[indices]
        
        # è®¡ç®—æ¯ä¸ªä½“ç´ çš„èµ·å§‹ä½ç½®
        cumsum = np.cumsum(np.insert(voxel_counts, 0, 0)).astype(np.int64)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç‹¬ç«‹çš„éšæœºæ•°ç”Ÿæˆå™¨ï¼Œä¸å—å…¨å±€ç§å­å½±å“
        # ç»“åˆå¤šç§ç†µæºç¡®ä¿çœŸæ­£çš„éšæœºæ€§ï¼š
        # - å½“å‰æ—¶é—´çº³ç§’çº§
        # - segment_id (ä¸åŒ segment ä¸åŒ)
        # - Python å¯¹è±¡ id (ä¸åŒè°ƒç”¨ä¸åŒ)
        import time
        import os
        
        # ä½¿ç”¨å¤šç§ç†µæºåˆ›å»ºç§å­
        entropy = (
            int(time.time_ns()) ^  # çº³ç§’çº§æ—¶é—´æˆ³
            id(segment_info) ^     # å¯¹è±¡åœ°å€
            segment_info.get('segment_id', 0) ^  # segment id
            os.getpid()            # è¿›ç¨‹ id (å¤š worker æ—¶ä¸åŒ)
        )
        
        # åˆ›å»ºç‹¬ç«‹çš„éšæœºæ•°ç”Ÿæˆå™¨
        rng = np.random.Generator(np.random.PCG64(entropy & 0xFFFFFFFFFFFFFFFF))
        
        n_voxels = len(voxel_counts)
        random_offsets = rng.integers(0, 2**31, size=n_voxels, dtype=np.int32)
        
        # ä½¿ç”¨ Numba å¹¶è¡ŒåŠ é€Ÿé‡‡æ ·
        sampled_local_indices = _voxel_random_sample_parallel(
            sort_idx.astype(np.int32), 
            voxel_counts.astype(np.int32), 
            cumsum,
            random_offsets
        )
        
        global_indices = indices[sampled_local_indices]
        return mmap_data[global_indices]
    
    def _voxel_modulo_sample(self, segment_info: dict, mmap_data: np.ndarray,
                             loop_idx: int, points_per_loop: int = 1) -> Tuple[np.ndarray, np.ndarray]:
        """
        å¯¹ segment è¿›è¡Œä½“ç´ æ¨¡è¿ç®—é‡‡æ · (ç”¨äº test/predict)
        ä½¿ç”¨ Numba åŠ é€Ÿ
        
        Args:
            segment_info: segment å…ƒæ•°æ®
            mmap_data: å†…å­˜æ˜ å°„çš„ bin æ•°æ®
            loop_idx: å½“å‰é‡‡æ ·è½®æ¬¡
            points_per_loop: æ¯è½®æ¯ä½“ç´ é‡‡æ ·ç‚¹æ•°
            
        Returns:
            (é‡‡æ ·åçš„ç»“æ„åŒ–æ•°ç»„, åŸå§‹ç‚¹ç´¢å¼•)
        """
        indices = segment_info['indices']
        sort_idx = segment_info.get('sort_idx', None)
        voxel_counts = segment_info.get('voxel_counts', None)
        
        # å¦‚æœæ²¡æœ‰ä½“ç´ åŒ–ä¿¡æ¯ï¼Œè¿”å›å…¨éƒ¨æ•°æ®
        if sort_idx is None or voxel_counts is None:
            return mmap_data[indices], indices.copy()
        
        # è®¡ç®—æ¯ä¸ªä½“ç´ çš„èµ·å§‹ä½ç½®
        cumsum = np.cumsum(np.insert(voxel_counts, 0, 0)).astype(np.int64)
        
        # ä½¿ç”¨ Numba å¹¶è¡ŒåŠ é€Ÿé‡‡æ ·
        sampled_local_indices = _voxel_modulo_sample_parallel(
            sort_idx.astype(np.int32),
            voxel_counts.astype(np.int32),
            cumsum,
            loop_idx,
            points_per_loop
        )
        
        global_indices = indices[sampled_local_indices]
        return mmap_data[global_indices], global_indices
    
    def _compute_h_norm(self, coord: np.ndarray, is_ground: np.ndarray, 
                       grid_resolution: float = 1.0) -> np.ndarray:
        """
        åŸºäºåœ°é¢ç‚¹æ ‡è®°è®¡ç®—å½’ä¸€åŒ–é«˜ç¨‹ï¼ˆåœ°ä¸Šé«˜ç¨‹ï¼‰
        """
        ground_mask = (is_ground == 1)
        
        if not np.any(ground_mask):
            z_min = coord[:, 2].min()
            return (coord[:, 2] - z_min).astype(np.float32)
        
        ground_points = coord[ground_mask]
        ground_xy = ground_points[:, :2]
        ground_z = ground_points[:, 2]
        n_ground = len(ground_points)
        
        if n_ground < 10:
            ground_z_base = ground_z.min()
            h_norm = coord[:, 2] - ground_z_base
        elif n_ground < 50:
            from scipy.spatial import cKDTree
            tree = cKDTree(ground_xy)
            k = min(3, n_ground)
            distances, indices = tree.query(coord[:, :2], k=k)
            
            if k == 1:
                local_ground_z = ground_z[indices]
            else:
                weights = 1.0 / (distances + 1e-8)
                weights = weights / weights.sum(axis=1, keepdims=True)
                local_ground_z = (ground_z[indices] * weights).sum(axis=1)
            
            h_norm = coord[:, 2] - local_ground_z
        else:
            h_norm = self._compute_h_norm_tin_raster(
                coord, ground_xy, ground_z, grid_resolution
            )
        
        return h_norm.astype(np.float32)
    
    def _compute_h_norm_tin_raster(self, coord: np.ndarray, ground_xy: np.ndarray, 
                                   ground_z: np.ndarray, grid_resolution: float) -> np.ndarray:
        """ä½¿ç”¨ TIN + Raster æ··åˆæ–¹æ³•è®¡ç®— h_norm"""
        from scipy.interpolate import griddata
        from scipy.spatial import cKDTree
        
        x_min, y_min = coord[:, :2].min(axis=0)
        x_max, y_max = coord[:, :2].max(axis=0)
        
        n_x = int(np.ceil((x_max - x_min) / grid_resolution)) + 1
        n_y = int(np.ceil((y_max - y_min) / grid_resolution)) + 1
        
        MAX_GRID_SIZE = 2000
        if n_x > MAX_GRID_SIZE or n_y > MAX_GRID_SIZE:
            grid_resolution = max(
                (x_max - x_min) / MAX_GRID_SIZE,
                (y_max - y_min) / MAX_GRID_SIZE
            )
            n_x = int(np.ceil((x_max - x_min) / grid_resolution)) + 1
            n_y = int(np.ceil((y_max - y_min) / grid_resolution)) + 1
        
        grid_x = np.linspace(x_min, x_max, n_x)
        grid_y = np.linspace(y_min, y_max, n_y)
        grid_xx, grid_yy = np.meshgrid(grid_x, grid_y)
        
        dtm_grid = griddata(
            ground_xy, ground_z, (grid_xx, grid_yy),
            method='linear', fill_value=np.nan
        )
        
        indices_x = ((coord[:, 0] - x_min) / grid_resolution).astype(int)
        indices_y = ((coord[:, 1] - y_min) / grid_resolution).astype(int)
        indices_x = np.clip(indices_x, 0, dtm_grid.shape[1] - 1)
        indices_y = np.clip(indices_y, 0, dtm_grid.shape[0] - 1)
        
        z_ground = dtm_grid[indices_y, indices_x]
        
        nan_mask = np.isnan(z_ground)
        if np.any(nan_mask):
            tree = cKDTree(ground_xy)
            k = min(3, len(ground_xy))
            nan_points = coord[nan_mask, :2]
            
            if k == 1:
                _, indices = tree.query(nan_points, k=1)
                z_ground[nan_mask] = ground_z[indices]
            else:
                distances, indices = tree.query(nan_points, k=k)
                weights = 1.0 / (distances + 1e-8)
                weights = weights / weights.sum(axis=1, keepdims=True)
                z_ground[nan_mask] = (ground_z[indices] * weights).sum(axis=1)
        
        return coord[:, 2] - z_ground
    
    def _load_data(self, idx: int) -> Dict[str, Any]:
        """
        åŠ è½½ç‰¹å®šçš„æ•°æ®æ ·æœ¬
        """
        sample_info = self.data_list[idx]
        
        bin_path = sample_info['bin_path']
        pkl_path = sample_info['pkl_path']
        segment_id = sample_info['segment_id']
        loop_idx = sample_info.get('loop_idx', None)
        points_per_loop = sample_info.get('points_per_loop', 1)
        
        # è·å–å…ƒæ•°æ®
        metadata = self._get_metadata(pkl_path)
        
        # æŸ¥æ‰¾ segment ä¿¡æ¯
        segment_info = None
        for seg in metadata['segments']:
            if seg['segment_id'] == segment_id:
                segment_info = seg
                break
        
        if segment_info is None:
            raise ValueError(f"åœ¨ {pkl_path} ä¸­æœªæ‰¾åˆ° segment {segment_id}")
        
        # è·å– memmap æ•°æ®
        mmap_data = self._get_mmap(bin_path, metadata['dtype'])
        
        # æ ¹æ®æ¨¡å¼å’Œ split é‡‡æ ·æ•°æ®
        original_indices = None
        
        if self.mode == 'full':
            # å…¨é‡æ¨¡å¼
            indices = segment_info['indices']
            segment_points = mmap_data[indices]
            if self.split in ['test', 'predict']:
                original_indices = indices.copy()
                
        elif self.mode == 'voxel':
            if self.split in ['train', 'val']:
                # éšæœºé‡‡æ ·
                segment_points = self._voxel_random_sample(segment_info, mmap_data)
            else:
                # æ¨¡è¿ç®—é‡‡æ ·
                segment_points, original_indices = self._voxel_modulo_sample(
                    segment_info, mmap_data, loop_idx, points_per_loop
                )
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å¼: {self.mode}")
        
        # æå–è¯·æ±‚çš„èµ„äº§
        data = {}
        
        # ğŸ”¥ åæ ‡ï¼šè½¬æ¢ä¸ºå±€éƒ¨åæ ‡ä»¥ä¿æŒ float32 ç²¾åº¦
        # åŸå§‹åæ ‡ä¸º float64ï¼ˆå¦‚ 508000.0, 5443500.0ï¼‰ï¼Œç›´æ¥è½¬ float32 ä¼šä¸¢å¤±ç²¾åº¦
        # ä½¿ç”¨å±€éƒ¨åæ ‡ï¼ˆå‡å» local_minï¼‰åï¼Œåæ ‡èŒƒå›´é€šå¸¸åœ¨ 0~50m å†…ï¼Œfloat32 è¶³å¤Ÿç²¾ç¡®
        local_min = segment_info.get('local_min', None)
        
        coord = np.stack([
            segment_points['X'],
            segment_points['Y'],
            segment_points['Z']
        ], axis=1)  # ä¿æŒ float64
        
        if local_min is not None:
            # è½¬æ¢ä¸ºå±€éƒ¨åæ ‡
            coord = coord - local_min.astype(np.float64)
        
        coord = coord.astype(np.float32)
        data['coord'] = coord
        
        # ä¿å­˜åŸå§‹åæ ‡åç§»é‡ï¼ˆç”¨äºé¢„æµ‹æ—¶æ¢å¤å…¨å±€åæ ‡ï¼‰
        if self.split in ['test', 'predict'] and local_min is not None:
            data['coord_offset'] = local_min.astype(np.float64)
        
        # å…¶ä»–èµ„äº§
        for asset in self.assets:
            if asset == 'coord':
                continue
                
            elif asset == 'intensity':
                if 'intensity' not in segment_points.dtype.names:
                    raise ValueError(
                        f"è¯·æ±‚çš„å±æ€§ 'intensity' åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚\n"
                        f"å¯ç”¨å­—æ®µ: {list(segment_points.dtype.names)}\n"
                        f"è¯·æ£€æŸ¥ assets é…ç½®æˆ–æ•°æ®æ–‡ä»¶ã€‚"
                    )
                intensity = segment_points['intensity'].astype(np.float32)
                # å½’ä¸€åŒ–åˆ° [0, 1]
                intensity = intensity / 65535.0
                data['intensity'] = intensity
                    
            elif asset == 'color':
                required_fields = ['red', 'green', 'blue']
                missing = [f for f in required_fields if f not in segment_points.dtype.names]
                if missing:
                    raise ValueError(
                        f"è¯·æ±‚çš„å±æ€§ 'color' æ‰€éœ€å­—æ®µ {missing} åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚\n"
                        f"å¯ç”¨å­—æ®µ: {list(segment_points.dtype.names)}\n"
                        f"è¯·æ£€æŸ¥ assets é…ç½®æˆ–æ•°æ®æ–‡ä»¶ã€‚"
                    )
                color = np.stack([
                    segment_points['red'],
                    segment_points['green'],
                    segment_points['blue']
                ], axis=1).astype(np.float32)
                data['color'] = color

            elif asset == 'echo':
                required_fields = ['return_number', 'number_of_returns']
                missing = [f for f in required_fields if f not in segment_points.dtype.names]
                if missing:
                    raise ValueError(
                        f"è¯·æ±‚çš„å±æ€§ 'echo' æ‰€éœ€å­—æ®µ {missing} åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚\n"
                        f"å¯ç”¨å­—æ®µ: {list(segment_points.dtype.names)}\n"
                        f"è¯·æ£€æŸ¥ assets é…ç½®æˆ–æ•°æ®æ–‡ä»¶ã€‚"
                    )
                return_number = segment_points['return_number'].astype(np.float32)
                number_of_returns = segment_points['number_of_returns'].astype(np.float32)
                echo = np.stack([
                    (return_number == 1).astype(np.float32) * 2 - 1,
                    (return_number == number_of_returns).astype(np.float32) * 2 - 1,
                ], axis=1)
                data['echo'] = echo

            elif asset == 'normal':
                required_fields = ['normal_x', 'normal_y', 'normal_z']
                missing = [f for f in required_fields if f not in segment_points.dtype.names]
                if missing:
                    raise ValueError(
                        f"è¯·æ±‚çš„å±æ€§ 'normal' æ‰€éœ€å­—æ®µ {missing} åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚\n"
                        f"å¯ç”¨å­—æ®µ: {list(segment_points.dtype.names)}\n"
                        f"è¯·æ£€æŸ¥ assets é…ç½®æˆ–æ•°æ®æ–‡ä»¶ã€‚"
                    )
                normal = np.stack([
                    segment_points['normal_x'],
                    segment_points['normal_y'],
                    segment_points['normal_z']
                ], axis=1).astype(np.float32)
                data['normal'] = normal

            elif asset == 'h_norm':
                if 'is_ground' in segment_points.dtype.names:
                    is_ground = segment_points['is_ground']
                    h_norm = self._compute_h_norm(coord, is_ground, self.h_norm_grid)
                    data['h_norm'] = h_norm
                else:
                    raise ValueError(
                        f"è¯·æ±‚çš„å±æ€§ 'h_norm' æ‰€éœ€å­—æ®µ 'is_ground' åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚\n"
                        f"å¯ç”¨å­—æ®µ: {list(segment_points.dtype.names)}\n"
                        f"è¯·æ£€æŸ¥ assets é…ç½®æˆ–æ•°æ®æ–‡ä»¶ã€‚"
                    )

            elif asset == 'class':
                if 'classification' not in segment_points.dtype.names:
                    raise ValueError(
                        f"è¯·æ±‚çš„å±æ€§ 'class' æ‰€éœ€å­—æ®µ 'classification' åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚\n"
                        f"å¯ç”¨å­—æ®µ: {list(segment_points.dtype.names)}\n"
                        f"è¯·æ£€æŸ¥ assets é…ç½®æˆ–æ•°æ®æ–‡ä»¶ã€‚"
                    )
                labels = segment_points['classification'].astype(np.int64)
                # åº”ç”¨ç±»åˆ«æ˜ å°„
                if self.class_mapping is not None:
                    mapped_labels = np.full_like(labels, self.ignore_label)
                    for orig_label, new_label in self.class_mapping.items():
                        mapped_labels[labels == orig_label] = new_label
                    labels = mapped_labels
                data['class'] = labels

        # test/predict æ—¶å­˜å‚¨ç´¢å¼•ä¿¡æ¯
        if self.split in ['test', 'predict']:
            if original_indices is not None:
                data['indices'] = original_indices
            data['bin_file'] = sample_info.get('file_name', Path(bin_path).stem)
            data['bin_path'] = bin_path
            data['pkl_path'] = pkl_path
            data['segment_id'] = segment_id
            if loop_idx is not None:
                data['loop_idx'] = loop_idx
        
        return data
    
    def get_segment_info(self, idx: int) -> Dict[str, Any]:
        """è·å–ç‰¹å®šç‰‡æ®µçš„å…ƒæ•°æ®"""
        if idx < 0 or idx >= len(self.data_list):
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {len(self.data_list)})")
        
        sample_info = self.data_list[idx]
        pkl_path = sample_info['pkl_path']
        segment_id = sample_info['segment_id']
        
        metadata = self._get_metadata(pkl_path)
        
        for seg in metadata['segments']:
            if seg['segment_id'] == segment_id:
                return seg
        
        raise ValueError(f"æœªæ‰¾åˆ° segment {segment_id}")
    
    def get_file_metadata(self, idx: int) -> Dict[str, Any]:
        """è·å–åŒ…å«ç‰¹å®šç‰‡æ®µçš„æ–‡ä»¶çš„å…ƒæ•°æ®"""
        if idx < 0 or idx >= len(self.data_list):
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {len(self.data_list)})")
        
        sample_info = self.data_list[idx]
        pkl_path = sample_info['pkl_path']
        
        metadata = self._get_metadata(pkl_path)
        
        return {k: v for k, v in metadata.items() if k != 'segments'}
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.data_list) == 0:
            return {}
        
        num_points_list = [s['num_points'] for s in self.data_list]
        num_voxels_list = [s.get('num_voxels', 0) for s in self.data_list]
        
        stats = {
            'num_samples': len(self.data_list),
            'mode': self.mode,
            'split': self.split,
            'num_points': {
                'total': sum(num_points_list),
                'mean': np.mean(num_points_list),
                'median': np.median(num_points_list),
                'min': np.min(num_points_list),
                'max': np.max(num_points_list),
                'std': np.std(num_points_list),
            },
            'num_voxels': {
                'mean': np.mean(num_voxels_list) if num_voxels_list else 0,
                'min': np.min(num_voxels_list) if num_voxels_list else 0,
                'max': np.max(num_voxels_list) if num_voxels_list else 0,
            }
        }
        
        return stats
    
    def print_stats(self):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        
        print("="*70)
        print(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ ({self.__class__.__name__})")
        print("="*70)
        print(f"åˆ’åˆ†: {self.split}")
        print(f"æ¨¡å¼: {self.mode}")
        print(f"æ ·æœ¬æ•°: {stats['num_samples']:,}")
        print(f"\næ¯æ ·æœ¬ç‚¹æ•°:")
        print(f"  - æ€»è®¡: {stats['num_points']['total']:,}")
        print(f"  - å¹³å‡: {stats['num_points']['mean']:,.1f}")
        print(f"  - ä¸­ä½æ•°: {stats['num_points']['median']:,.0f}")
        print(f"  - æœ€å°: {stats['num_points']['min']:,}")
        print(f"  - æœ€å¤§: {stats['num_points']['max']:,}")
        if self.mode == 'voxel':
            print(f"\nä½“ç´ æ•°:")
            print(f"  - å¹³å‡: {stats['num_voxels']['mean']:,.1f}")
            print(f"  - æœ€å°: {stats['num_voxels']['min']:,}")
            print(f"  - æœ€å¤§: {stats['num_voxels']['max']:,}")
        print("="*70)
    
    def get_class_distribution(self) -> Optional[Dict[int, int]]:
        """è·å–æ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ"""
        if len(self.data_list) == 0:
            return {}
        
        pkl_path = self.data_list[0]['pkl_path']
        metadata = self._get_metadata(pkl_path)
        
        if 'label_counts' in metadata:
            if self.class_mapping is not None:
                mapped_counts = {}
                for orig_label, count in metadata['label_counts'].items():
                    if orig_label in self.class_mapping:
                        new_label = self.class_mapping[orig_label]
                        mapped_counts[new_label] = mapped_counts.get(new_label, 0) + count
                return mapped_counts
            else:
                return metadata['label_counts']
        
        return {}
    
    def get_sample_num_points(self) -> List[int]:
        """
        è·å–æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°åˆ—è¡¨ï¼ˆç”¨äº DynamicBatchSamplerï¼‰
        
        æ³¨æ„ï¼šåœ¨ voxel æ¨¡å¼ä¸‹ï¼Œè¿”å›çš„æ˜¯ä½“ç´ æ•°ï¼ˆé‡‡æ ·åçš„ç‚¹æ•°ï¼‰
        """
        if self.mode == 'voxel':
            # ä½“ç´ æ¨¡å¼ï¼šé‡‡æ ·åçš„ç‚¹æ•° = ä½“ç´ æ•° Ã— points_per_loop
            return [
                s.get('num_voxels', s['num_points']) * s.get('points_per_loop', 1)
                for s in self.data_list
            ]
        else:
            # å…¨é‡æ¨¡å¼ï¼šåŸå§‹ç‚¹æ•°
            return [s['num_points'] for s in self.data_list]


def create_dataset(
    data_root,
    split='train',
    assets=None,
    transform=None,
    ignore_label=-1,
    loop=1,
    cache_data=False,
    mode='voxel',
    max_loops=None,
    **kwargs
):
    """
    åˆ›å»º BinPklDataset1 çš„å·¥å‚å‡½æ•°
    """
    return BinPklDataset1(
        data_root=data_root,
        split=split,
        assets=assets,
        transform=transform,
        ignore_label=ignore_label,
        loop=loop,
        cache_data=cache_data,
        mode=mode,
        max_loops=max_loops,
        **kwargs
    )
