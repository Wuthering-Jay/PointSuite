"""
ç”¨äºåŠ è½½ bin+pkl æ ¼å¼ç‚¹äº‘æ•°æ®çš„æ•°æ®é›†

æœ¬æ¨¡å—å®ç°äº†æˆ‘ä»¬è‡ªå®šä¹‰ bin+pkl æ•°æ®æ ¼å¼çš„æ•°æ®é›†ç±»ï¼Œ
å…¶ä¸­ç‚¹äº‘æ•°æ®å­˜å‚¨åœ¨äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆ.binï¼‰ä¸­ï¼Œå…ƒæ•°æ®å­˜å‚¨åœ¨ pickle æ–‡ä»¶ï¼ˆ.pklï¼‰ä¸­
"""
import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional

from .dataset_base import DatasetBase


class BinPklDataset(DatasetBase):
    """
    bin+pkl æ ¼å¼ç‚¹äº‘æ•°æ®çš„æ•°æ®é›†ç±»
    
    æ­¤æ•°æ®é›†åŠ è½½ä»¥äºŒè¿›åˆ¶æ ¼å¼ï¼ˆ.binï¼‰å­˜å‚¨çš„é¢„å¤„ç†ç‚¹äº‘ç‰‡æ®µï¼Œ
    å…ƒæ•°æ®ä»¥ pickle æ ¼å¼ï¼ˆ.pklï¼‰å­˜å‚¨
    
    æ•°æ®ç»“æ„ï¼š
    - .bin æ–‡ä»¶ï¼šä»¥ç»“æ„åŒ– numpy æ•°ç»„æ ¼å¼åŒ…å«æ‰€æœ‰ç‚¹æ•°æ®
    - .pkl æ–‡ä»¶ï¼šåŒ…å«å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
        - ç‰‡æ®µä¿¡æ¯ï¼ˆç´¢å¼•ã€è¾¹ç•Œã€æ ‡ç­¾è®¡æ•°ï¼‰
        - åŸå§‹ LAS æ–‡ä»¶å¤´
        - å¤„ç†å‚æ•°
    
    æ¯ä¸ªç‰‡æ®µæˆä¸ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬
    """
    
    def __init__(
        self,
        data_root,
        split='train',
        assets=None,
        transform=None,
        ignore_label=-1,
        loop=1,
        cache_data=False,
        class_mapping=None,
        h_norm_grid=1.0
    ):
        """
        åˆå§‹åŒ– BinPklDataset
        
        å‚æ•°ï¼š
            data_root: åŒ…å« bin+pkl æ–‡ä»¶çš„æ ¹ç›®å½•ï¼Œæˆ–å•ä¸ª pkl æ–‡ä»¶è·¯å¾„ï¼Œ
                      æˆ– pkl æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            split: æ•°æ®é›†åˆ’åˆ†ï¼ˆ'train'ã€'val'ã€'test'ã€'predict'ï¼‰
                  - train/val: ä¸å­˜å‚¨ç‚¹ç´¢å¼•
                  - test/predict: å­˜å‚¨ç‚¹ç´¢å¼•ç”¨äºé¢„æµ‹æŠ•ç¥¨æœºåˆ¶
            assets: è¦åŠ è½½çš„æ•°æ®å±æ€§åˆ—è¡¨ï¼ˆé»˜è®¤ï¼š['coord', 'intensity', 'classification']ï¼‰
            transform: è¦åº”ç”¨çš„æ•°æ®å˜æ¢
            ignore_label: åœ¨è®­ç»ƒä¸­å¿½ç•¥çš„æ ‡ç­¾
            loop: éå†æ•°æ®é›†çš„æ¬¡æ•°ï¼ˆç”¨äºè®­ç»ƒï¼‰
            cache_data: æ˜¯å¦åœ¨å†…å­˜ä¸­ç¼“å­˜åŠ è½½çš„æ•°æ®
                       - å¦‚æœä¸º Trueï¼šæ‰€æœ‰åŠ è½½çš„æ ·æœ¬éƒ½ç¼“å­˜åœ¨å†…å­˜ä¸­ä»¥åŠ å¿«é‡å¤è®¿é—®
                                     é€‚ç”¨äºèƒ½æ”¾å…¥ RAM çš„å°å‹æ•°æ®é›†
                       - å¦‚æœä¸º Falseï¼šæ¯æ¬¡ä»ç£ç›˜åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨ memmap æé«˜æ•ˆç‡ï¼‰
                                      é€‚ç”¨äºå¤§å‹æ•°æ®é›†
            class_mapping: å°†åŸå§‹ç±»åˆ«æ ‡ç­¾æ˜ å°„åˆ°è¿ç»­æ ‡ç­¾çš„å­—å…¸
                          ç¤ºä¾‹ï¼š{0: 0, 1: 1, 2: 2, 6: 3, 9: 4}
                          å¦‚æœä¸º Noneï¼Œåˆ™ä¸åº”ç”¨æ˜ å°„
            h_norm_grid: è®¡ç®—å½’ä¸€åŒ–é«˜ç¨‹æ—¶ä½¿ç”¨çš„æ …æ ¼åˆ†è¾¨ç‡ï¼ˆç±³ï¼‰
        """
        # å¦‚æœæœªæŒ‡å®šï¼Œåˆ™è®¾ç½®é»˜è®¤èµ„äº§
        if assets is None:
            assets = ['coord', 'intensity', 'classification']
        
        # åˆå§‹åŒ–å…ƒæ•°æ®ç¼“å­˜ï¼ˆæ˜¾è‘—åŠ å¿«æ•°æ®åŠ è½½é€Ÿåº¦ï¼‰
        self._metadata_cache = {}
        self.h_norm_grid = h_norm_grid
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆä¼ é€’ class_mapping å‚æ•°ï¼‰
        super().__init__(
            data_root=data_root,
            split=split,
            assets=assets,
            transform=transform,
            ignore_label=ignore_label,
            loop=loop,
            cache_data=cache_data,
            class_mapping=class_mapping  # ä¼ é€’ class_mapping ç»™çˆ¶ç±»
        )
    
    def _load_data_list(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ‰€æœ‰æ•°æ®æ ·æœ¬çš„åˆ—è¡¨
        
        è¿”å›ï¼š
            åŒ…å«æ ·æœ¬ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        data_list = []
        
        # å¤„ç†ä¸åŒçš„ data_root ç±»å‹
        pkl_files = []
        
        if isinstance(self.data_root, (list, tuple)):
            # pkl æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            pkl_files = [Path(p) for p in self.data_root]
            print(f"ä» {len(pkl_files)} ä¸ªæŒ‡å®šçš„ pkl æ–‡ä»¶åŠ è½½")
        elif self.data_root.is_file() and self.data_root.suffix == '.pkl':
            # å•ä¸ª pkl æ–‡ä»¶
            pkl_files = [self.data_root]
            print(f"ä»å•ä¸ª pkl æ–‡ä»¶åŠ è½½: {self.data_root.name}")
        else:
            # åŒ…å« pkl æ–‡ä»¶çš„ç›®å½•
            pkl_files = sorted(self.data_root.glob('*.pkl'))
            if len(pkl_files) == 0:
                raise ValueError(f"åœ¨ {self.data_root} ä¸­æœªæ‰¾åˆ° pkl æ–‡ä»¶")
            print(f"åœ¨ç›®å½•ä¸­æ‰¾åˆ° {len(pkl_files)} ä¸ª pkl æ–‡ä»¶")
        
        # ä»æ¯ä¸ª pkl æ–‡ä»¶åŠ è½½å…ƒæ•°æ®
        total_segments = 0
        
        for pkl_path in pkl_files:
            if not pkl_path.exists():
                print(f"è­¦å‘Š: {pkl_path} æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
                continue
                
            bin_path = pkl_path.with_suffix('.bin')
            
            if not bin_path.exists():
                print(f"è­¦å‘Š: {bin_path.name} æœªæ‰¾åˆ°ï¼Œè·³è¿‡ {pkl_path.name}")
                continue
            
            # åŠ è½½ pkl å…ƒæ•°æ®
            with open(pkl_path, 'rb') as f:
                metadata = pickle.load(f)
            
            # å°†æ¯ä¸ªç‰‡æ®µæ·»åŠ ä¸ºå•ç‹¬çš„æ•°æ®æ ·æœ¬
            for segment_info in metadata['segments']:
                total_segments += 1
                
                data_list.append({
                    'bin_path': str(bin_path),
                    'pkl_path': str(pkl_path),
                    'segment_id': segment_info['segment_id'],
                    'num_points': segment_info['num_points'],
                    'file_name': bin_path.stem,
                    'bounds': {
                        'x_min': segment_info.get('x_min', 0),
                        'x_max': segment_info.get('x_max', 0),
                        'y_min': segment_info.get('y_min', 0),
                        'y_max': segment_info.get('y_max', 0),
                        'z_min': segment_info.get('z_min', 0),
                        'z_max': segment_info.get('z_max', 0),
                    }
                })
        
        print(f"ä» {len(pkl_files)} ä¸ªæ–‡ä»¶åŠ è½½äº† {total_segments} ä¸ªç‰‡æ®µ")
        
        return data_list
    
    def _compute_h_norm(self, coord: np.ndarray, is_ground: np.ndarray, 
                       grid_resolution: float = 1.0) -> np.ndarray:
        """
        åŸºäºåœ°é¢ç‚¹æ ‡è®°è®¡ç®—å½’ä¸€åŒ–é«˜ç¨‹ï¼ˆåœ°ä¸Šé«˜ç¨‹ï¼‰
        
        é‡‡ç”¨ TIN + Raster æ··åˆæ–¹æ³•ï¼ˆå·¥ä¸šç•Œæ ‡å‡†ï¼‰ï¼š
        1. ä½¿ç”¨ TIN æ’å€¼ç”Ÿæˆ DTMï¼ˆæ•°å­—åœ°å½¢æ¨¡å‹ï¼‰æ …æ ¼
        2. é€šè¿‡å¿«é€Ÿæ …æ ¼æŸ¥è¯¢è®¡ç®—æ‰€æœ‰ç‚¹çš„åœ°é¢é«˜ç¨‹
        3. å¯¹ DTM æœªè¦†ç›–åŒºåŸŸä½¿ç”¨ KNN å›é€€ç­–ç•¥
        
        ä¼˜åŠ¿ï¼š
        - é€Ÿåº¦ï¼šæ …æ ¼æŸ¥è¯¢ O(1)ï¼Œæ¯” KNN å¿«å¾—å¤š
        - ç²¾åº¦ï¼šTIN æ’å€¼ä¿æŒåœ°é¢ç‚¹çš„å‡ ä½•ç²¾åº¦
        - å†…å­˜å‹å¥½ï¼šæ …æ ¼å¤§å°å¯æ§
        
        å‚æ•°ï¼š
            coord: [N, 3] ç‚¹äº‘åæ ‡ (X, Y, Z)
            is_ground: [N,] åœ°é¢ç‚¹æ ‡è®°ï¼Œ1 è¡¨ç¤ºåœ°é¢ç‚¹ï¼Œ0 è¡¨ç¤ºéåœ°é¢ç‚¹
            grid_resolution: DTM æ …æ ¼åˆ†è¾¨ç‡ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤ 0.5m
                            æ›´å°çš„å€¼ = æ›´ç²¾ç¡®ä½†æ›´æ…¢ã€å ç”¨æ›´å¤šå†…å­˜
                            æ›´å¤§çš„å€¼ = æ›´å¿«ä½†ç²¾åº¦ç¨ä½
            
        è¿”å›ï¼š
            h_norm: [N,] å½’ä¸€åŒ–é«˜ç¨‹ï¼ˆåœ°ä¸Šé«˜ç¨‹ï¼‰ï¼Œå•ä½ä¸è¾“å…¥åæ ‡ç›¸åŒ
        """
        # æå–åœ°é¢ç‚¹
        ground_mask = (is_ground == 1)
        
        # å¦‚æœæ²¡æœ‰åœ°é¢ç‚¹ï¼Œè¿”å›ç›¸å¯¹äºæœ€ä½ç‚¹çš„é«˜åº¦
        if not np.any(ground_mask):
            z_min = coord[:, 2].min()
            return (coord[:, 2] - z_min).astype(np.float32)
        
        ground_points = coord[ground_mask]
        ground_xy = ground_points[:, :2]
        ground_z = ground_points[:, 2]
        n_ground = len(ground_points)
        
        # ç­–ç•¥é€‰æ‹©ï¼šæ ¹æ®åœ°é¢ç‚¹æ•°é‡é€‰æ‹©æœ€ä¼˜æ–¹æ³•
        
        if n_ground < 10:
            # åœ°é¢ç‚¹å¤ªå°‘ï¼šä½¿ç”¨å…¨å±€æœ€å°å€¼æ–¹æ³•
            ground_z_base = ground_z.min()
            h_norm = coord[:, 2] - ground_z_base
            
        elif n_ground < 50:
            # åœ°é¢ç‚¹å¾ˆå°‘ï¼šä½¿ç”¨ç®€å• KNNï¼ˆä¸å€¼å¾—æ„å»ºæ …æ ¼ï¼‰
            from scipy.spatial import cKDTree
            tree = cKDTree(ground_xy)
            k = min(3, n_ground)
            distances, indices = tree.query(coord[:, :2], k=k)
            
            if k == 1:
                local_ground_z = ground_z[indices]
            else:
                weights = 1.0 / (distances + 1e-8)
                weights = weights / weights.sum(axis=1, keepdims=True)
                local_ground_z = (ground_z[indices] * weights).sum(axis=1)
            
            h_norm = coord[:, 2] - local_ground_z
            
        else:
            # åœ°é¢ç‚¹è¶³å¤Ÿï¼šä½¿ç”¨ TIN + Raster æ··åˆæ–¹æ³•ï¼ˆæ¨èï¼‰
            h_norm = self._compute_h_norm_tin_raster(
                coord, ground_xy, ground_z, grid_resolution
            )
        
        return h_norm.astype(np.float32)
    
    def _compute_h_norm_tin_raster(self, coord: np.ndarray, ground_xy: np.ndarray, 
                                   ground_z: np.ndarray, grid_resolution: float) -> np.ndarray:
        """
        ä½¿ç”¨ TIN + Raster æ··åˆæ–¹æ³•è®¡ç®— h_normï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰
        
        æ­¥éª¤ï¼š
        1. ç”¨ scipy.interpolate.griddata æ„å»º TIN å¹¶æ’å€¼åˆ°è§„åˆ™æ …æ ¼
        2. å°†æ‰€æœ‰ç‚¹åæ ‡æ˜ å°„åˆ°æ …æ ¼ç´¢å¼•
        3. å¿«é€ŸæŸ¥è¯¢æ …æ ¼å¾—åˆ°åœ°é¢é«˜ç¨‹
        4. å¯¹ DTM æœªè¦†ç›–åŒºåŸŸï¼ˆNaNï¼‰ä½¿ç”¨ KNN å›é€€
        
        å‚æ•°ï¼š
            coord: [N, 3] æ‰€æœ‰ç‚¹åæ ‡
            ground_xy: [M, 2] åœ°é¢ç‚¹ XY åæ ‡
            ground_z: [M,] åœ°é¢ç‚¹ Z åæ ‡
            grid_resolution: DTM æ …æ ¼åˆ†è¾¨ç‡
            
        è¿”å›ï¼š
            h_norm: [N,] å½’ä¸€åŒ–é«˜ç¨‹
        """
        from scipy.interpolate import griddata
        
        # ===== æ­¥éª¤ 1: å®šä¹‰ DTM æ …æ ¼ =====
        x_min, y_min = coord[:, :2].min(axis=0)
        x_max, y_max = coord[:, :2].max(axis=0)
        
        # è®¡ç®—æ …æ ¼å¤§å°ï¼ˆå‘ä¸Šå–æ•´ï¼‰
        n_x = int(np.ceil((x_max - x_min) / grid_resolution)) + 1
        n_y = int(np.ceil((y_max - y_min) / grid_resolution)) + 1
        
        # é™åˆ¶æ …æ ¼å¤§å°ï¼ˆé˜²æ­¢å†…å­˜çˆ†ç‚¸ï¼‰
        MAX_GRID_SIZE = 2000  # æœ€å¤§ 2000x2000 = 400 ä¸‡æ ¼å­
        if n_x > MAX_GRID_SIZE or n_y > MAX_GRID_SIZE:
            # åŠ¨æ€è°ƒæ•´åˆ†è¾¨ç‡
            grid_resolution = max(
                (x_max - x_min) / MAX_GRID_SIZE,
                (y_max - y_min) / MAX_GRID_SIZE
            )
            n_x = int(np.ceil((x_max - x_min) / grid_resolution)) + 1
            n_y = int(np.ceil((y_max - y_min) / grid_resolution)) + 1
        
        # åˆ›å»ºè§„åˆ™æ …æ ¼
        grid_x = np.linspace(x_min, x_max, n_x)
        grid_y = np.linspace(y_min, y_max, n_y)
        grid_xx, grid_yy = np.meshgrid(grid_x, grid_y)
        
        # ===== æ­¥éª¤ 2: TIN æ’å€¼ç”Ÿæˆ DTM =====
        # ä½¿ç”¨ 'linear' æ–¹æ³•ï¼ˆDelaunay ä¸‰è§’ç½‘ï¼‰
        # 'cubic' æ›´å¹³æ»‘ä½†æ›´æ…¢ï¼Œ'nearest' æœ€å¿«ä½†è´¨é‡å·®
        dtm_grid = griddata(
            ground_xy,           # ç¨€ç–åœ°é¢ç‚¹ XY
            ground_z,            # ç¨€ç–åœ°é¢ç‚¹ Z
            (grid_xx, grid_yy),  # ç›®æ ‡æ …æ ¼
            method='linear',     # TIN æ–¹æ³•
            fill_value=np.nan    # æ— æ³•æ’å€¼åŒºåŸŸå¡«å…… NaN
        )
        
        # ===== æ­¥éª¤ 3: è®¡ç®—æ‰€æœ‰ç‚¹çš„æ …æ ¼ç´¢å¼• =====
        # å°†çœŸå®åæ ‡æ˜ å°„åˆ°æ …æ ¼ç´¢å¼•
        indices_x = ((coord[:, 0] - x_min) / grid_resolution).astype(int)
        indices_y = ((coord[:, 1] - y_min) / grid_resolution).astype(int)
        
        # é˜²æ­¢ç´¢å¼•è¶Šç•Œï¼ˆè¾¹ç•Œç‚¹å¯èƒ½è¶…å‡ºï¼‰
        indices_x = np.clip(indices_x, 0, dtm_grid.shape[1] - 1)
        indices_y = np.clip(indices_y, 0, dtm_grid.shape[0] - 1)
        
        # ===== æ­¥éª¤ 4: å¿«é€Ÿæ …æ ¼æŸ¥è¯¢ =====
        # æ³¨æ„ï¼šmeshgrid åˆ›å»ºçš„æ•°ç»„æ˜¯ (n_y, n_x) å½¢çŠ¶
        z_ground = dtm_grid[indices_y, indices_x]
        
        # ===== æ­¥éª¤ 5: å¤„ç† DTM æœªè¦†ç›–åŒºåŸŸï¼ˆNaNï¼‰ =====
        nan_mask = np.isnan(z_ground)
        
        if np.any(nan_mask):
            # DTM æœªè¦†ç›–çš„ç‚¹ï¼ˆé€šå¸¸åœ¨è¾¹ç•Œæˆ–åœ°é¢ç‚¹ç¨€ç–åŒºåŸŸï¼‰
            # ä½¿ç”¨ KNN å›é€€ç­–ç•¥ï¼šæŸ¥è¯¢æœ€è¿‘çš„åœ°é¢ç‚¹
            from scipy.spatial import cKDTree
            
            tree = cKDTree(ground_xy)
            # å¯¹ NaN ç‚¹æŸ¥è¯¢æœ€è¿‘çš„ 3 ä¸ªåœ°é¢ç‚¹
            k = min(3, len(ground_xy))
            nan_points = coord[nan_mask, :2]
            
            if k == 1:
                _, indices = tree.query(nan_points, k=1)
                z_ground[nan_mask] = ground_z[indices]
            else:
                distances, indices = tree.query(nan_points, k=k)
                # è·ç¦»åŠ æƒå¹³å‡
                weights = 1.0 / (distances + 1e-8)
                weights = weights / weights.sum(axis=1, keepdims=True)
                z_ground[nan_mask] = (ground_z[indices] * weights).sum(axis=1)
        
        # ===== æ­¥éª¤ 6: è®¡ç®—å½’ä¸€åŒ–é«˜ç¨‹ =====
        h_norm = coord[:, 2] - z_ground
        
        return h_norm
    
    def _load_data(self, idx: int) -> Dict[str, Any]:
        """
        åŠ è½½ç‰¹å®šçš„æ•°æ®æ ·æœ¬
        
        é‡è¦ï¼šæ­¤æ–¹æ³•è¿”å›çš„æ•°æ®å­—å…¸ä¸­å„ä¸ªç‰¹å¾ï¼ˆcoordã€intensityã€color ç­‰ï¼‰æ˜¯ç‹¬ç«‹çš„ï¼Œ
        ä¸ä¼šé¢„å…ˆæ‹¼æ¥æˆ featureã€‚è¿™æ · transforms.py ä¸­çš„æ•°æ®å¢å¼ºæ‰èƒ½æ­£ç¡®å¤„ç†å„ä¸ªç‰¹å¾ã€‚
        æœ€ç»ˆçš„ feature æ‹¼æ¥åº”è¯¥åœ¨ transforms ä¹‹åé€šè¿‡ Collect å˜æ¢å®Œæˆã€‚
        
        å‚æ•°ï¼š
            idx: è¦åŠ è½½çš„æ ·æœ¬ç´¢å¼•
            
        è¿”å›ï¼š
            åŒ…å«åŠ è½½æ•°æ®çš„å­—å…¸ï¼ŒåŒ…æ‹¬ï¼š
            - coord: [N, 3] åæ ‡ï¼ˆå¿…éœ€ï¼‰
            - intensity: [N,] å¼ºåº¦å€¼ï¼ˆå¦‚æœåœ¨ assets ä¸­ï¼‰
            - color: [N, 3] RGB é¢œè‰²ï¼ŒèŒƒå›´ [0, 255]ï¼ˆå¦‚æœåœ¨ assets ä¸­ï¼‰
            - echo: [N, 2] å›æ³¢ä¿¡æ¯ï¼ŒèŒƒå›´ [-1, 1]ï¼ˆå¦‚æœåœ¨ assets ä¸­ï¼‰
                - ç¬¬ 0 åˆ—ï¼šæ˜¯å¦é¦–æ¬¡å›æ³¢
                - ç¬¬ 1 åˆ—ï¼šæ˜¯å¦æœ«æ¬¡å›æ³¢
            - normal: [N, 3] æ³•å‘é‡ï¼ˆå¦‚æœåœ¨ assets ä¸­ï¼‰
            - h_norm: [N,] é«˜åº¦å½’ä¸€åŒ–å€¼ï¼ˆå¦‚æœåœ¨ assets ä¸­ï¼‰
            - class: [N,] åˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚æœåœ¨ assets ä¸­ï¼‰
            - indices: [N,] åŸå§‹ç‚¹ç´¢å¼•ï¼ˆä»…åœ¨ test/predict split ä¸­ï¼‰
        """
        sample_info = self.data_list[idx]
        
        # è·å–è·¯å¾„
        bin_path = Path(sample_info['bin_path'])
        segment_id = sample_info['segment_id']
        
        # åŠ è½½ pkl å…ƒæ•°æ®ï¼ˆä½¿ç”¨ç¼“å­˜é¿å…é‡å¤ç£ç›˜ I/Oï¼‰
        pkl_path = Path(sample_info['pkl_path'])
        pkl_key = str(pkl_path)
        
        if pkl_key not in self._metadata_cache:
            with open(pkl_path, 'rb') as f:
                self._metadata_cache[pkl_key] = pickle.load(f)
        
        metadata = self._metadata_cache[pkl_key]
        
        # æŸ¥æ‰¾ç‰‡æ®µä¿¡æ¯
        segment_info = None
        for seg in metadata['segments']:
            if seg['segment_id'] == segment_id:
                segment_info = seg
                break
        
        if segment_info is None:
            raise ValueError(f"åœ¨ {pkl_path} ä¸­æœªæ‰¾åˆ°ç‰‡æ®µ {segment_id}")
        
        # ä½¿ç”¨ memmap ä» bin æ–‡ä»¶åŠ è½½ç‚¹æ•°æ®
        point_data = np.memmap(bin_path, dtype=metadata['dtype'], mode='r')
        
        # ä½¿ç”¨ç¦»æ•£ç´¢å¼•æå–ç‰‡æ®µç‚¹
        # ç‚¹äº‘æ•°æ®å§‹ç»ˆä½¿ç”¨ç¦»æ•£ç´¢å¼•ï¼ˆéè¿ç»­ï¼‰
        if 'indices' not in segment_info:
            raise ValueError(f"ç‰‡æ®µä¿¡æ¯å¿…é¡»åŒ…å« 'indices' å­—æ®µ")
        
        indices = segment_info['indices']
        segment_points = point_data[indices]
        
        # æå–è¯·æ±‚çš„èµ„äº§
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ‹¼æ¥ featureï¼Œè€Œæ˜¯ä¿æŒå„ä¸ªç‰¹å¾ç‹¬ç«‹
        # è¿™æ · transforms.py ä¸­çš„æ•°æ®å¢å¼ºå¯ä»¥åˆ†åˆ«å¤„ç† intensityã€color ç­‰
        # æœ€åé€šè¿‡ Collect å˜æ¢æ¥æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        data = {}
        
        # æ€»æ˜¯é¦–å…ˆæå– coord
        coord = np.stack([
            segment_points['X'],
            segment_points['Y'],
            segment_points['Z']
        ], axis=1).astype(np.float32)
        data['coord'] = coord
        
        # æ ¹æ®èµ„äº§é¡ºåºæå–å…¶ä»–ç‰¹å¾ï¼ˆä¿æŒç‹¬ç«‹ï¼Œä¸æ‹¼æ¥ï¼‰
        for asset in self.assets:
            if asset == 'coord':
                continue  # å·²å¤„ç†
                
            elif asset == 'intensity':
                # æå–åŸå§‹å¼ºåº¦å€¼ï¼ˆä¿æŒåŸå§‹ä½æ•°ï¼Œä¸å½’ä¸€åŒ–ï¼‰
                # å½’ä¸€åŒ–åº”åœ¨ transforms ä¸­å®Œæˆï¼Œå¦‚ AutoNormalizeIntensity
                intensity = segment_points['intensity'].astype(np.float32)
                data['intensity'] = intensity  # [N,]
                
            elif asset == 'color' and all(c in segment_points.dtype.names for c in ['red', 'green', 'blue']):
                # æå–åŸå§‹ RGB é¢œè‰²å€¼ï¼ˆä¿æŒåŸå§‹ä½æ•°ï¼Œä¸å½’ä¸€åŒ–ï¼‰
                # å½’ä¸€åŒ–åº”åœ¨ transforms ä¸­å®Œæˆï¼Œå¦‚ AutoNormalizeColor
                color = np.stack([
                    segment_points['red'],
                    segment_points['green'],
                    segment_points['blue']
                ], axis=1).astype(np.float32)
                data['color'] = color  # [N, 3]

            elif asset == 'echo' and all(c in segment_points.dtype.names for c in ['return_number', 'number_of_returns']):
                # æå–å›æ³¢ä¿¡æ¯
                is_first = (segment_points['return_number'] == 1).astype(np.float32)
                is_last = (segment_points['return_number'] == segment_points['number_of_returns']).astype(np.float32)
                # è½¬æ¢ä¸º [-1, 1] èŒƒå›´ï¼šTrue -> 1, False -> -1
                is_first = is_first * 2.0 - 1.0
                is_last = is_last * 2.0 - 1.0
                echo = np.stack([is_first, is_last], axis=1)  # [N, 2]
                data['echo'] = echo  # [N, 2]

            elif asset == 'normal' and all(c in segment_points.dtype.names for c in ['normal_x', 'normal_y', 'normal_z']):
                # æå–æ³•å‘é‡
                normal = np.stack([
                    segment_points['normal_x'],
                    segment_points['normal_y'],
                    segment_points['normal_z']
                ], axis=1).astype(np.float32)
                data['normal'] = normal  # [N, 3]

            elif asset == 'h_norm':
                # è®¡ç®—å½’ä¸€åŒ–é«˜ç¨‹ï¼ˆåœ°ä¸Šé«˜ç¨‹ï¼‰
                # å¦‚æœ bin æ–‡ä»¶ä¸­å·²æœ‰é¢„è®¡ç®—çš„ h_normï¼Œç›´æ¥ä½¿ç”¨
                if 'h_norm' in segment_points.dtype.names:
                    h_norm = segment_points['h_norm'].astype(np.float32)
                # å¦åˆ™ï¼ŒåŸºäº is_ground å­—æ®µåŠ¨æ€è®¡ç®—
                elif 'is_ground' in segment_points.dtype.names:
                    h_norm = self._compute_h_norm(coord, segment_points['is_ground'], self.h_norm_grid)
                else:
                    raise ValueError("æ—¢æ²¡æœ‰ 'h_norm' ä¹Ÿæ²¡æœ‰ 'is_ground' å­—æ®µï¼Œæ— æ³•è®¡ç®—å½’ä¸€åŒ–é«˜ç¨‹")
                data['h_norm'] = h_norm

            elif asset == 'class':
                # å•ç‹¬å­˜å‚¨åˆ†ç±»æ ‡ç­¾ä¸ºç›®æ ‡
                classification = segment_points['classification'].astype(np.int64)
                
                # å¦‚æœæä¾›äº†ç±»åˆ«æ˜ å°„åˆ™åº”ç”¨
                if self.class_mapping is not None:
                    # ğŸ”¥ æ–°ç­–ç•¥ï¼šä¸åœ¨ class_mapping ä¸­çš„ç±»åˆ«è®¾ä¸º ignore_label
                    # è¿™äº›ç‚¹ä¼šå‚ä¸ç½‘ç»œå‰å‘ä¼ æ’­ï¼ˆä¿æŒæ•°æ®è¿ç»­æ€§ï¼‰ï¼Œ
                    # ä½†ä¸å‚ä¸æŸå¤±è®¡ç®—å’Œç²¾åº¦è¯„ä¼°ï¼ˆé€šè¿‡ ignore_index æœºåˆ¶ï¼‰
                    
                    # åˆå§‹åŒ–æ‰€æœ‰æ ‡ç­¾ä¸º ignore_label
                    mapped_classification = np.full_like(classification, self.ignore_label, dtype=np.int64)
                    
                    # åªæ˜ å°„ class_mapping ä¸­å®šä¹‰çš„ç±»åˆ«
                    for original_label, new_label in self.class_mapping.items():
                        mask = (classification == original_label)
                        mapped_classification[mask] = new_label
                    
                    data['class'] = mapped_classification
                else:
                    data['class'] = classification

        # åœ¨ test å’Œ predict åˆ’åˆ†ä¸­ï¼Œå­˜å‚¨ç‚¹ç´¢å¼•ç”¨äºæŠ•ç¥¨æœºåˆ¶
        if self.split in ['test', 'predict']:
            data['indices'] = indices.copy()  # å­˜å‚¨åŸå§‹ç‚¹ç´¢å¼•
            
            # ğŸ”¥ æ–°å¢ï¼šç›´æ¥ä¼ é€’æ–‡ä»¶ä¿¡æ¯ï¼Œé¿å…åœ¨ callback ä¸­æ¨æ–­
            # è¿™äº›ä¿¡æ¯åœ¨ tile.py ä¸­å·²ç»ä¿å­˜åˆ° segment_info ä¸­
            data['bin_file'] = sample_info.get('bin_file', Path(sample_info['bin_path']).stem)
            data['bin_path'] = sample_info['bin_path']
            data['pkl_path'] = sample_info['pkl_path']
        
        return data
    
    def get_segment_info(self, idx: int) -> Dict[str, Any]:
        """
        è·å–ç‰¹å®šç‰‡æ®µçš„å…ƒæ•°æ®
        
        å‚æ•°ï¼š
            idx: ç‰‡æ®µçš„ç´¢å¼•
            
        è¿”å›ï¼š
            åŒ…å«ç‰‡æ®µå…ƒæ•°æ®çš„å­—å…¸
        """
        if idx < 0 or idx >= len(self.data_list):
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {len(self.data_list)})")
        
        sample_info = self.data_list[idx]
        pkl_path = Path(sample_info['pkl_path'])
        
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        
        # æŸ¥æ‰¾ç‰‡æ®µä¿¡æ¯
        segment_id = sample_info['segment_id']
        for seg in metadata['segments']:
            if seg['segment_id'] == segment_id:
                return seg
        
        raise ValueError(f"æœªæ‰¾åˆ°ç‰‡æ®µ {segment_id}")
    
    def get_file_metadata(self, idx: int) -> Dict[str, Any]:
        """
        è·å–åŒ…å«ç‰¹å®šç‰‡æ®µçš„æ–‡ä»¶çš„å…ƒæ•°æ®
        
        å‚æ•°ï¼š
            idx: ç‰‡æ®µçš„ç´¢å¼•
            
        è¿”å›ï¼š
            åŒ…å«æ–‡ä»¶çº§å…ƒæ•°æ®çš„å­—å…¸
        """
        if idx < 0 or idx >= len(self.data_list):
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {len(self.data_list)})")
        
        sample_info = self.data_list[idx]
        pkl_path = Path(sample_info['pkl_path'])
        
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        
        # è¿”å›å…ƒæ•°æ®ï¼Œæ’é™¤ç‰‡æ®µåˆ—è¡¨ï¼ˆå¯èƒ½å¾ˆå¤§ï¼‰
        file_metadata = {k: v for k, v in metadata.items() if k != 'segments'}
        return file_metadata
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        
        è¿”å›ï¼š
            åŒ…å«æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if len(self.data_list) == 0:
            return {}
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        num_points_list = [s['num_points'] for s in self.data_list]
        
        stats = {
            'num_samples': len(self.data_list),
            'num_points': {
                'total': sum(num_points_list),
                'mean': np.mean(num_points_list),
                'median': np.median(num_points_list),
                'min': np.min(num_points_list),
                'max': np.max(num_points_list),
                'std': np.std(num_points_list),
            }
        }
        
        # ä»ç¬¬ä¸€ä¸ªæ–‡ä»¶è·å–æ ‡ç­¾åˆ†å¸ƒ
        if len(self.data_list) > 0:
            pkl_path = Path(self.data_list[0]['pkl_path'])
            with open(pkl_path, 'rb') as f:
                metadata = pickle.load(f)
            
            if 'label_counts' in metadata:
                stats['label_distribution'] = metadata['label_counts']
        
        return stats
    
    def print_stats(self):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        
        print("="*70)
        print("æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
        print("="*70)
        print(f"åˆ’åˆ†: {self.split}")
        print(f"æ ·æœ¬æ•°: {stats['num_samples']:,}")
        print(f"\næ¯æ ·æœ¬ç‚¹æ•°:")
        print(f"  - æ€»è®¡: {stats['num_points']['total']:,}")
        print(f"  - å¹³å‡: {stats['num_points']['mean']:,.1f}")
        print(f"  - ä¸­ä½æ•°: {stats['num_points']['median']:,.0f}")
        print(f"  - æœ€å°: {stats['num_points']['min']:,}")
        print(f"  - æœ€å¤§: {stats['num_points']['max']:,}")
        print(f"  - æ ‡å‡†å·®: {stats['num_points']['std']:,.1f}")
        
        if 'label_distribution' in stats:
            print(f"\næ ‡ç­¾åˆ†å¸ƒï¼ˆæ•´ä½“ï¼‰:")
            for label, count in sorted(stats['label_distribution'].items()):
                print(f"  ç±»åˆ« {label}: {count:,}")
        
        print("="*70)
    
    def get_class_distribution(self) -> Optional[Dict[int, int]]:
        """
        è·å–æ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ
        
        è¿”å›ï¼š
            ç±»åˆ«åˆ†å¸ƒå­—å…¸ {class_id: count}
        """
        if len(self.data_list) == 0:
            return {}
        
        # ä»ç¬¬ä¸€ä¸ª pkl æ–‡ä»¶è·å–æ•´ä½“ç±»åˆ«åˆ†å¸ƒ
        pkl_path = Path(self.data_list[0]['pkl_path'])
        with open(pkl_path, 'rb') as f:
            metadata = pickle.load(f)
        
        if 'label_counts' in metadata:
            # å¦‚æœæœ‰ class_mappingï¼Œè½¬æ¢ç±»åˆ«æ ‡ç­¾
            if self.class_mapping is not None:
                mapped_counts = {}
                for original_label, count in metadata['label_counts'].items():
                    if original_label in self.class_mapping:
                        new_label = self.class_mapping[original_label]
                        mapped_counts[new_label] = mapped_counts.get(new_label, 0) + count
                return mapped_counts
            else:
                return dict(metadata['label_counts'])
        
        return {}
    
    def get_sample_weights(self, class_weights: Optional[Dict[int, float]] = None) -> Optional[np.ndarray]:
        """
        è®¡ç®—æ¯ä¸ªæ ·æœ¬ï¼ˆsegmentï¼‰çš„æƒé‡
        
        æƒé‡è®¡ç®—ç­–ç•¥ï¼š
        - æ ·æœ¬æƒé‡ = Î£(æ ·æœ¬ä¸­åŒ…å«çš„æ¯ä¸ªç±»åˆ«çš„ç±»åˆ«æƒé‡)
        - åŒ…å«ç¨€æœ‰ç±»åˆ«çš„æ ·æœ¬è·å¾—æ›´é«˜æƒé‡
        - åŒ…å«å¤šä¸ªä¸åŒç±»åˆ«çš„æ ·æœ¬è·å¾—æ›´é«˜æƒé‡
        
        å‚æ•°ï¼š
            class_weights: ç±»åˆ«æƒé‡å­—å…¸ {class_id: weight}
        
        è¿”å›ï¼š
            æ ·æœ¬æƒé‡æ•°ç»„ [num_samples]
        """
        if class_weights is None or len(self.data_list) == 0:
            return None
        
        # åŠ è½½ pkl å…ƒæ•°æ®è·å–æ¯ä¸ª segment çš„ç±»åˆ«ä¿¡æ¯
        sample_weights = []
        
        # æŒ‰ pkl æ–‡ä»¶åˆ†ç»„å¤„ç†ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        pkl_to_samples = {}
        for idx, sample_info in enumerate(self.data_list):
            pkl_path = sample_info['pkl_path']
            if pkl_path not in pkl_to_samples:
                pkl_to_samples[pkl_path] = []
            pkl_to_samples[pkl_path].append((idx, sample_info['segment_id']))
        
        # ä¸ºæ¯ä¸ª pkl æ–‡ä»¶è®¡ç®—å…¶ segments çš„æƒé‡
        weights_dict = {}
        for pkl_path, samples in pkl_to_samples.items():
            with open(pkl_path, 'rb') as f:
                metadata = pickle.load(f)
            
            # ä¸ºæ¯ä¸ª segment è®¡ç®—æƒé‡
            for idx, segment_id in samples:
                segment_info = None
                for seg in metadata['segments']:
                    if seg['segment_id'] == segment_id:
                        segment_info = seg
                        break
                
                if segment_info is None or 'unique_labels' not in segment_info:
                    # å¦‚æœæ²¡æœ‰ç±»åˆ«ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æƒé‡ 1.0
                    weights_dict[idx] = 1.0
                    continue
                
                # è®¡ç®—æƒé‡ï¼šåŒ…å«çš„æ‰€æœ‰ç±»åˆ«çš„ç±»åˆ«æƒé‡ä¹‹å’Œ
                unique_labels = segment_info['unique_labels']
                segment_weight = 0.0
                
                for label in unique_labels:
                    # å¦‚æœæœ‰ class_mappingï¼Œå…ˆæ˜ å°„æ ‡ç­¾
                    if self.class_mapping is not None:
                        if label in self.class_mapping:
                            mapped_label = self.class_mapping[label]
                            segment_weight += class_weights.get(mapped_label, 0.0)
                    else:
                        segment_weight += class_weights.get(label, 0.0)
                
                weights_dict[idx] = max(segment_weight, 1e-6)  # é¿å…é›¶æƒé‡
        
        # æŒ‰é¡ºåºæ„å»ºæƒé‡æ•°ç»„
        sample_weights = np.array([weights_dict.get(i, 1.0) for i in range(len(self.data_list))], dtype=np.float32)
        
        return sample_weights


def create_dataset(
    data_root,
    split='train',
    assets=None,
    transform=None,
    ignore_label=-1,
    loop=1,
    cache_data=False,
    **kwargs
):
    """
    åˆ›å»º BinPklDataset çš„å·¥å‚å‡½æ•°
    
    å‚æ•°ï¼š
        data_root: æ ¹ç›®å½•ã€å•ä¸ª pkl æ–‡ä»¶æˆ– pkl æ–‡ä»¶åˆ—è¡¨
        split: æ•°æ®é›†åˆ’åˆ†ï¼ˆ'train'ã€'val'ã€'test'ã€'predict'ï¼‰
        assets: è¦åŠ è½½çš„æ•°æ®å±æ€§åˆ—è¡¨
        transform: æ•°æ®å˜æ¢
        ignore_label: è¦å¿½ç•¥çš„æ ‡ç­¾
        loop: æ•°æ®é›†å¾ªç¯å› å­
        cache_data: æ˜¯å¦ç¼“å­˜æ•°æ®
        **kwargs: å…¶ä»–å‚æ•°
        
    è¿”å›ï¼š
        BinPklDataset å®ä¾‹
    """
    return BinPklDataset(
        data_root=data_root,
        split=split,
        assets=assets,
        transform=transform,
        ignore_label=ignore_label,
        loop=loop,
        cache_data=cache_data,
        **kwargs
    )
