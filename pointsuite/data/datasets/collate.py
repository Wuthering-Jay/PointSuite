"""
è‡ªå®šä¹‰çš„ DataLoader Collate å‡½æ•°

åŒ…å«:
1. åŸºç¡€çš„ collate_fnï¼šæ‹¼æ¥ç‚¹äº‘æ•°æ®
2. å¸¦ç‚¹æ•°é™åˆ¶çš„ collate_fnï¼šåŠ¨æ€è°ƒæ•´ batch å¤§å°
"""
import numpy as np
import torch
from collections.abc import Mapping, Sequence


def collate_fn(batch):
    """
    åŸºç¡€ collate function for point cloud which support dict and list
    
    è¯¥å‡½æ•°å°†å¤šä¸ªä¸åŒç‚¹æ•°çš„æ ·æœ¬åˆå¹¶æˆä¸€ä¸ªbatchï¼š
    - ç‚¹äº‘æ•°æ®ï¼ˆcoord, feat ç­‰ï¼‰ä¼šè¢«æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„ç‚¹äº‘
    - åˆ†ç±»æ ‡ç­¾ï¼ˆclassï¼‰ä¿æŒä¸ºåˆ—è¡¨ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸€ä¸ª tensorï¼‰
    - è‡ªåŠ¨æ·»åŠ  'offset' å­—æ®µï¼Œæ ¼å¼ä¸º [n1, n1+n2, ...]ï¼ˆä¸åŒ…å«èµ·å§‹0ï¼‰ï¼Œé•¿åº¦ä¸º batch_size
    - testæ¨¡å¼ä¸‹ä¼šæ‹¼æ¥ indices ç”¨äºæŠ•ç¥¨æœºåˆ¶
    
    ç»Ÿä¸€çš„è¾“å‡ºæ ¼å¼ï¼š
    - coord: [total_points, 3] - æ‹¼æ¥æ‰€æœ‰ç‚¹
    - feat: [total_points, C] - æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    - class: [total_points] - æ‹¼æ¥æ‰€æœ‰ç‚¹çš„æ ‡ç­¾ï¼ˆç‚¹çº§æ ‡ç­¾ï¼‰
    - offset: [batch_size] - ç´¯ç§¯åç§»ï¼ˆä¸åŒ…å«èµ·å§‹0ï¼‰ï¼Œæ ¼å¼ä¸º [n1, n1+n2, ...]
    
    æ³¨æ„ï¼šå¦‚æœéœ€è¦æ ·æœ¬çº§æ ‡ç­¾ï¼ˆå¦‚æ•´ä¸ªåœºæ™¯çš„åˆ†ç±»ï¼‰ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„é”®åï¼ˆå¦‚ 'scene_label'ï¼‰
    """
    if not isinstance(batch, Sequence):
        raise TypeError(f"{type(batch)} is not supported.")

    # å¤„ç† dict ç±»å‹ï¼ˆæˆ‘ä»¬çš„æ•°æ®é›†è¿”å›çš„æ˜¯ dictï¼‰
    if isinstance(batch[0], Mapping):
        # è·å–æ‰€æœ‰keys
        keys = batch[0].keys()
        
        # åˆå¹¶åçš„ç»“æœ
        result = {}
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°ï¼ˆç”¨äºoffsetï¼‰
        num_points_per_sample = []
        
        # éœ€è¦æ‹¼æ¥çš„å­—æ®µï¼ˆç‚¹çº§æ•°æ®ï¼‰
        concat_keys = ['coord', 'feat', 'feature', 'indices', 'normal', 'class', 'label', 'classification', 
                       'echo', 'intensity', 'color', 'h_norm']
        
        # éœ€è¦ä¿æŒä¸ºåˆ—è¡¨çš„å­—æ®µï¼ˆæ ·æœ¬çº§æ•°æ®ï¼‰
        # æ³¨æ„ï¼šå¦‚æœæœ‰çœŸæ­£çš„æ ·æœ¬çº§æ ‡ç­¾ï¼ˆå¦‚æ•´ä¸ªåœºæ™¯çš„åˆ†ç±»ï¼‰ï¼Œåº”è¯¥ä½¿ç”¨ä¸åŒçš„é”®åï¼ˆå¦‚ 'scene_label'ï¼‰
        list_keys = []
        
        for key in keys:
            # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„è¯¥å­—æ®µ
            values = [torch.from_numpy(d[key]) if isinstance(d[key], np.ndarray) else d[key] for d in batch]
            
            # è·³è¿‡ offsetï¼ˆå¦‚æœæ ·æœ¬ä¸­å·²æœ‰ï¼Œä¼šè¢«è¦†ç›–ï¼‰
            if key == 'offset':
                continue
            
            # æ‹¼æ¥ç‚¹çº§æ•°æ®
            if key in concat_keys:
                result[key] = torch.cat(values, dim=0)
                
                # è®°å½•ç‚¹æ•°ï¼ˆä» coord ä¼˜å…ˆï¼Œå¦åˆ™ä»ä»»ä½•æ‹¼æ¥å­—æ®µè·å–ï¼‰
                if len(num_points_per_sample) == 0:
                    num_points_per_sample = [v.shape[0] for v in values]
            
            # ä¿æŒæ ·æœ¬çº§æ•°æ®ä¸ºåˆ—è¡¨
            elif key in list_keys:
                result[key] = values  # ä¿æŒä¸ºåˆ—è¡¨
            
            # å…¶ä»–å­—æ®µå°è¯• stackï¼Œå¤±è´¥åˆ™ä¿æŒä¸ºåˆ—è¡¨
            else:
                try:
                    result[key] = torch.stack(values, dim=0)
                except:
                    result[key] = values
        
        # æ·»åŠ  offset å­—æ®µï¼ˆæ ¼å¼ï¼š[n1, n1+n2, ...]ï¼Œé•¿åº¦ä¸º batch_sizeï¼‰
        if len(num_points_per_sample) > 0:
            # ç”Ÿæˆç´¯ç§¯å’Œï¼Œä½†ä¸åŒ…å«èµ·å§‹ 0ï¼šoffset[i] è¡¨ç¤ºå‰ i+1 ä¸ªæ ·æœ¬çš„ç´¯è®¡ç‚¹æ•°
            offset = torch.cumsum(torch.tensor(num_points_per_sample), dim=0).int()
            result['offset'] = offset  # ä¸åŒ…å«èµ·å§‹ 0ï¼Œé•¿åº¦ä¸º batch_size
        
        return result
    
    # å¤„ç†å…¶ä»–ç±»å‹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    elif isinstance(batch[0], torch.Tensor):
        return torch.cat(list(batch))
    elif isinstance(batch[0], str):
        return list(batch)
    elif isinstance(batch[0], Sequence):
        for data in batch:
            # data[0] æ˜¯ç‚¹çº§å­—æ®µï¼ˆå¦‚ coordï¼‰ï¼Œappend æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°
            data.append(torch.tensor([data[0].shape[0]]))
        batch = [collate_fn(samples) for samples in zip(*batch)]
        # æ­¤å¤„ batch[-1] åŒ…å«æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°ï¼Œç›´æ¥å–ç´¯ç§¯å’Œï¼ˆä¸æ·»åŠ èµ·å§‹0ï¼‰
        batch[-1] = torch.cumsum(batch[-1], dim=0).int()
        return batch
    else:
        from torch.utils.data.dataloader import default_collate
        return default_collate(batch)


class DynamicBatchSampler:
    """
    åŠ¨æ€ Batch Samplerï¼Œæ ¹æ®ç‚¹æ•°åŠ¨æ€è°ƒæ•´ batch å¤§å°
    
    è¿™æ˜¯ä¸€ä¸ªæ›´ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨é‡‡æ ·é˜¶æ®µå°±æ§åˆ¶ batch å¤§å°ï¼Œ
    è€Œä¸æ˜¯åœ¨ collate é˜¶æ®µä¸¢å¼ƒæ ·æœ¬ã€‚
    
    å·¥ä½œæ¨¡å¼ï¼ˆç”± shuffle å’Œ sampler ç»„åˆæ§åˆ¶ï¼‰ï¼š
    
    1. è®­ç»ƒæ¨¡å¼ (shuffle=True æˆ– sampler!=None)ï¼š
       - ä½¿ç”¨ä¿å®ˆä¼°è®¡çš„ batch æ•°ï¼ˆç†æƒ³å€¼ Ã— 0.95ï¼‰
       - __iter__ æœ€å¤šäº§å‡º __len__ ä¸ª batch
       - ç¡®ä¿ PyTorch Lightning ä¸ä¼šå› ä¸ºç­‰å¾…æ›´å¤š batch è€Œè·³è¿‡éªŒè¯
       - ä¼šä¸¢å¼ƒçº¦ 5% çš„ batchï¼ˆä½†ç”±äº shuffleï¼Œæ ·æœ¬è¦†ç›–ä»ç„¶æ˜¯å‡åŒ€çš„ï¼‰
    
    2. æ¨ç†æ¨¡å¼ (shuffle=False ä¸” sampler=None)ï¼š
       - ç²¾ç¡®è®¡ç®— batch æ•°
       - __iter__ äº§å‡ºæ‰€æœ‰ batch
       - ç¡®ä¿ 100% æ ·æœ¬è¢«è®¿é—®ï¼ˆtest/predict å¿…éœ€ï¼‰
    
    æ³¨æ„ï¼šä¸å†æ”¯æŒ drop_last å‚æ•°ï¼Œè¡Œä¸ºç”± shuffle/sampler ç»„åˆè‡ªåŠ¨å†³å®š
    
    ä½¿ç”¨æ–¹æ³•:
        # è®­ç»ƒ - ä½¿ç”¨ shuffle
        sampler = DynamicBatchSampler(dataset, max_points=500000, shuffle=True)
        
        # è®­ç»ƒ - ä½¿ç”¨åŠ æƒé‡‡æ ·ï¼ˆshuffle å¯ä»¥ä¸º Falseï¼Œå› ä¸º sampler æœ¬èº«å·²éšæœºï¼‰
        base_sampler = WeightedRandomSampler(weights, num_samples=len(dataset))
        sampler = DynamicBatchSampler(dataset, max_points=500000, sampler=base_sampler)
        
        # æµ‹è¯•/é¢„æµ‹ - å¿…é¡»ç²¾ç¡®è¦†ç›–æ‰€æœ‰æ ·æœ¬
        sampler = DynamicBatchSampler(dataset, max_points=500000, shuffle=False)
    """
    
    def __init__(self, dataset, max_points=500000, shuffle=True, sampler=None, drop_last=False):
        """
        Args:
            dataset: æ•°æ®é›†å¯¹è±¡ï¼Œéœ€è¦èƒ½å¤Ÿè·å–æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°
            max_points: æ¯ä¸ª batch çš„æœ€å¤§ç‚¹æ•°
            shuffle: æ˜¯å¦æ‰“ä¹±é¡ºåºï¼ˆå½“ sampler=None æ—¶ç”Ÿæ•ˆï¼‰
                    - True: è®­ç»ƒæ¨¡å¼ï¼Œä¿å®ˆä¼°è®¡ batch æ•°
                    - False: æµ‹è¯•/é¢„æµ‹æ¨¡å¼ï¼Œç²¾ç¡®è®¡ç®— batch æ•°
            sampler: å¯é€‰çš„åŸºç¡€ Samplerï¼ˆå¦‚ WeightedRandomSamplerï¼‰
                    å¦‚æœæä¾›ï¼Œåˆ™ä½¿ç”¨è¯¥ sampler ç”Ÿæˆç´¢å¼•åºåˆ—ï¼Œshuffle å‚æ•°å°†è¢«å¿½ç•¥
            drop_last: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»…ä¸ºå…¼å®¹æ€§ï¼Œå®é™…ä¸ç”Ÿæ•ˆ
        """
        self.dataset = dataset
        self.max_points = max_points
        self.shuffle = shuffle
        self.sampler = sampler
        
        # drop_last ä¸å†ä½¿ç”¨ï¼Œä½†ä¿ç•™å‚æ•°ä»¥å…¼å®¹æ—§ä»£ç 
        if drop_last:
            print("Warning: drop_last å‚æ•°åœ¨ DynamicBatchSampler ä¸­å·²åºŸå¼ƒï¼Œå°†è¢«å¿½ç•¥")
        
        # é¢„å…ˆè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°
        self.num_points_list = self._get_num_points_list()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºä¼°ç®—ï¼‰
        self._total_points = sum(self.num_points_list)
        self._num_samples = len(self.num_points_list)
        self._avg_points = self._total_points / self._num_samples if self._num_samples > 0 else 0
        
        # ğŸ”¥ é¢„å…ˆè®¡ç®— batch æ•°
        self._cached_len = self._compute_batch_count()
        
    def _get_num_points_list(self):
        """è·å–æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°ï¼ˆè€ƒè™‘ loop å‚æ•°å’Œé‡‡æ ·æ¨¡å¼ï¼‰"""
        
        # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ get_sample_num_points() æ–¹æ³•ï¼ˆæ”¯æŒä½“ç´ æ¨¡å¼çš„æ­£ç¡®ç‚¹æ•°ï¼‰
        if hasattr(self.dataset, 'get_sample_num_points'):
            num_points_list = self.dataset.get_sample_num_points()
            # å¦‚æœ dataset æœ‰ loop å‚æ•°ï¼Œéœ€è¦æ‰©å±•åˆ—è¡¨
            if hasattr(self.dataset, 'loop') and self.dataset.loop > 1:
                num_points_list = num_points_list * self.dataset.loop
            return num_points_list
        
        base_num_points_list = []
        
        # å°è¯•ä» dataset.data_list è·å–
        if hasattr(self.dataset, 'data_list'):
            for sample_info in self.dataset.data_list:
                if 'num_points' in sample_info:
                    base_num_points_list.append(sample_info['num_points'])
                else:
                    # å¦‚æœæ²¡æœ‰ num_pointsï¼ŒåŠ è½½æ ·æœ¬ç»Ÿè®¡
                    sample = self.dataset[len(base_num_points_list)]
                    if 'coord' in sample:
                        base_num_points_list.append(len(sample['coord']))
                    else:
                        base_num_points_list.append(0)
            
            # å¦‚æœ dataset æœ‰ loop å‚æ•°ï¼Œéœ€è¦æ‰©å±•åˆ—è¡¨
            if hasattr(self.dataset, 'loop') and self.dataset.loop > 1:
                num_points_list = base_num_points_list * self.dataset.loop
            else:
                num_points_list = base_num_points_list
        else:
            # éå†æ•´ä¸ªæ•°æ®é›†è·å–ç‚¹æ•°ï¼ˆè¾ƒæ…¢ï¼‰
            print("Warning: Dataset doesn't have data_list, scanning all samples...")
            num_points_list = []
            for i in range(len(self.dataset)):
                sample = self.dataset[i]
                if 'coord' in sample:
                    num_points_list.append(len(sample['coord']))
                else:
                    num_points_list.append(0)
        
        return num_points_list
    
    def __iter__(self):
        """
        ç”Ÿæˆ batch ç´¢å¼•
        
        - shuffle=False: ç”Ÿæˆæ‰€æœ‰ batchï¼ˆç²¾ç¡®ï¼‰
        - shuffle=True: ç”Ÿæˆ batch ç›´åˆ°è¾¾åˆ°é¢„ä¼°æ•°é‡ï¼ˆä¿å®ˆï¼‰
        """
        # ç”Ÿæˆç´¢å¼•åˆ—è¡¨
        if self.sampler is not None:
            indices = list(self.sampler)
        elif self.shuffle:
            indices = torch.randperm(len(self.dataset)).tolist()
        else:
            indices = list(range(len(self.dataset)))
        
        # åŠ¨æ€ç”Ÿæˆ batch
        batch = []
        batch_points = 0
        batch_count = 0
        
        for idx in indices:
            num_points = self.num_points_list[idx]
            
            # å¦‚æœå½“å‰ batch ä¸ºç©ºï¼Œæˆ–è€…åŠ å…¥å½“å‰æ ·æœ¬ä¸ä¼šè¶…è¿‡é™åˆ¶
            if len(batch) == 0 or batch_points + num_points <= self.max_points:
                batch.append(idx)
                batch_points += num_points
            else:
                # å½“å‰ batch å·²æ»¡ï¼Œyield å¹¶å¼€å§‹æ–° batch
                yield batch
                batch_count += 1
                
                # ğŸ”¥ shuffle æ¨¡å¼ä¸‹ï¼Œè¾¾åˆ°é¢„ä¼° batch æ•°ååœæ­¢
                # è¿™ç¡®ä¿ __len__ è¿”å›å€¼ == å®é™… yield çš„ batch æ•°
                if (self.shuffle or self.sampler is not None) and batch_count >= self._cached_len:
                    return
                
                batch = [idx]
                batch_points = num_points
        
        # å¤„ç†æœ€åä¸€ä¸ª batch
        if len(batch) > 0:
            # shuffle=False æ—¶æ€»æ˜¯è¾“å‡ºæœ€åä¸€ä¸ª batch
            # shuffle=True æ—¶åªæœ‰æœªè¾¾åˆ°é¢„ä¼°æ•°é‡æ‰è¾“å‡º
            if not self.shuffle and self.sampler is None:
                yield batch
            elif batch_count < self._cached_len:
                yield batch
    
    def _compute_batch_count(self) -> int:
        """
        è®¡ç®— batch æ•°é‡
        
        - shuffle=False: ç²¾ç¡®è®¡ç®—ï¼ˆæ¨¡æ‹Ÿé¡ºåºéå†ï¼‰
        - shuffle=True: ä¿å®ˆä¼°è®¡ï¼ˆç¡®ä¿ <= å®é™… batch æ•°ï¼‰
        
        ğŸ”¥ å…³é”®è®¾è®¡ï¼š
        å¯¹äº shuffle=Trueï¼Œæˆ‘ä»¬ä½¿ç”¨ 95% ç½®ä¿¡åŒºé—´çš„ä¸‹ç•Œä½œä¸ºä¼°è®¡å€¼ã€‚
        è¿™æ ·å¯ä»¥ç¡®ä¿é¢„ä¼°å€¼å‡ ä¹æ€»æ˜¯ <= å®é™…å€¼ï¼Œé¿å… Lightning è·³è¿‡éªŒè¯ã€‚
        åŒæ—¶ä¸ä¼šè¿‡äºä¿å®ˆï¼Œé¿å…ä¸¢å¼ƒå¤ªå¤š batchã€‚
        """
        if self._num_samples == 0:
            return 1
        
        # å¯¹äº shuffle=True æˆ–æœ‰ sampler çš„æƒ…å†µï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
        if self.shuffle or self.sampler is not None:
            # åŸºäºå¹³å‡ç‚¹æ•°ä¼°ç®—
            # æ¯ä¸ª batch å¹³å‡èƒ½æ”¾çš„æ ·æœ¬æ•°
            avg_samples_per_batch = self.max_points / self._avg_points if self._avg_points > 0 else 1
            
            # ç†æƒ³æƒ…å†µä¸‹çš„ batch æ•°
            ideal_batches = self._num_samples / avg_samples_per_batch
            
            # ğŸ”¥ ä½¿ç”¨ 0.95 çš„ç¼©æ”¾å› å­ä½œä¸ºä¿å®ˆä¼°è®¡
            # è¿™æ„å‘³ç€æˆ‘ä»¬é¢„ä¼°çš„ batch æ•°æ˜¯ç†æƒ³å€¼çš„ 95%
            # å®é™…ä¸Šç”±äºæ ·æœ¬å¤§å°çš„æ–¹å·®ï¼ŒçœŸå® batch æ•°é€šå¸¸æ›´æ¥è¿‘ç†æƒ³å€¼
            conservative_factor = 0.95
            estimated_batches = int(ideal_batches * conservative_factor)
            
            return max(1, estimated_batches)
        
        # å¯¹äºé¡ºåºéå†ï¼ˆshuffle=False, sampler=Noneï¼‰ï¼Œç²¾ç¡®è®¡ç®—
        batch_count = 0
        batch_points = 0
        
        for num_points in self.num_points_list:
            if batch_points == 0 or batch_points + num_points <= self.max_points:
                batch_points += num_points
            else:
                batch_count += 1
                batch_points = num_points
        
        # æœ€åä¸€ä¸ª batchï¼ˆé¡ºåºæ¨¡å¼ä¸‹æ€»æ˜¯åŒ…å«ï¼‰
        if batch_points > 0:
            batch_count += 1
        
        return max(1, batch_count)
    
    def __len__(self):
        """è¿”å› batch æ•°é‡"""
        return self._cached_len

