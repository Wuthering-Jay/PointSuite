"""
è‡ªå®šä¹‰çš„ DataLoader Collate å‡½æ•°

åŒ…å«:
1. åŸºç¡€çš„ collate_fnï¼šæ‹¼æ¥ç‚¹äº‘æ•°æ®
2. å¸¦ç‚¹æ•°é™åˆ¶çš„ collate_fnï¼šåŠ¨æ€è°ƒæ•´ batch å¤§å°
"""
import numpy as np
import torch
from collections.abc import Mapping, Sequence


def collate_fn(batch):
    """
    åŸºç¡€ collate function for point cloud which support dict and list
    
    è¯¥å‡½æ•°å°†å¤šä¸ªä¸åŒç‚¹æ•°çš„æ ·æœ¬åˆå¹¶æˆä¸€ä¸ªbatchï¼š
    - ç‚¹äº‘æ•°æ®ï¼ˆcoord, feat ç­‰ï¼‰ä¼šè¢«æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„ç‚¹äº‘
    - åˆ†ç±»æ ‡ç­¾ï¼ˆclassï¼‰ä¿æŒä¸ºåˆ—è¡¨ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸€ä¸ª tensorï¼‰
    - è‡ªåŠ¨æ·»åŠ  'offset' å­—æ®µï¼Œæ ¼å¼ä¸º [n1, n1+n2, ...]ï¼ˆä¸åŒ…å«èµ·å§‹0ï¼‰ï¼Œé•¿åº¦ä¸º batch_size
    - testæ¨¡å¼ä¸‹ä¼šæ‹¼æ¥ indices ç”¨äºæŠ•ç¥¨æœºåˆ¶
    
    ç»Ÿä¸€çš„è¾“å‡ºæ ¼å¼ï¼š
    - coord: [total_points, 3] - æ‹¼æ¥æ‰€æœ‰ç‚¹
    - feat: [total_points, C] - æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    - class: [total_points] - æ‹¼æ¥æ‰€æœ‰ç‚¹çš„æ ‡ç­¾ï¼ˆç‚¹çº§æ ‡ç­¾ï¼‰
    - offset: [batch_size] - ç´¯ç§¯åç§»ï¼ˆä¸åŒ…å«èµ·å§‹0ï¼‰ï¼Œæ ¼å¼ä¸º [n1, n1+n2, ...]
    
    æ³¨æ„ï¼šå¦‚æœéœ€è¦æ ·æœ¬çº§æ ‡ç­¾ï¼ˆå¦‚æ•´ä¸ªåœºæ™¯çš„åˆ†ç±»ï¼‰ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„é”®åï¼ˆå¦‚ 'scene_label'ï¼‰
    """
    if not isinstance(batch, Sequence):
        raise TypeError(f"{type(batch)} is not supported.")

    # å¤„ç† dict ç±»å‹ï¼ˆæˆ‘ä»¬çš„æ•°æ®é›†è¿”å›çš„æ˜¯ dictï¼‰
    if isinstance(batch[0], Mapping):
        # è·å–æ‰€æœ‰keys
        keys = batch[0].keys()
        
        # åˆå¹¶åçš„ç»“æœ
        result = {}
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°ï¼ˆç”¨äºoffsetï¼‰
        num_points_per_sample = []
        
        # éœ€è¦æ‹¼æ¥çš„å­—æ®µï¼ˆç‚¹çº§æ•°æ®ï¼‰
        concat_keys = ['coord', 'feat', 'feature', 'indices', 'normal', 'class', 'label', 'classification', 
                       'echo', 'intensity', 'color', 'h_norm']
        
        # éœ€è¦ä¿æŒä¸ºåˆ—è¡¨çš„å­—æ®µï¼ˆæ ·æœ¬çº§æ•°æ®ï¼‰
        # æ³¨æ„ï¼šå¦‚æœæœ‰çœŸæ­£çš„æ ·æœ¬çº§æ ‡ç­¾ï¼ˆå¦‚æ•´ä¸ªåœºæ™¯çš„åˆ†ç±»ï¼‰ï¼Œåº”è¯¥ä½¿ç”¨ä¸åŒçš„é”®åï¼ˆå¦‚ 'scene_label'ï¼‰
        list_keys = []
        
        for key in keys:
            # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„è¯¥å­—æ®µ
            values = [torch.from_numpy(d[key]) if isinstance(d[key], np.ndarray) else d[key] for d in batch]
            
            # è·³è¿‡ offsetï¼ˆå¦‚æœæ ·æœ¬ä¸­å·²æœ‰ï¼Œä¼šè¢«è¦†ç›–ï¼‰
            if key == 'offset':
                continue
            
            # æ‹¼æ¥ç‚¹çº§æ•°æ®
            if key in concat_keys:
                result[key] = torch.cat(values, dim=0)
                
                # è®°å½•ç‚¹æ•°ï¼ˆä» coord ä¼˜å…ˆï¼Œå¦åˆ™ä»ä»»ä½•æ‹¼æ¥å­—æ®µè·å–ï¼‰
                if len(num_points_per_sample) == 0:
                    num_points_per_sample = [v.shape[0] for v in values]
            
            # ä¿æŒæ ·æœ¬çº§æ•°æ®ä¸ºåˆ—è¡¨
            elif key in list_keys:
                result[key] = values  # ä¿æŒä¸ºåˆ—è¡¨
            
            # å…¶ä»–å­—æ®µå°è¯• stackï¼Œå¤±è´¥åˆ™ä¿æŒä¸ºåˆ—è¡¨
            else:
                try:
                    result[key] = torch.stack(values, dim=0)
                except:
                    result[key] = values
        
        # æ·»åŠ  offset å­—æ®µï¼ˆæ ¼å¼ï¼š[n1, n1+n2, ...]ï¼Œé•¿åº¦ä¸º batch_sizeï¼‰
        if len(num_points_per_sample) > 0:
            # ç”Ÿæˆç´¯ç§¯å’Œï¼Œä½†ä¸åŒ…å«èµ·å§‹ 0ï¼šoffset[i] è¡¨ç¤ºå‰ i+1 ä¸ªæ ·æœ¬çš„ç´¯è®¡ç‚¹æ•°
            offset = torch.cumsum(torch.tensor(num_points_per_sample), dim=0).int()
            result['offset'] = offset  # ä¸åŒ…å«èµ·å§‹ 0ï¼Œé•¿åº¦ä¸º batch_size
        
        return result
    
    # å¤„ç†å…¶ä»–ç±»å‹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    elif isinstance(batch[0], torch.Tensor):
        return torch.cat(list(batch))
    elif isinstance(batch[0], str):
        return list(batch)
    elif isinstance(batch[0], Sequence):
        for data in batch:
            # data[0] æ˜¯ç‚¹çº§å­—æ®µï¼ˆå¦‚ coordï¼‰ï¼Œappend æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°
            data.append(torch.tensor([data[0].shape[0]]))
        batch = [collate_fn(samples) for samples in zip(*batch)]
        # æ­¤å¤„ batch[-1] åŒ…å«æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°ï¼Œç›´æ¥å–ç´¯ç§¯å’Œï¼ˆä¸æ·»åŠ èµ·å§‹0ï¼‰
        batch[-1] = torch.cumsum(batch[-1], dim=0).int()
        return batch
    else:
        from torch.utils.data.dataloader import default_collate
        return default_collate(batch)


class DynamicBatchSampler:
    """
    åŠ¨æ€ Batch Samplerï¼Œæ ¹æ®ç‚¹æ•°åŠ¨æ€è°ƒæ•´ batch å¤§å°
    
    è¿™æ˜¯ä¸€ä¸ªæ›´ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨é‡‡æ ·é˜¶æ®µå°±æ§åˆ¶ batch å¤§å°ï¼Œ
    è€Œä¸æ˜¯åœ¨ collate é˜¶æ®µä¸¢å¼ƒæ ·æœ¬ã€‚
    
    ç‰¹æ€§ï¼š
    - ç¡®ä¿è¦†ç›–æ‰€æœ‰æ ·æœ¬ï¼ˆæ¯ä¸ª epochï¼‰
    - æ”¯æŒä¸ WeightedRandomSampler ç­‰å…¶ä»– Sampler ç»“åˆä½¿ç”¨
    - åŠ¨æ€è°ƒæ•´ batch å¤§å°ä»¥æ»¡è¶³ç‚¹æ•°é™åˆ¶
    
    ä½¿ç”¨æ–¹æ³•:
        # åŸºç¡€ç”¨æ³•
        sampler = DynamicBatchSampler(dataset, max_points=500000, shuffle=True)
        dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn)
        
        # ä¸ WeightedRandomSampler ç»“åˆ
        from torch.utils.data import WeightedRandomSampler
        base_sampler = WeightedRandomSampler(weights, num_samples=len(dataset))
        sampler = DynamicBatchSampler(dataset, max_points=500000, sampler=base_sampler)
        dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn)
    """
    
    def __init__(self, dataset, max_points=500000, shuffle=True, drop_last=False, sampler=None):
        """
        Args:
            dataset: æ•°æ®é›†å¯¹è±¡ï¼Œéœ€è¦èƒ½å¤Ÿè·å–æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°
            max_points: æ¯ä¸ª batch çš„æœ€å¤§ç‚¹æ•°
            shuffle: æ˜¯å¦æ‰“ä¹±é¡ºåºï¼ˆå½“ sampler=None æ—¶ç”Ÿæ•ˆï¼‰
            drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„ batch
            sampler: å¯é€‰çš„åŸºç¡€ Samplerï¼ˆå¦‚ WeightedRandomSamplerï¼‰
                    å¦‚æœæä¾›ï¼Œåˆ™ä½¿ç”¨è¯¥ sampler ç”Ÿæˆç´¢å¼•åºåˆ—ï¼Œshuffle å‚æ•°å°†è¢«å¿½ç•¥
        """
        self.dataset = dataset
        self.max_points = max_points
        self.shuffle = shuffle
        self.drop_last = drop_last
        self.sampler = sampler
        
        # é¢„å…ˆè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°
        self.num_points_list = self._get_num_points_list()
        
    def _get_num_points_list(self):
        """è·å–æ¯ä¸ªæ ·æœ¬çš„ç‚¹æ•°ï¼ˆè€ƒè™‘ loop å‚æ•°å’Œé‡‡æ ·æ¨¡å¼ï¼‰"""
        
        # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ get_sample_num_points() æ–¹æ³•ï¼ˆæ”¯æŒä½“ç´ æ¨¡å¼çš„æ­£ç¡®ç‚¹æ•°ï¼‰
        if hasattr(self.dataset, 'get_sample_num_points'):
            num_points_list = self.dataset.get_sample_num_points()
            # å¦‚æœ dataset æœ‰ loop å‚æ•°ï¼Œéœ€è¦æ‰©å±•åˆ—è¡¨
            if hasattr(self.dataset, 'loop') and self.dataset.loop > 1:
                num_points_list = num_points_list * self.dataset.loop
            return num_points_list
        
        base_num_points_list = []
        
        # å°è¯•ä» dataset.data_list è·å–
        if hasattr(self.dataset, 'data_list'):
            for sample_info in self.dataset.data_list:
                if 'num_points' in sample_info:
                    base_num_points_list.append(sample_info['num_points'])
                else:
                    # å¦‚æœæ²¡æœ‰ num_pointsï¼ŒåŠ è½½æ ·æœ¬ç»Ÿè®¡
                    sample = self.dataset[len(base_num_points_list)]
                    if 'coord' in sample:
                        base_num_points_list.append(len(sample['coord']))
                    else:
                        base_num_points_list.append(0)
            
            # å¦‚æœ dataset æœ‰ loop å‚æ•°ï¼Œéœ€è¦æ‰©å±•åˆ—è¡¨
            if hasattr(self.dataset, 'loop') and self.dataset.loop > 1:
                num_points_list = base_num_points_list * self.dataset.loop
            else:
                num_points_list = base_num_points_list
        else:
            # éå†æ•´ä¸ªæ•°æ®é›†è·å–ç‚¹æ•°ï¼ˆè¾ƒæ…¢ï¼‰
            print("Warning: Dataset doesn't have data_list, scanning all samples...")
            num_points_list = []
            for i in range(len(self.dataset)):
                sample = self.dataset[i]
                if 'coord' in sample:
                    num_points_list.append(len(sample['coord']))
                else:
                    num_points_list.append(0)
        
        return num_points_list
    
    def __iter__(self):
        # ç”Ÿæˆç´¢å¼•åˆ—è¡¨
        if self.sampler is not None:
            # ä½¿ç”¨æä¾›çš„ samplerï¼ˆå¦‚ WeightedRandomSamplerï¼‰
            indices = list(self.sampler)
        elif self.shuffle:
            # éšæœºæ‰“ä¹±
            indices = torch.randperm(len(self.dataset)).tolist()
        else:
            # é¡ºåºéå†
            indices = list(range(len(self.dataset)))
        
        # åŠ¨æ€ç”Ÿæˆ batch
        batch = []
        batch_points = 0
        
        for idx in indices:
            num_points = self.num_points_list[idx]
            
            # å¦‚æœå½“å‰ batch ä¸ºç©ºï¼Œæˆ–è€…åŠ å…¥å½“å‰æ ·æœ¬ä¸ä¼šè¶…è¿‡é™åˆ¶
            if len(batch) == 0 or batch_points + num_points <= self.max_points:
                batch.append(idx)
                batch_points += num_points
            else:
                # å½“å‰ batch å·²æ»¡ï¼Œyield å¹¶å¼€å§‹æ–° batch
                yield batch
                batch = [idx]
                batch_points = num_points
        
        # å¤„ç†æœ€åä¸€ä¸ª batch
        if len(batch) > 0 and not self.drop_last:
            yield batch
    
    def __len__(self):
        # ä¼°ç®— batch æ•°é‡ï¼ˆä¸å®Œå…¨å‡†ç¡®ï¼Œä½†è¶³å¤Ÿç”¨ï¼‰
        total_points = sum(self.num_points_list)
        estimated_batches = (total_points + self.max_points - 1) // self.max_points
        return max(1, estimated_batches)

