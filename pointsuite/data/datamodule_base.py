"""
ç‚¹äº‘æ•°æ®çš„åŸºç¡€ DataModule

æœ¬æ¨¡å—æä¾›äº†ä¸€ä¸ªæŠ½è±¡åŸºç±»ï¼Œç”¨äº PyTorch Lightning DataModuleï¼Œ
å¯ä»¥æ‰©å±•ä»¥æ”¯æŒä¸åŒçš„æ•°æ®é›†æ ¼å¼ã€‚
"""

import pytorch_lightning as pl
from torch.utils.data import DataLoader, WeightedRandomSampler
from typing import Optional, List, Dict, Any
from pathlib import Path
from abc import ABC, abstractmethod

from .datasets.collate import collate_fn, DynamicBatchSampler


class DataModuleBase(pl.LightningDataModule, ABC):
    """
    ç‚¹äº‘æ•°æ®æ¨¡å—çš„æŠ½è±¡åŸºç±»
    
    æœ¬ç±»æä¾›æ•°æ®åŠ è½½çš„é€šç”¨åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - è®¾ç½®è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†
    - åˆ›å»ºæ”¯æŒ DynamicBatchSampler çš„ DataLoader
    - æ”¯æŒ WeightedRandomSampler ä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    - é…ç½®å¯è°ƒèŠ‚çš„å·¥ä½œè¿›ç¨‹ä»¥å®ç°å†…å­˜é«˜æ•ˆçš„æ•°æ®åŠ è½½
    
    å­ç±»å¿…é¡»å®ç°ï¼š
    - _create_dataset(): ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ’åˆ†åˆ›å»ºæ•°æ®é›†å®ä¾‹
    
    ç¤ºä¾‹ï¼š
        >>> class MyDataModule(DataModuleBase):
        ...     def _create_dataset(self, data_paths, split, transforms):
        ...         return MyDataset(data_paths, split=split, transform=transforms)
        ...
        >>> datamodule = MyDataModule(
        ...     data_root='path/to/data',
        ...     train_files=['train.pkl'],
        ...     batch_size=8
        ... )
    """
    
    def __init__(
        self,
        train_data: Optional[Any] = None,
        val_data: Optional[Any] = None,
        test_data: Optional[Any] = None,
        predict_data: Optional[Any] = None,
        batch_size: int = 8,
        num_workers: int = 4,
        train_transforms: Optional[List] = None,
        val_transforms: Optional[List] = None,
        test_transforms: Optional[List] = None,
        predict_transforms: Optional[List] = None,
        train_loop: int = 1,
        val_loop: int = 1,
        test_loop: int = 1,
        predict_loop: int = 1,
        use_dynamic_batch: bool = False,
        max_points: int = 500000,
        use_dynamic_batch_inference: bool = False,
        max_points_inference: Optional[int] = None,
        use_weighted_sampler: bool = False,
        train_sampler_weights: Optional[List[float]] = None,
        class_weights: Optional[Any] = None,
        pin_memory: bool = True,
        persistent_workers: bool = False,
        prefetch_factor: Optional[int] = 2,
        **kwargs
    ):
        """
        åˆå§‹åŒ– DataModuleBase
        
        å‚æ•°ï¼š
            train_data: è®­ç»ƒæ•°æ®è·¯å¾„ï¼Œå¯ä»¥æ˜¯:
                       - å­—ç¬¦ä¸²ï¼šå•ä¸ªæ–‡ä»¶è·¯å¾„æˆ–åŒ…å«å¤šä¸ªæ–‡ä»¶çš„ç›®å½•
                       - åˆ—è¡¨ï¼šæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆæ”¯æŒè·¨ç›®å½•ï¼‰
                       - Noneï¼šä¸ä½¿ç”¨è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®è·¯å¾„ï¼ˆæ ¼å¼åŒ train_dataï¼‰
            test_data: æµ‹è¯•æ•°æ®è·¯å¾„ï¼ˆæ ¼å¼åŒ train_dataï¼‰
            predict_data: é¢„æµ‹æ•°æ®è·¯å¾„ï¼ˆæ ¼å¼åŒ train_dataï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ test_data
            batch_size: DataLoader çš„æ‰¹æ¬¡å¤§å°ï¼ˆå½“ use_dynamic_batch=True æ—¶ä¸ä½¿ç”¨ï¼‰
            num_workers: æ•°æ®åŠ è½½çš„å·¥ä½œè¿›ç¨‹æ•°
            train_transforms: è®­ç»ƒæ•°æ®çš„å˜æ¢åˆ—è¡¨
            val_transforms: éªŒè¯æ•°æ®çš„å˜æ¢åˆ—è¡¨
            test_transforms: æµ‹è¯•æ•°æ®çš„å˜æ¢åˆ—è¡¨
            predict_transforms: é¢„æµ‹æ•°æ®çš„å˜æ¢åˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ test_transforms
            train_loop: è®­ç»ƒæ•°æ®é›†å¾ªç¯æ¬¡æ•°ï¼ˆæ•°æ®å¢å¼ºï¼‰
            val_loop: éªŒè¯æ•°æ®é›†å¾ªç¯æ¬¡æ•°ï¼ˆTest-Time Augmentationï¼‰
            test_loop: æµ‹è¯•æ•°æ®é›†å¾ªç¯æ¬¡æ•°ï¼ˆTest-Time Augmentationï¼‰
            predict_loop: é¢„æµ‹æ•°æ®é›†å¾ªç¯æ¬¡æ•°ï¼ˆTest-Time Augmentationï¼‰
            use_dynamic_batch: æ˜¯å¦åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨ DynamicBatchSamplerï¼ˆæ¨èç”¨äºå†…å­˜æ§åˆ¶ï¼‰
                              å¦‚æœä¸º Trueï¼Œbatch_size å‚æ•°å°†è¢«å¿½ç•¥
            max_points: è®­ç»ƒé˜¶æ®µæ¯ä¸ªæ‰¹æ¬¡çš„æœ€å¤§ç‚¹æ•°ï¼ˆä»…åœ¨ use_dynamic_batch=True æ—¶ä½¿ç”¨ï¼‰
            use_dynamic_batch_inference: æ˜¯å¦åœ¨æ¨ç†é˜¶æ®µï¼ˆval/test/predictï¼‰ä½¿ç”¨ DynamicBatchSampler
                                        é»˜è®¤ä¸º Falseï¼ˆä¸è®­ç»ƒé˜¶æ®µç‹¬ç«‹ï¼‰
                                        æ¨èï¼šå¤§åœºæ™¯æ¨ç†æ—¶è®¾ç½®ä¸º True ä»¥é¿å… OOM
                                        æ³¨æ„ï¼šä¸ TTA (loop > 1) ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç‚¹æ•°åŸºäº transform å‰çš„å€¼é¢„è®¡ç®—
                                              å¦‚æœ transform å¤§å¹…å¢åŠ ç‚¹æ•°ï¼ˆå¦‚å¯†é›†é‡‡æ ·ï¼‰ï¼Œè¯·è°¨æ…ä½¿ç”¨
            max_points_inference: æ¨ç†é˜¶æ®µæ¯ä¸ªæ‰¹æ¬¡çš„æœ€å¤§ç‚¹æ•°
                                 å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨ max_points çš„å€¼
                                 æ¨èï¼šæ ¹æ® GPU å†…å­˜è®¾ç½®ï¼ˆæ¨ç†æ—¶é€šå¸¸å¯ä»¥æ¯”è®­ç»ƒæ—¶æ›´å¤§ï¼‰
            use_weighted_sampler: æ˜¯å¦ä½¿ç”¨ WeightedRandomSamplerï¼ˆç‹¬ç«‹äº use_dynamic_batchï¼‰
                                 å¦‚æœä¸º True ä¸”æä¾›äº† train_sampler_weightsï¼Œå°†å¯ç”¨åŠ æƒé‡‡æ ·
            train_sampler_weights: WeightedRandomSampler çš„æƒé‡åˆ—è¡¨ï¼ˆä»…ç”¨äºè®­ç»ƒï¼‰
                                  é•¿åº¦å¿…é¡»ç­‰äº train_dataset çš„å®é™…é•¿åº¦ï¼ˆè€ƒè™‘ loopï¼‰
                                  âš ï¸ ä¸ä¼šä¿å­˜åˆ°è¶…å‚æ•°ä¸­ï¼ˆæ•°ç»„å¤ªé•¿ï¼‰
            pin_memory: æ˜¯å¦åœ¨ DataLoader ä¸­ä½¿ç”¨å›ºå®šå†…å­˜ï¼ˆæ›´å¿«çš„ GPU ä¼ è¾“ï¼‰
            persistent_workers: åœ¨ epoch ä¹‹é—´ä¿æŒå·¥ä½œè¿›ç¨‹æ´»åŠ¨ï¼ˆæ›´å¿«ä½†ä½¿ç”¨æ›´å¤šå†…å­˜ï¼‰
            prefetch_factor: æ¯ä¸ªå·¥ä½œè¿›ç¨‹é¢„å–çš„æ‰¹æ¬¡æ•°
            **kwargs: ä¼ é€’ç»™å­ç±»å’Œæ•°æ®é›†çš„å…¶ä»–å‚æ•°
        """
        super().__init__()
        
        # ä¿å­˜è¶…å‚æ•°ï¼ˆæ’é™¤ transforms å’Œ weights ä»¥é¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰
        self.save_hyperparameters(ignore=[
            'train_transforms', 'val_transforms', 'test_transforms', 'predict_transforms',
            # 'train_sampler_weights'  # weights æ•°ç»„å¤ªé•¿ï¼Œä¸é€‚åˆä¿å­˜
        ])
        
        # å­˜å‚¨æ•°æ®è·¯å¾„ï¼ˆçµæ´»æ”¯æŒæ–‡ä»¶/ç›®å½•/åˆ—è¡¨ï¼‰
        self.train_data = train_data
        self.val_data = val_data
        self.test_data = test_data
        self.predict_data = predict_data if predict_data is not None else test_data
        
        # å­˜å‚¨åŸºæœ¬å‚æ•°
        self.batch_size = batch_size
        self.num_workers = num_workers
        
        # å­˜å‚¨æ•°æ®å˜æ¢
        self.train_transforms = train_transforms
        self.val_transforms = val_transforms
        self.test_transforms = test_transforms
        self.predict_transforms = predict_transforms if predict_transforms is not None else test_transforms
        
        # å­˜å‚¨å¾ªç¯å‚æ•°ï¼ˆæ”¯æŒ Test-Time Augmentationï¼‰
        self.train_loop = train_loop
        self.val_loop = val_loop
        self.test_loop = test_loop
        self.predict_loop = predict_loop
        
        # å­˜å‚¨é‡‡æ ·å‚æ•°
        self.use_dynamic_batch = use_dynamic_batch
        self.max_points = max_points
        # æ¨ç†é˜¶æ®µçš„åŠ¨æ€ batch è®¾ç½®ï¼ˆä¸è®­ç»ƒé˜¶æ®µç‹¬ç«‹ï¼‰
        self.use_dynamic_batch_inference = use_dynamic_batch_inference
        self.max_points_inference = max_points_inference if max_points_inference is not None else max_points
        self.use_weighted_sampler = use_weighted_sampler
        self.train_sampler_weights = train_sampler_weights
        self.class_weights = class_weights
        
        # å­˜å‚¨ DataLoader å‚æ•°
        self.pin_memory = pin_memory
        self.persistent_workers = persistent_workers
        self.prefetch_factor = prefetch_factor
        
        # å­˜å‚¨å­ç±»çš„é¢å¤–å‚æ•°
        self.kwargs = kwargs
        
        # åˆå¹¶å‡½æ•°ï¼ˆå§‹ç»ˆä½¿ç”¨åŸºæœ¬çš„ collate_fn é…åˆ DynamicBatchSamplerï¼‰
        self.collate_fn = collate_fn
        
        # æ•°æ®é›†å ä½ç¬¦
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None
        self.predict_dataset = None
        
        # éªŒè¯ï¼šè‡³å°‘æä¾›ä¸€ä¸ªæ•°æ®æº
        if all(x is None for x in [train_data, val_data, test_data, predict_data]):
            raise ValueError("å¿…é¡»è‡³å°‘æä¾›ä¸€ä¸ªæ•°æ®æºï¼ˆtrain_data/val_data/test_data/predict_dataï¼‰")
    
    @abstractmethod
    def _create_dataset(self, data_paths, split: str, transforms):
        """
        ä¸ºç»™å®šçš„æ•°æ®é›†åˆ’åˆ†åˆ›å»ºæ•°æ®é›†å®ä¾‹
        
        å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•ä»¥åˆ›å»ºé€‚å½“çš„æ•°æ®é›†ç±»å‹
        
        å‚æ•°ï¼š
            data_paths: æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ Pathã€Path åˆ—è¡¨æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
            split: æ•°æ®é›†åˆ’åˆ†ï¼ˆ'train'ã€'val'ã€'test'ï¼‰
            transforms: è¦åº”ç”¨çš„å˜æ¢åˆ—è¡¨
            
        è¿”å›ï¼š
            æ•°æ®é›†å®ä¾‹
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° _create_dataset() æ–¹æ³•")
    
    def prepare_data(self):
        """
        ä¸‹è½½ã€åˆ†è¯ç­‰æ•°æ®å‡†å¤‡å·¥ä½œï¼ˆåœ¨ 1 ä¸ª GPU/TPU ä¸Šçš„å•è¿›ç¨‹ä¸­æ‰§è¡Œï¼‰
        
        åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ­¤æ–¹æ³•ä»…åœ¨ 1 ä¸ª GPU ä¸Šè°ƒç”¨
        ç”¨äºåªéœ€æ‰§è¡Œä¸€æ¬¡çš„æ•°æ®å‡†å¤‡æ­¥éª¤
        """
        # åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‡è®¾æ•°æ®å·²ç»å‡†å¤‡å¥½äº†
        # å¦‚æœéœ€è¦ï¼Œå­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•
        pass
    
    def setup(self, stage: Optional[str] = None):
        """
        ä¸ºæ¯ä¸ªé˜¶æ®µè®¾ç½®æ•°æ®é›†ï¼ˆfitã€validateã€testã€predictï¼‰
        
        åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ­¤æ–¹æ³•åœ¨æ¯ä¸ª GPU ä¸Šè°ƒç”¨
        
        å‚æ•°ï¼š
            stage: å½“å‰é˜¶æ®µï¼ˆ'fit'ã€'validate'ã€'test'ã€'predict'ï¼Œæˆ– None è¡¨ç¤ºæ‰€æœ‰é˜¶æ®µï¼‰
        """
        # ğŸ” è°ƒè¯•ï¼šæ‰“å° setup è¢«è°ƒç”¨çš„ stage
        print(f"[DEBUG] DataModuleBase.setup(stage='{stage}')")
        
        # è®¾ç½®è®­ç»ƒæ•°æ®é›†
        if (stage == 'fit' or stage is None) and self.train_data is not None:
            self.train_dataset = self._create_dataset(
                data_paths=self.train_data,
                split='train',
                transforms=self.train_transforms
            )
            
            # å¦‚æœå¯ç”¨åŠ æƒé‡‡æ ·ä½†æœªæä¾›æƒé‡ï¼Œåˆ™è‡ªåŠ¨è®¡ç®—
            if self.use_weighted_sampler and self.train_sampler_weights is None:
                self.train_sampler_weights = self._compute_sample_weights(self.train_dataset)
            
            # å¦‚æœå¯ç”¨åŠ æƒé‡‡æ ·ä½†æœªæä¾›æƒé‡ï¼Œåˆ™è‡ªåŠ¨è®¡ç®—
            if self.use_weighted_sampler and self.train_sampler_weights is None:
                self.train_sampler_weights = self._compute_sample_weights(self.train_dataset)
        
        # è®¾ç½®éªŒè¯æ•°æ®é›†
        if (stage == 'fit' or stage == 'validate' or stage is None) and self.val_data is not None:
            self.val_dataset = self._create_dataset(
                data_paths=self.val_data,
                split='val',
                transforms=self.val_transforms
            )
        
        # è®¾ç½®æµ‹è¯•æ•°æ®é›†
        if (stage == 'test' or stage is None) and self.test_data is not None:
            self.test_dataset = self._create_dataset(
                data_paths=self.test_data,
                split='test',
                transforms=self.test_transforms
            )
        
        # è®¾ç½®é¢„æµ‹æ•°æ®é›†ï¼ˆç‹¬ç«‹äºæµ‹è¯•ï¼‰
        if (stage == 'predict' or stage is None) and self.predict_data is not None:
            print(f"[DEBUG] Creating predict_dataset from predict_data={self.predict_data}")
            self.predict_dataset = self._create_dataset(
                data_paths=self.predict_data,
                split='predict',
                transforms=self.predict_transforms
            )
            print(f"[DEBUG] predict_dataset created with {len(self.predict_dataset)} samples")
    
    def _compute_sample_weights(self, dataset):
        """
        è‡ªåŠ¨è®¡ç®—è®­ç»ƒæ ·æœ¬æƒé‡
        
        å‚æ•°ï¼š
            dataset: æ•°æ®é›†å®ä¾‹
            
        è¿”å›ï¼š
            æ ·æœ¬æƒé‡åˆ—è¡¨ï¼ˆè€ƒè™‘ train_loopï¼‰
        """
        import torch
        import numpy as np
        
        # è½¬æ¢ class_weights ä¸ºå­—å…¸
        if self.class_weights is None:
            # ä»æ•°æ®é›†è‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡
            print("è‡ªåŠ¨ä»æ•°æ®é›†è®¡ç®—ç±»åˆ«æƒé‡...")
            class_weights_dict = dataset.compute_class_weights(
                method='sqrt_inverse',  # sqrt_inverse æ¯” log_inverse å·®å¼‚æ›´å¤§
                smooth=1.0,
                normalize=True
            )
            
            if class_weights_dict is None:
                print("è­¦å‘Š: æ•°æ®é›†ä¸æ”¯æŒè‡ªåŠ¨ç±»åˆ«æƒé‡è®¡ç®—ï¼Œä½¿ç”¨å‡åŒ€æƒé‡")
                return None
            
            print(f"è®¡ç®—çš„ç±»åˆ«æƒé‡: {class_weights_dict}")
        elif isinstance(self.class_weights, torch.Tensor):
            class_weights_dict = {i: float(w) for i, w in enumerate(self.class_weights)}
        elif isinstance(self.class_weights, dict):
            class_weights_dict = self.class_weights
        else:
            print(f"è­¦å‘Š: class_weights ç±»å‹ä¸æ”¯æŒ: {type(self.class_weights)}")
            return None
        
        # è·å–åŸºç¡€æ ·æœ¬æƒé‡ï¼ˆä¸è€ƒè™‘ loopï¼‰
        base_weights = dataset.get_sample_weights(class_weights_dict)
        
        if base_weights is None:
            print("è­¦å‘Š: æ•°æ®é›†ä¸æ”¯æŒæ ·æœ¬æƒé‡è®¡ç®—")
            return None
        
        # å¦‚æœ train_loop > 1ï¼Œé‡å¤æƒé‡
        if self.train_loop > 1:
            weights = np.tile(base_weights, self.train_loop)
        else:
            weights = base_weights
        
        print(f"è®¡ç®—æ ·æœ¬æƒé‡:")
        print(f"  - åŸºç¡€æ ·æœ¬æ•°: {len(base_weights)}")
        print(f"  - Train loop: {self.train_loop}")
        print(f"  - æœ€ç»ˆæ ·æœ¬æ•°: {len(weights)}")
        print(f"  - æƒé‡èŒƒå›´: [{weights.min():.4f}, {weights.max():.4f}]")
        print(f"  - æƒé‡å‡å€¼: {weights.mean():.4f}")
        
        return weights.tolist()
    
    def _create_dataloader(
        self,
        dataset,
        shuffle: bool = False,
        drop_last: bool = False,
        use_sampler_weights: bool = False,
        use_dynamic_batch: Optional[bool] = None,
        max_points: Optional[int] = None
    ) -> DataLoader:
        """
        åˆ›å»ºå…·æœ‰é€‚å½“è®¾ç½®çš„ DataLoader
        
        å‚æ•°ï¼š
            dataset: æ•°æ®é›†å®ä¾‹
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®ï¼ˆå¦‚æœä½¿ç”¨ sampler åˆ™å¿½ç•¥ï¼‰
            drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
            use_sampler_weights: æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ ·ï¼ˆä»…ç”¨äºè®­ç»ƒï¼‰
            use_dynamic_batch: æ˜¯å¦ä½¿ç”¨åŠ¨æ€ batchï¼ˆå¦‚æœä¸º None åˆ™ä½¿ç”¨å®ä¾‹é»˜è®¤å€¼ï¼‰
            max_points: æœ€å¤§ç‚¹æ•°ï¼ˆå¦‚æœä¸º None åˆ™ä½¿ç”¨å®ä¾‹é»˜è®¤å€¼ï¼‰
            
        è¿”å›ï¼š
            DataLoader å®ä¾‹
        """
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–å®ä¾‹é»˜è®¤å€¼
        _use_dynamic_batch = use_dynamic_batch if use_dynamic_batch is not None else self.use_dynamic_batch
        _max_points = max_points if max_points is not None else self.max_points
        # åˆ›å»ºåŸºç¡€é‡‡æ ·å™¨ï¼ˆä»…ç”¨äºè®­ç»ƒï¼‰
        # val/test/predict å¿…é¡»è®¿é—®æ‰€æœ‰æ ·æœ¬ï¼Œä¸ä½¿ç”¨ sampler
        base_sampler = None
        if use_sampler_weights and self.use_weighted_sampler and self.train_sampler_weights is not None:
            # éªŒè¯ weights é•¿åº¦ä¸ dataset é•¿åº¦åŒ¹é…
            if len(self.train_sampler_weights) != len(dataset):
                raise ValueError(
                    f"train_sampler_weights é•¿åº¦ ({len(self.train_sampler_weights)}) "
                    f"ä¸ dataset é•¿åº¦ ({len(dataset)}) ä¸åŒ¹é…ã€‚\n"
                    f"æç¤ºï¼šå¦‚æœä½¿ç”¨ train_loop > 1ï¼Œweights éœ€è¦é‡å¤ train_loop æ¬¡ã€‚\n"
                    f"ä¾‹å¦‚ï¼šweights = original_weights * train_loop"
                )
            
            base_sampler = WeightedRandomSampler(
                weights=self.train_sampler_weights,
                num_samples=len(dataset),
                replacement=True  # ä½¿ç”¨æœ‰æ”¾å›é‡‡æ ·ä»¥æ”¯æŒè¿‡é‡‡æ ·
            )
        
        if _use_dynamic_batch:
            # ä½¿ç”¨ DynamicBatchSamplerï¼ˆå¯ä»¥ä¸ base_sampler ç»“åˆï¼‰
            # æ³¨æ„ï¼šshuffle æ§åˆ¶æ˜¯å¦æ‰“ä¹±ï¼ŒåŒæ—¶ä¹Ÿå†³å®š batch æ•°çš„è®¡ç®—æ–¹å¼
            # - shuffle=True: ä¿å®ˆä¼°è®¡ batch æ•°ï¼Œç”¨äºè®­ç»ƒ
            # - shuffle=False: ç²¾ç¡®è®¡ç®— batch æ•°ï¼Œç”¨äº test/predict
            batch_sampler = DynamicBatchSampler(
                dataset=dataset,
                max_points=_max_points,
                shuffle=(shuffle and base_sampler is None),  # ä»…åœ¨æ²¡æœ‰ base_sampler æ—¶æ‰“ä¹±
                sampler=base_sampler
            )
            
            return DataLoader(
                dataset,
                batch_sampler=batch_sampler,
                num_workers=self.num_workers,
                collate_fn=self.collate_fn,
                pin_memory=self.pin_memory,
                persistent_workers=self.persistent_workers and self.num_workers > 0,
                prefetch_factor=self.prefetch_factor if self.num_workers > 0 else None,
            )
        else:
            # ä½¿ç”¨æ ‡å‡†çš„å›ºå®š batch_size
            # æ³¨æ„ï¼šå¦‚æœæœ‰ base_samplerï¼Œåˆ™ä¸èƒ½åŒæ—¶ä½¿ç”¨ shuffle
            return DataLoader(
                dataset,
                batch_size=self.batch_size,
                sampler=base_sampler,
                shuffle=(shuffle and base_sampler is None),  # sampler å’Œ shuffle äº’æ–¥
                num_workers=self.num_workers,
                collate_fn=self.collate_fn,
                pin_memory=self.pin_memory,
                persistent_workers=self.persistent_workers and self.num_workers > 0,
                prefetch_factor=self.prefetch_factor if self.num_workers > 0 else None,
                drop_last=drop_last,
            )
    
    def train_dataloader(self) -> DataLoader:
        """åˆ›å»ºå¹¶è¿”å›è®­ç»ƒ DataLoader"""
        return self._create_dataloader(
            dataset=self.train_dataset,
            shuffle=True,
            drop_last=True,
            use_sampler_weights=True,  # ä¸ºè®­ç»ƒå¯ç”¨åŠ æƒé‡‡æ ·
            use_dynamic_batch=self.use_dynamic_batch,
            max_points=self.max_points
        )
    
    def val_dataloader(self) -> DataLoader:
        """åˆ›å»ºå¹¶è¿”å›éªŒè¯ DataLoaderï¼ˆå¿…é¡»è®¿é—®æ‰€æœ‰æ ·æœ¬ï¼‰"""
        if self.val_dataset is None:
            return None
        return self._create_dataloader(
            dataset=self.val_dataset,
            shuffle=False,
            drop_last=False,
            use_sampler_weights=False,  # éªŒè¯ä¸ä½¿ç”¨åŠ æƒé‡‡æ ·ï¼Œå¿…é¡»è®¿é—®æ‰€æœ‰æ ·æœ¬
            use_dynamic_batch=self.use_dynamic_batch_inference,
            max_points=self.max_points_inference
        )
    
    def test_dataloader(self) -> DataLoader:
        """åˆ›å»ºå¹¶è¿”å›æµ‹è¯• DataLoaderï¼ˆå¿…é¡»è®¿é—®æ‰€æœ‰æ ·æœ¬ï¼‰"""
        return self._create_dataloader(
            dataset=self.test_dataset,
            shuffle=False,
            drop_last=False,
            use_sampler_weights=False,  # æµ‹è¯•ä¸ä½¿ç”¨åŠ æƒé‡‡æ ·ï¼Œå¿…é¡»è®¿é—®æ‰€æœ‰æ ·æœ¬
            use_dynamic_batch=self.use_dynamic_batch_inference,
            max_points=self.max_points_inference
        )
    
    def predict_dataloader(self) -> DataLoader:
        """åˆ›å»ºå¹¶è¿”å›é¢„æµ‹ DataLoaderï¼ˆå¿…é¡»è®¿é—®æ‰€æœ‰æ ·æœ¬ï¼‰"""
        # å¦‚æœæœ‰ç‹¬ç«‹çš„ predict_datasetï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™å›é€€åˆ° test_dataset
        dataset = self.predict_dataset if self.predict_dataset is not None else self.test_dataset
        return self._create_dataloader(
            dataset=dataset,
            shuffle=False,
            drop_last=False,
            use_sampler_weights=False,  # é¢„æµ‹ä¸ä½¿ç”¨åŠ æƒé‡‡æ ·ï¼Œå¿…é¡»è®¿é—®æ‰€æœ‰æ ·æœ¬
            use_dynamic_batch=self.use_dynamic_batch_inference,
            max_points=self.max_points_inference
        )
    
    def teardown(self, stage: Optional[str] = None):
        """
        è®­ç»ƒ/æµ‹è¯•åæ¸…ç†èµ„æº
        
        å‚æ•°ï¼š
            stage: å½“å‰é˜¶æ®µï¼ˆ'fit'ã€'validate'ã€'test'ã€'predict'ï¼‰
        """
        # æ¸…ç†æ•°æ®é›†ä»¥é‡Šæ”¾å†…å­˜
        if stage == 'fit':
            self.train_dataset = None
            self.val_dataset = None
        elif stage == 'test':
            self.test_dataset = None
        elif stage == 'predict':
            self.predict_dataset = None
    
    def on_exception(self, exception: BaseException):
        """
        åœ¨è®­ç»ƒ/æµ‹è¯•æœŸé—´å¼•å‘å¼‚å¸¸æ—¶è°ƒç”¨
        
        å‚æ•°ï¼š
            exception: å¼•å‘çš„å¼‚å¸¸
        """
        # æ¸…ç†èµ„æº
        self.teardown()
    
    # å·¥å…·æ–¹æ³•
    
    def get_dataset_info(self, split: str = 'train') -> Dict[str, Any]:
        """
        è·å–æ•°æ®é›†åˆ’åˆ†çš„ä¿¡æ¯
        
        å‚æ•°ï¼š
            split: æ•°æ®é›†åˆ’åˆ†ï¼ˆ'train'ã€'val'ã€'test'ã€'predict'ï¼‰
            
        è¿”å›ï¼š
            åŒ…å«æ•°æ®é›†ä¿¡æ¯çš„å­—å…¸
        """
        if split == 'train' and self.train_dataset is not None:
            dataset = self.train_dataset
        elif split == 'val' and self.val_dataset is not None:
            dataset = self.val_dataset
        elif split == 'test' and self.test_dataset is not None:
            dataset = self.test_dataset
        elif split == 'predict' and self.predict_dataset is not None:
            dataset = self.predict_dataset
        else:
            raise ValueError(f"åˆ’åˆ† '{split}' çš„æ•°æ®é›†æœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ setup()")
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        info = {
            'split': split,
            'total_length': len(dataset),
        }
        
        # å¦‚æœå¯ç”¨ï¼Œæ·»åŠ æ•°æ®é›†ç‰¹å®šçš„ä¿¡æ¯
        if hasattr(dataset, 'data_list'):
            info['num_samples'] = len(dataset.data_list)
        if hasattr(dataset, 'loop'):
            info['loop'] = dataset.loop
        if hasattr(dataset, 'cache_data'):
            info['cache_enabled'] = dataset.cache_data
        if hasattr(dataset, 'assets'):
            info['assets'] = dataset.assets
        if hasattr(dataset, 'class_mapping'):
            info['class_mapping'] = dataset.class_mapping
        
        return info
    
    def print_info(self):
        """æ‰“å°æ‰€æœ‰å·²åˆå§‹åŒ–æ•°æ®é›†çš„ä¿¡æ¯"""
        print("=" * 60)
        print(f"{self.__class__.__name__} ä¿¡æ¯")
        print("=" * 60)
        print(f"è®­ç»ƒæ•°æ®: {self.train_data}")
        print(f"éªŒè¯æ•°æ®: {self.val_data}")
        print(f"æµ‹è¯•æ•°æ®: {self.test_data}")
        print(f"é¢„æµ‹æ•°æ®: {self.predict_data}")
        print(f"ä½¿ç”¨åŠ¨æ€æ‰¹æ¬¡: {self.use_dynamic_batch}")
        if self.use_dynamic_batch:
            print(f"æ¯æ‰¹æ¬¡æœ€å¤§ç‚¹æ•°: {self.max_points}")
        else:
            print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"ä½¿ç”¨åŠ æƒé‡‡æ ·: {self.use_weighted_sampler}")
        if self.use_weighted_sampler:
            print(f"æƒé‡å·²æä¾›: {'æ˜¯' if self.train_sampler_weights is not None else 'å¦'}")
            if self.train_sampler_weights is not None:
                print(f"æƒé‡æ•°é‡: {len(self.train_sampler_weights)}")
        print(f"å·¥ä½œè¿›ç¨‹æ•°: {self.num_workers}")
        print(f"åˆå¹¶å‡½æ•°: {self.collate_fn.__name__ if hasattr(self.collate_fn, '__name__') else type(self.collate_fn).__name__}")
        print("-" * 60)
        
        for split in ['train', 'val', 'test', 'predict']:
            try:
                info = self.get_dataset_info(split)
                print(f"{split.upper()} æ•°æ®é›†:")
                for key, value in info.items():
                    if key != 'split':
                        print(f"  - {key}: {value}")
            except ValueError:
                print(f"{split.upper()} æ•°æ®é›†: æœªåˆå§‹åŒ–")
        
        print("=" * 60)
