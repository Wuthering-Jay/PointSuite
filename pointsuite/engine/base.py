"""
ç‚¹äº‘ä»»åŠ¡å¼•æ“åŸºç±»

æä¾›é€šç”¨çš„ä»»åŠ¡æ‰§è¡Œæµç¨‹ï¼ŒåŒ…æ‹¬:
- é…ç½®è§£æå’Œç»„ä»¶å®ä¾‹åŒ–
- DataModule/Task/Trainer åˆ›å»º
- è®­ç»ƒ/æµ‹è¯•/é¢„æµ‹æµç¨‹æ§åˆ¶

å­ç±»éœ€è¦å®ç°:
- _create_datamodule(): åˆ›å»ºæ•°æ®æ¨¡å—
- _create_task(): åˆ›å»ºä»»åŠ¡æ¨¡å—
- _get_default_callbacks(): è·å–é»˜è®¤å›è°ƒ
"""

import os
import sys
import warnings
import importlib
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Type

import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import (
    ModelCheckpoint,
    EarlyStopping,
    LearningRateMonitor,
    Callback,
)

from ..utils.config import (
    ExperimentConfig,
    ConfigLoader,
    create_experiment_config,
    load_yaml,
    save_yaml,
    deep_merge,
)
from ..utils.logger import setup_logger, Colors, print_header, print_section, print_config


class BaseEngine(ABC):
    """
    ç‚¹äº‘ä»»åŠ¡å¼•æ“åŸºç±»
    
    è´Ÿè´£æ•´ä¸ªå®éªŒæµç¨‹çš„ç®¡ç†:
    1. é…ç½®åŠ è½½å’Œè§£æ
    2. ç»„ä»¶å®ä¾‹åŒ– (DataModule, Task, Callbacks)
    3. Trainer åˆ›å»º
    4. è®­ç»ƒ/æµ‹è¯•/é¢„æµ‹æµç¨‹æ‰§è¡Œ
    
    ä½¿ç”¨æ–¹å¼:
        # æ–¹å¼1: YAML é…ç½®
        >>> engine = SemanticSegmentationEngine.from_config('configs/experiments/dales.yaml')
        >>> engine.run()
        
        # æ–¹å¼2: Python é…ç½®
        >>> engine = SemanticSegmentationEngine(
        ...     config=ExperimentConfig(...),
        ...     datamodule=my_datamodule,
        ...     task=my_task
        ... )
        >>> engine.run()
        
        # æ–¹å¼3: åˆ†æ­¥æ‰§è¡Œ
        >>> engine.setup()
        >>> engine.train()
        >>> engine.test()
        >>> engine.predict()
    """
    
    # ä»»åŠ¡ç±»å‹æ ‡è¯†ï¼Œå­ç±»éœ€è¦è¦†ç›–
    TASK_TYPE: str = "base"
    
    def __init__(
        self,
        config: Optional[Union[ExperimentConfig, Dict, str, Path]] = None,
        datamodule: Optional[pl.LightningDataModule] = None,
        task: Optional[pl.LightningModule] = None,
        trainer: Optional[pl.Trainer] = None,
        callbacks: Optional[List[Callback]] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–å¼•æ“
        
        Args:
            config: å®éªŒé…ç½®ï¼Œå¯ä»¥æ˜¯:
                   - ExperimentConfig å¯¹è±¡
                   - é…ç½®å­—å…¸
                   - YAML é…ç½®æ–‡ä»¶è·¯å¾„
            datamodule: æ•°æ®æ¨¡å— (å¯é€‰ï¼Œå¦‚æœæä¾›åˆ™è·³è¿‡é…ç½®åˆ›å»º)
            task: ä»»åŠ¡æ¨¡å— (å¯é€‰ï¼Œå¦‚æœæä¾›åˆ™è·³è¿‡é…ç½®åˆ›å»º)
            trainer: Trainer (å¯é€‰ï¼Œå¦‚æœæä¾›åˆ™è·³è¿‡é…ç½®åˆ›å»º)
            callbacks: é¢å¤–çš„å›è°ƒåˆ—è¡¨
            **kwargs: é¢å¤–å‚æ•°ï¼Œç”¨äºè¦†ç›–é…ç½®
        """
        # è§£æé…ç½®
        self.config = self._parse_config(config, **kwargs)
        
        # ç»„ä»¶ (å¯ä»¥é¢„è®¾æˆ–åç»­åˆ›å»º)
        self._datamodule = datamodule
        self._task = task
        self._trainer = trainer
        self._callbacks = callbacks or []
        
        # çŠ¶æ€æ ‡å¿—
        self._is_setup = False
        self._logger_initialized = False
        
        # å¿½ç•¥å¸¸è§è­¦å‘Š
        warnings.filterwarnings("ignore", ".*does not have many workers.*")
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    @classmethod
    def from_config(
        cls,
        config_path: Union[str, Path],
        cli_args: Optional[List[str]] = None,
        **kwargs
    ) -> 'BaseEngine':
        """
        ä» YAML é…ç½®æ–‡ä»¶åˆ›å»ºå¼•æ“
        
        Args:
            config_path: YAML é…ç½®æ–‡ä»¶è·¯å¾„
            cli_args: å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
            **kwargs: é¢å¤–å‚æ•°è¦†ç›–
            
        Returns:
            å¼•æ“å®ä¾‹
        """
        config = create_experiment_config(
            config_path=config_path,
            cli_args=cli_args,
            **kwargs
        )
        return cls(config=config)
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> 'BaseEngine':
        """
        ä»é…ç½®å­—å…¸åˆ›å»ºå¼•æ“
        
        Args:
            config_dict: é…ç½®å­—å…¸
            **kwargs: é¢å¤–å‚æ•°è¦†ç›–
            
        Returns:
            å¼•æ“å®ä¾‹
        """
        config = create_experiment_config(config_dict=config_dict, **kwargs)
        return cls(config=config)
    
    def _parse_config(
        self,
        config: Optional[Union[ExperimentConfig, Dict, str, Path]],
        **kwargs
    ) -> ExperimentConfig:
        """
        è§£æé…ç½®
        
        Args:
            config: é…ç½®è¾“å…¥
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            ExperimentConfig å¯¹è±¡
        """
        if config is None:
            config = ExperimentConfig()
        elif isinstance(config, (str, Path)):
            config = create_experiment_config(config_path=config, **kwargs)
        elif isinstance(config, dict):
            config = create_experiment_config(config_dict=config, **kwargs)
        elif isinstance(config, ExperimentConfig):
            if kwargs:
                # åº”ç”¨é¢å¤–å‚æ•°
                config = create_experiment_config(
                    config_dict=config.to_dict(),
                    **kwargs
                )
        
        return config
    
    # ========================================================================
    # å±æ€§è®¿é—®
    # ========================================================================
    
    @property
    def datamodule(self) -> pl.LightningDataModule:
        """è·å–æ•°æ®æ¨¡å—"""
        if self._datamodule is None:
            raise RuntimeError("DataModule æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ setup()")
        return self._datamodule
    
    @property
    def task(self) -> pl.LightningModule:
        """è·å–ä»»åŠ¡æ¨¡å—"""
        if self._task is None:
            raise RuntimeError("Task æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ setup()")
        return self._task
    
    @property
    def trainer(self) -> pl.Trainer:
        """è·å– Trainer"""
        if self._trainer is None:
            raise RuntimeError("Trainer æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ setup()")
        return self._trainer
    
    # ========================================================================
    # æŠ½è±¡æ–¹æ³• (å­ç±»å¿…é¡»å®ç°)
    # ========================================================================
    
    @abstractmethod
    def _create_datamodule(self) -> pl.LightningDataModule:
        """
        åˆ›å»ºæ•°æ®æ¨¡å—
        
        å­ç±»éœ€è¦å®ç°æ­¤æ–¹æ³•ï¼Œæ ¹æ®é…ç½®åˆ›å»ºå¯¹åº”çš„ DataModule
        
        Returns:
            DataModule å®ä¾‹
        """
        raise NotImplementedError
    
    @abstractmethod
    def _create_task(self) -> pl.LightningModule:
        """
        åˆ›å»ºä»»åŠ¡æ¨¡å—
        
        å­ç±»éœ€è¦å®ç°æ­¤æ–¹æ³•ï¼Œæ ¹æ®é…ç½®åˆ›å»ºå¯¹åº”çš„ Task
        
        Returns:
            Task å®ä¾‹
        """
        raise NotImplementedError
    
    def _get_default_callbacks(self) -> List[Callback]:
        """
        è·å–é»˜è®¤å›è°ƒåˆ—è¡¨
        
        å­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•æ·»åŠ ä»»åŠ¡ç‰¹å®šçš„å›è°ƒ
        
        Returns:
            å›è°ƒåˆ—è¡¨
        """
        return []
    
    # ========================================================================
    # ç»„ä»¶åˆ›å»ºè¾…åŠ©æ–¹æ³•
    # ========================================================================
    
    def _import_class(self, class_path: str) -> Type:
        """
        ä»å­—ç¬¦ä¸²è·¯å¾„å¯¼å…¥ç±»
        
        Args:
            class_path: ç±»è·¯å¾„ (å¦‚ 'pointsuite.models.PointTransformerV2')
            
        Returns:
            ç±»å¯¹è±¡
        """
        module_name, class_name = class_path.rsplit('.', 1)
        module = importlib.import_module(module_name)
        return getattr(module, class_name)
    
    def _instantiate_class(
        self,
        config: Dict[str, Any],
        **override_kwargs
    ) -> Any:
        """
        ä»é…ç½®å®ä¾‹åŒ–ç±»
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« 'class_path' å’Œå¯é€‰çš„ 'init_args'
            **override_kwargs: è¦†ç›–å‚æ•°
            
        Returns:
            å®ä¾‹åŒ–çš„å¯¹è±¡
        """
        class_path = config['class_path']
        init_args = config.get('init_args', {}).copy()
        init_args.update(override_kwargs)
        
        cls = self._import_class(class_path)
        return cls(**init_args)
    
    def _instantiate_transforms(self, transform_configs: List[Dict]) -> List:
        """
        ä»é…ç½®å®ä¾‹åŒ–å˜æ¢åˆ—è¡¨
        
        Args:
            transform_configs: å˜æ¢é…ç½®åˆ—è¡¨
            
        Returns:
            å˜æ¢å¯¹è±¡åˆ—è¡¨
        """
        transforms = []
        for cfg in transform_configs:
            if isinstance(cfg, dict) and 'class_path' in cfg:
                transforms.append(self._instantiate_class(cfg))
            else:
                # å·²ç»æ˜¯å˜æ¢å¯¹è±¡
                transforms.append(cfg)
        return transforms
    
    def _create_callbacks(self) -> List[Callback]:
        """
        åˆ›å»ºå›è°ƒåˆ—è¡¨
        
        Returns:
            å›è°ƒåˆ—è¡¨
        """
        callbacks = []
        callback_config = self.config._raw.get('callbacks', {})
        
        # ModelCheckpoint
        if 'model_checkpoint' in callback_config:
            ckpt_cfg = callback_config['model_checkpoint']
            callbacks.append(ModelCheckpoint(
                dirpath=os.path.join(self.config.output_dir, 'checkpoints'),
                **ckpt_cfg
            ))
        else:
            # é»˜è®¤æ£€æŸ¥ç‚¹
            callbacks.append(ModelCheckpoint(
                dirpath=os.path.join(self.config.output_dir, 'checkpoints'),
                monitor='mean_iou',
                mode='max',
                save_top_k=3,
                save_last=True,
                filename='{epoch:02d}-{mean_iou:.4f}',
                verbose=True
            ))
        
        # EarlyStopping
        if 'early_stopping' in callback_config:
            es_cfg = callback_config['early_stopping']
            callbacks.append(EarlyStopping(**es_cfg))
        
        # LearningRateMonitor
        callbacks.append(LearningRateMonitor(logging_interval='step'))
        
        # ä»»åŠ¡ç‰¹å®šçš„é»˜è®¤å›è°ƒ
        callbacks.extend(self._get_default_callbacks())
        
        # ç”¨æˆ·é¢å¤–çš„å›è°ƒ
        callbacks.extend(self._callbacks)
        
        # ä»é…ç½®å®ä¾‹åŒ–çš„è‡ªå®šä¹‰å›è°ƒ
        if 'custom_callbacks' in callback_config:
            for cb_cfg in callback_config['custom_callbacks']:
                callbacks.append(self._instantiate_class(cb_cfg))
        
        return callbacks
    
    def _create_trainer(self, callbacks: List[Callback]) -> pl.Trainer:
        """
        åˆ›å»º Trainer
        
        Args:
            callbacks: å›è°ƒåˆ—è¡¨
            
        Returns:
            Trainer å®ä¾‹
        """
        trainer_config = self.config.trainer.copy()
        
        # è®¾ç½®é»˜è®¤æ ¹ç›®å½•
        if trainer_config.get('default_root_dir') is None:
            trainer_config['default_root_dir'] = self.config.output_dir
        
        # ç¡®å®šåŠ é€Ÿå™¨
        if trainer_config.get('accelerator') == 'auto':
            trainer_config['accelerator'] = 'gpu' if torch.cuda.is_available() else 'cpu'
        
        # ç¦ç”¨ logger (æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰æ—¥å¿—)
        trainer_config.setdefault('logger', False)
        
        return pl.Trainer(
            callbacks=callbacks,
            **trainer_config
        )
    
    def _configure_optimizer(self, task: pl.LightningModule) -> None:
        """
        é…ç½®ä¼˜åŒ–å™¨
        
        å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†ä¼˜åŒ–å™¨ï¼Œé€šè¿‡ monkey patch è¦†ç›– task çš„ configure_optimizers
        
        Args:
            task: ä»»åŠ¡æ¨¡å—
        """
        optimizer_config = self.config._raw.get('optimizer')
        scheduler_config = self.config._raw.get('lr_scheduler')
        
        if optimizer_config is None:
            return
        
        # åˆ›å»ºæ–°çš„ configure_optimizers æ–¹æ³•
        def configure_optimizers(self_task):
            # å®ä¾‹åŒ–ä¼˜åŒ–å™¨
            optimizer_cls = self._import_class(optimizer_config['class_path'])
            optimizer_args = optimizer_config.get('init_args', {}).copy()
            
            # å¤„ç† lr å¼•ç”¨
            if 'lr' in optimizer_args and isinstance(optimizer_args['lr'], str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²å¼•ç”¨ï¼Œå°è¯•è§£æ
                optimizer_args['lr'] = self.config.get('task.init_args.learning_rate', 1e-3)
            
            optimizer = optimizer_cls(self_task.parameters(), **optimizer_args)
            
            if scheduler_config is None:
                return optimizer
            
            # å®ä¾‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler_cls = self._import_class(scheduler_config['class_path'])
            scheduler_args = scheduler_config.get('init_args', {}).copy()
            
            # ç‰¹æ®Šå¤„ç†: T_max ä¸º null æ—¶è‡ªåŠ¨è®¾ç½®
            if scheduler_args.get('T_max') is None and hasattr(self_task.trainer, 'estimated_stepping_batches'):
                scheduler_args['T_max'] = self_task.trainer.estimated_stepping_batches
            
            scheduler = scheduler_cls(optimizer, **scheduler_args)
            
            return {
                'optimizer': optimizer,
                'lr_scheduler': {
                    'scheduler': scheduler,
                    'interval': scheduler_config.get('interval', 'step'),
                    'frequency': scheduler_config.get('frequency', 1),
                }
            }
        
        # Monkey patch
        import types
        task.configure_optimizers = types.MethodType(configure_optimizers, task)
    
    # ========================================================================
    # ä¸»è¦æµç¨‹æ–¹æ³•
    # ========================================================================
    
    def setup(self, stage: Optional[str] = None) -> 'BaseEngine':
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        
        Args:
            stage: é˜¶æ®µ ('fit', 'test', 'predict', None è¡¨ç¤ºå…¨éƒ¨)
            
        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        if self._is_setup:
            return self
        
        # è®¾ç½®éšæœºç§å­
        pl.seed_everything(self.config.seed)
        
        # è®¾ç½®æ—¥å¿—
        if not self._logger_initialized:
            setup_logger(self.config.output_dir)
            self._logger_initialized = True
        
        # æ‰“å°é…ç½®
        self._print_config()
        
        # åˆ›å»º DataModule
        print_section("ğŸ“¦ åˆå§‹åŒ– DataModule")
        if self._datamodule is None:
            self._datamodule = self._create_datamodule()
        self._datamodule.setup(stage='fit' if stage is None else stage)
        
        # åˆ›å»º Task
        print_section("ğŸ§  åˆå§‹åŒ–æ¨¡å‹")
        if self._task is None:
            self._task = self._create_task()
        
        # é…ç½®ä¼˜åŒ–å™¨
        self._configure_optimizer(self._task)
        
        # åˆ›å»º Callbacks
        callbacks = self._create_callbacks()
        
        # åˆ›å»º Trainer
        print_section("ğŸ”§ åˆå§‹åŒ– Trainer")
        if self._trainer is None:
            self._trainer = self._create_trainer(callbacks)
        
        self._print_trainer_info()
        
        self._is_setup = True
        return self
    
    def train(self, ckpt_path: Optional[str] = None) -> 'BaseEngine':
        """
        æ‰§è¡Œè®­ç»ƒ
        
        Args:
            ckpt_path: checkpoint è·¯å¾„ (ç”¨äº resume)
            
        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        if not self._is_setup:
            self.setup()
        
        print_header("å¼€å§‹è®­ç»ƒ", "ğŸ‹ï¸")
        
        mode = self.config.mode
        
        if mode == 'train':
            self.trainer.fit(self.task, self.datamodule)
            
        elif mode == 'resume':
            ckpt = ckpt_path or self.config.checkpoint_path
            if ckpt is None:
                raise ValueError("resume æ¨¡å¼éœ€è¦æŒ‡å®š checkpoint_path")
            print(f"  ä» checkpoint ç»§ç»­è®­ç»ƒ: {Colors.CYAN}{ckpt}{Colors.RESET}")
            self.trainer.fit(self.task, self.datamodule, ckpt_path=ckpt)
            
        elif mode == 'finetune':
            ckpt = ckpt_path or self.config.checkpoint_path
            if ckpt is None:
                raise ValueError("finetune æ¨¡å¼éœ€è¦æŒ‡å®š checkpoint_path")
            print(f"  åŠ è½½é¢„è®­ç»ƒæƒé‡: {Colors.CYAN}{ckpt}{Colors.RESET}")
            self._load_pretrained_weights(ckpt)
            self.trainer.fit(self.task, self.datamodule)
        
        return self
    
    def test(self, ckpt_path: Optional[str] = None) -> 'BaseEngine':
        """
        æ‰§è¡Œæµ‹è¯•
        
        Args:
            ckpt_path: checkpoint è·¯å¾„
            
        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        if not self._is_setup:
            self.setup()
        
        print_header("å¼€å§‹æµ‹è¯•", "ğŸ§ª")
        
        # ç¡®å®š checkpoint
        if ckpt_path is None:
            if self.config.mode == 'test':
                ckpt_path = self.config.checkpoint_path
            else:
                ckpt_path = 'best'
        
        self.trainer.test(self.task, self.datamodule, ckpt_path=ckpt_path)
        return self
    
    def predict(self, ckpt_path: Optional[str] = None) -> 'BaseEngine':
        """
        æ‰§è¡Œé¢„æµ‹
        
        Args:
            ckpt_path: checkpoint è·¯å¾„
            
        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        if not self._is_setup:
            self.setup()
        
        print_header("å¼€å§‹é¢„æµ‹", "ğŸ”®")
        
        # ç¡®å®š checkpoint
        if ckpt_path is None:
            if self.config.mode == 'test':
                ckpt_path = self.config.checkpoint_path
            else:
                ckpt_path = 'best'
        
        self.trainer.predict(self.task, datamodule=self.datamodule, ckpt_path=ckpt_path)
        return self
    
    def run(self) -> 'BaseEngine':
        """
        æ ¹æ®é…ç½®æ‰§è¡Œå®Œæ•´æµç¨‹
        
        æ ¹æ® run.mode å†³å®šæ‰§è¡Œçš„æµç¨‹:
        - train: è®­ç»ƒ -> æµ‹è¯• -> é¢„æµ‹
        - resume: ç»§ç»­è®­ç»ƒ -> æµ‹è¯• -> é¢„æµ‹
        - finetune: å¾®è°ƒ -> æµ‹è¯• -> é¢„æµ‹
        - test: æµ‹è¯• -> é¢„æµ‹
        - predict: ä»…é¢„æµ‹
        
        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        self.setup()
        
        mode = self.config.mode
        
        if mode in ['train', 'resume', 'finetune']:
            self.train()
            if self.config.data.get('test_data'):
                self.test()
            if self.config.data.get('predict_data'):
                self.predict()
                
        elif mode == 'test':
            self.test()
            if self.config.data.get('predict_data'):
                self.predict()
                
        elif mode == 'predict':
            self.predict()
            
        else:
            raise ValueError(f"æœªçŸ¥çš„è¿è¡Œæ¨¡å¼: {mode}")
        
        self._print_completion()
        return self
    
    # ========================================================================
    # è¾…åŠ©æ–¹æ³•
    # ========================================================================
    
    def _load_pretrained_weights(self, ckpt_path: str) -> None:
        """
        åŠ è½½é¢„è®­ç»ƒæƒé‡ (ç”¨äº finetune)
        
        Args:
            ckpt_path: checkpoint è·¯å¾„
        """
        checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)
        state_dict = checkpoint['state_dict']
        
        # ç§»é™¤ 'model.' å‰ç¼€ (å¦‚æœæœ‰)
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('model.'):
                new_state_dict[k[6:]] = v
            else:
                new_state_dict[k] = v
        
        missing_keys, unexpected_keys = self.task.load_state_dict(new_state_dict, strict=False)
        
        if missing_keys:
            print(f"  {Colors.YELLOW}ç¼ºå¤±çš„é”®: {missing_keys[:5]}...{Colors.RESET}")
        if unexpected_keys:
            print(f"  {Colors.YELLOW}æœªé¢„æœŸçš„é”®: {unexpected_keys[:5]}...{Colors.RESET}")
        print(f"  {Colors.GREEN}âœ“ æƒé‡åŠ è½½å®Œæˆ{Colors.RESET}")
    
    def _print_config(self) -> None:
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        print_header(f"{self.TASK_TYPE.upper()} ä»»åŠ¡", "ğŸ¯")
        
        print_config({
            'è¿è¡Œæ¨¡å¼': self.config.mode,
            'éšæœºç§å­': self.config.seed,
            'è¾“å‡ºç›®å½•': self.config.output_dir,
        }, "âš™ï¸  è¿è¡Œé…ç½®")
    
    def _print_trainer_info(self) -> None:
        """æ‰“å° Trainer ä¿¡æ¯"""
        device_name = 'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è®¾å¤‡: {Colors.GREEN}{device_name}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç²¾åº¦: {Colors.GREEN}{self.trainer.precision}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Epochs: {Colors.GREEN}{self.config.trainer.get('max_epochs', 100)}{Colors.RESET}")
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} æ£€æŸ¥ç‚¹ç›®å½•: {Colors.CYAN}{self.config.output_dir}{Colors.RESET}")
    
    def _print_completion(self) -> None:
        """æ‰“å°å®Œæˆä¿¡æ¯"""
        print()
        print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")
        print(f"{Colors.BOLD}{Colors.GREEN}  ğŸ‰ ä»»åŠ¡å®Œæˆ!{Colors.RESET}")
        print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ£€æŸ¥ç‚¹ç›®å½•: {Colors.CYAN}{self.trainer.default_root_dir}{Colors.RESET}")
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è¾“å‡ºç›®å½•: {Colors.CYAN}{self.config.output_dir}{Colors.RESET}")
        
        if hasattr(self.trainer, 'checkpoint_callback') and self.trainer.checkpoint_callback:
            if self.trainer.checkpoint_callback.best_model_path:
                print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æœ€ä½³æ¨¡å‹: {Colors.GREEN}{self.trainer.checkpoint_callback.best_model_path}{Colors.RESET}")
            
            if self.trainer.checkpoint_callback.best_model_score is not None:
                print(f"  {Colors.DIM}â””â”€{Colors.RESET} æœ€ä½³åˆ†æ•°: {Colors.GREEN}{self.trainer.checkpoint_callback.best_model_score:.4f}{Colors.RESET}")
            else:
                print(f"  {Colors.DIM}â””â”€{Colors.RESET} æœ€ä½³åˆ†æ•°: {Colors.DIM}N/A{Colors.RESET}")
        
        print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")
    
    # ========================================================================
    # ä¿å­˜/åŠ è½½é…ç½®
    # ========================================================================
    
    def save_config(self, path: Optional[Union[str, Path]] = None) -> None:
        """
        ä¿å­˜å½“å‰é…ç½®åˆ°æ–‡ä»¶
        
        Args:
            path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä¿å­˜åˆ° output_dir/config.yaml
        """
        if path is None:
            path = Path(self.config.output_dir) / 'config.yaml'
        
        self.config.save(path)
        print(f"é…ç½®å·²ä¿å­˜åˆ°: {path}")
