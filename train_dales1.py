"""
DALES æ•°æ®é›†è®­ç»ƒè„šæœ¬ (é€»è¾‘ç´¢å¼•æ ¼å¼ - tile_las1.py)

é€‚é…æ–°çš„ bin+pkl é€»è¾‘ç´¢å¼•æ•°æ®æ ¼å¼ï¼š
- ğŸ”¥ ä½“ç´ æ¨¡å¼ (voxel): æ¯ä¸ª segment æ ¹æ®ä½“ç´ åŒ–ç´¢å¼•é‡‡æ ·
  - train/val: æ¯ä¸ªä½“ç´ éšæœºå– 1 ä¸ªç‚¹ï¼Œæ¯ epoch ä¸åŒ
  - test/predict: æ¨¡è¿ç®—é‡‡æ ·ç¡®ä¿å…¨è¦†ç›–
- ğŸ“ å±€éƒ¨åæ ‡: è‡ªåŠ¨è½¬æ¢ä¸º 0~50m èŒƒå›´ï¼Œé¿å… float32 ç²¾åº¦æŸå¤±
- ğŸ¯ å®Œç¾æ‰¹æ¬¡æ§åˆ¶: å›ºå®šä½“ç´ æ•° = å›ºå®šæ˜¾å­˜å ç”¨

åŠŸèƒ½ç‰¹æ€§ï¼š
- âœ… è‡ªåŠ¨ç±»åˆ«æƒé‡è®¡ç®—å’ŒåŠ æƒé‡‡æ ·
- âœ… ä¸­æ–‡ç±»åˆ«åç§°æ”¯æŒ
- âœ… åŠ¨æ€æ‰¹æ¬¡é‡‡æ · (æŒ‰ä½“ç´ æ•°æ§åˆ¶)
- âœ… å¤šæ–‡ä»¶ LAS é¢„æµ‹æ”¯æŒ
- âœ… æ¢¯åº¦ç´¯ç§¯
- âœ… å±€éƒ¨åæ ‡è‡ªåŠ¨è½¬æ¢

é…ç½®å»ºè®®ï¼š
- å°æ˜¾å­˜(8GB):  max_points=80K,  accumulate=4  â†’ 320Kä½“ç´ /æ›´æ–°
- ä¸­æ˜¾å­˜(16GB): max_points=120K, accumulate=2  â†’ 240Kä½“ç´ /æ›´æ–°
- å¤§æ˜¾å­˜(24GB): max_points=160K, accumulate=1  â†’ 160Kä½“ç´ /æ›´æ–°
"""

import os
import sys
import warnings
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

# å¿½ç•¥ Windows ä¸‹ num_workers çš„è­¦å‘Š
warnings.filterwarnings("ignore", ".*does not have many workers.*")

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pointsuite.data import BinPklDataModule1
from pointsuite.data.transforms import *
from pointsuite.tasks import SemanticSegmentationTask
from pointsuite.utils.callbacks import SemanticPredictLasWriter1, AutoEmptyCacheCallback, TextLoggingCallback
from pointsuite.utils.logger import setup_logger


# ============================================================================
# ç¾åŒ–è¾“å‡ºè¾…åŠ©
# ============================================================================

class Colors:
    """ANSI é¢œè‰²ä»£ç """
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    RESET = '\033[0m'


def print_header(title: str, emoji: str = "ğŸš€"):
    """æ‰“å°ç¾åŒ–çš„æ ‡é¢˜"""
    print()
    print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.CYAN}  {emoji} {title}{Colors.RESET}")
    print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{Colors.BOLD}{Colors.BLUE}{'â”€' * 50}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}  {title}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{'â”€' * 50}{Colors.RESET}")


def print_config(configs: dict, title: str = "é…ç½®"):
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print_section(title)
    max_key_len = max(len(str(k)) for k in configs.keys())
    for key, value in configs.items():
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} {key:<{max_key_len}}: {Colors.GREEN}{value}{Colors.RESET}")


def main():
    # ========================================================================
    # é…ç½®
    # ========================================================================
    
    # æ•°æ®è·¯å¾„ (ä½¿ç”¨ tile_las1.py ç”Ÿæˆçš„é€»è¾‘ç´¢å¼•æ ¼å¼)
    TRAIN_DATA = r"E:\data\DALES\dales_las\bin_logical\train"
    VAL_DATA = r"E:\data\DALES\dales_las\bin_logical\test"   # ä½¿ç”¨ test ä½œä¸ºéªŒè¯
    TEST_DATA = r"E:\data\DALES\dales_las\bin_logical\test"
    PREDICT_DATA = r"E:\data\DALES\dales_las\bin_logical\test"
    OUTPUT_DIR = r"E:\data\DALES\dales_las\bin_logical\result"
    
    # è®¾ç½®æ—¥å¿— (æ•è·æ‰€æœ‰ç»ˆç«¯è¾“å‡º)
    log_file_path = setup_logger(OUTPUT_DIR)
    
    # ç±»åˆ«é…ç½® (DALES 8ç±»)
    CLASS_MAPPING = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7}
    CLASS_NAMES = ['åœ°é¢', 'æ¤è¢«', 'è½¦è¾†', 'å¡è½¦', 'ç”µçº¿', 'ç¯±ç¬†', 'æ†çŠ¶ç‰©', 'å»ºç­‘']
    NUM_CLASSES = 8
    IGNORE_LABEL = -1
    
    # è®­ç»ƒé…ç½®
    MAX_EPOCHS = 5
    BATCH_SIZE = 4 
    NUM_WORKERS = 4
    LEARNING_RATE = 1e-3
    
    # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šç¦ç”¨ persistent_workers é¿å…å¤šè¿›ç¨‹ç¼“å­˜å¤åˆ¶
    # æ¯ä¸ª worker è¿›ç¨‹ä¼šå¤åˆ¶ dataset å¯¹è±¡ï¼ŒåŒ…æ‹¬å…¶ä¸­çš„ç¼“å­˜
    # persistent_workers=True ä¼šè®©è¿™äº›è¿›ç¨‹å¸¸é©»ï¼Œç´¯ç§¯å¤§é‡å†…å­˜
    PERSISTENT_WORKERS = False
    
    # ä½“ç´ æ¨¡å¼é…ç½®
    MODE = 'voxel'           # 'voxel' æˆ– 'full'
    MAX_LOOPS = 10           # test/predict æ—¶æœ€å¤§é‡‡æ ·è½®æ•° (None = æŒ‰æœ€å¤§ä½“ç´ å¯†åº¦)
    MAX_POINTS = 150000      # æ¯æ‰¹æ¬¡æœ€å¤§ç‚¹æ•° (ä½“ç´ æ¨¡å¼ä¸‹ = ä½“ç´ æ•°)
    MAX_POINTS_INFERENCE = 150000  # æ¨ç†æ—¶æ›´å¤§æ‰¹æ¬¡
    
    # æ¢¯åº¦ç´¯ç§¯
    ACCUMULATE_GRAD_BATCHES = 2
    
    # éšæœºç§å­
    pl.seed_everything(42)
    
    # ========================================================================
    # æ‰“å°é…ç½®
    # ========================================================================
    
    print_header("DALES è¯­ä¹‰åˆ†å‰²è®­ç»ƒ (é€»è¾‘ç´¢å¼•æ ¼å¼)", "ğŸ¯")
    
    print_config({
        'è®­ç»ƒæ•°æ®': TRAIN_DATA,
        'éªŒè¯æ•°æ®': VAL_DATA,
        'æµ‹è¯•æ•°æ®': TEST_DATA,
        'é¢„æµ‹æ•°æ®': PREDICT_DATA,
        'è¾“å‡ºç›®å½•': OUTPUT_DIR,
    }, "ğŸ“ æ•°æ®è·¯å¾„")
    
    print_config({
        'ç±»åˆ«æ•°é‡': NUM_CLASSES,
        'ç±»åˆ«åç§°': ', '.join(CLASS_NAMES),
        'å¿½ç•¥æ ‡ç­¾': IGNORE_LABEL,
    }, "ğŸ·ï¸  ç±»åˆ«é…ç½®")
    
    print_config({
        'é‡‡æ ·æ¨¡å¼': MODE,
        'æœ€å¤§è½®æ•°': MAX_LOOPS if MAX_LOOPS else 'è‡ªåŠ¨',
        'æ‰¹æ¬¡å¤§å°': BATCH_SIZE,
        'æœ€å¤§ç‚¹æ•°(è®­ç»ƒ)': f'{MAX_POINTS:,}',
        'æœ€å¤§ç‚¹æ•°(æ¨ç†)': f'{MAX_POINTS_INFERENCE:,}',
        'æ¢¯åº¦ç´¯ç§¯': ACCUMULATE_GRAD_BATCHES,
        'ç­‰æ•ˆæ‰¹æ¬¡': f'~{MAX_POINTS * ACCUMULATE_GRAD_BATCHES / 1000:.0f}K ç‚¹/æ›´æ–°',
        'å­¦ä¹ ç‡': LEARNING_RATE,
        'æœ€å¤§Epoch': MAX_EPOCHS,
        'Workers': NUM_WORKERS,
    }, "âš™ï¸  è®­ç»ƒé…ç½®")
    
    # ========================================================================
    # æ•°æ®å¢å¼º
    # ========================================================================
    
    train_transforms = [
        CenterShift(),  # ä¸­å¿ƒåŒ–åæ ‡ (åœ¨å±€éƒ¨åæ ‡ç³»ä¸‹)
        RandomDropout(dropout_ratio=0.2, p=0.5),
        RandomRotate(angle=[-1, 1], axis='z', p=0.5),
        RandomScale(scale=[0.9, 1.1]),
        RandomFlip(p=0.5),
        RandomJitter(sigma=0.005, clip=0.02),
        Collect(keys=['coord', 'class'],
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    val_transforms = [
        CenterShift(),
        Collect(keys=['coord', 'class'],
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    test_transforms = val_transforms.copy()
    
    # é¢„æµ‹æ—¶éœ€è¦ä¿ç•™æ›´å¤šä¿¡æ¯
    predict_transforms = [
        CenterShift(),
        Collect(keys=['coord', 'indices', 'bin_file', 'bin_path', 'pkl_path'],
                feat_keys={'feat': ['coord', 'echo']}),
        ToTensor(),
    ]
    
    # ========================================================================
    # DataModule
    # ========================================================================
    
    print_section("ğŸ“¦ åˆå§‹åŒ– DataModule")
    
    datamodule = BinPklDataModule1(
        train_data=TRAIN_DATA,
        val_data=VAL_DATA,
        test_data=TEST_DATA,
        predict_data=PREDICT_DATA,
        assets=['coord', 'echo', 'class'],
        class_mapping=CLASS_MAPPING,
        class_names=CLASS_NAMES,
        ignore_label=IGNORE_LABEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        persistent_workers=PERSISTENT_WORKERS,
        prefetch_factor=2 if NUM_WORKERS > 0 else None,
        
        # ğŸ”¥ é€»è¾‘ç´¢å¼•æ ¼å¼ç‰¹æœ‰é…ç½®
        mode=MODE,
        max_loops=MAX_LOOPS,
        h_norm_grid=1.0,
        
        # åŠ¨æ€æ‰¹æ¬¡
        use_dynamic_batch=True,
        max_points=MAX_POINTS,
        use_dynamic_batch_inference=True,
        max_points_inference=MAX_POINTS_INFERENCE,
        
        # åŠ æƒé‡‡æ ·
        use_weighted_sampler=True,
        
        # å¾ªç¯é…ç½®
        train_loop=1,
        val_loop=1,
        test_loop=1,
        predict_loop=1,
        
        # å˜æ¢
        train_transforms=train_transforms,
        val_transforms=val_transforms,
        test_transforms=test_transforms,
        predict_transforms=predict_transforms,
    )
    
    # æ‰‹åŠ¨ setup ä»¥ä¾¿è®¿é—®æ•°æ®é›†
    datamodule.setup(stage='fit')
    
    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    if hasattr(datamodule, 'train_dataset') and datamodule.train_dataset is not None:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è®­ç»ƒæ ·æœ¬æ•°: {Colors.GREEN}{len(datamodule.train_dataset)}{Colors.RESET}")
    if hasattr(datamodule, 'val_dataset') and datamodule.val_dataset is not None:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} éªŒè¯æ ·æœ¬æ•°: {Colors.GREEN}{len(datamodule.val_dataset)}{Colors.RESET}")
        # ğŸ”¥ æ£€æŸ¥éªŒè¯ dataloader
        val_loader = datamodule.val_dataloader()
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} éªŒè¯ batch æ•°: {Colors.GREEN}{len(val_loader)}{Colors.RESET}")
    print(f"  {Colors.DIM}â””â”€{Colors.RESET} é‡‡æ ·æ¨¡å¼: {Colors.YELLOW}{MODE}{Colors.RESET}")
    
    # ========================================================================
    # æ¨¡å‹
    # ========================================================================
    
    print_section("ğŸ§  åˆå§‹åŒ–æ¨¡å‹")
    
    # æ¨¡å‹é…ç½®
    model_config = {
        'backbone': {
            'class_path': 'pointsuite.models.PointTransformerV2',
            'init_args': {
                'in_channels': 5,  # coord(3) + echo(2)
                'patch_embed_depth': 1,
                'patch_embed_channels': 24,
                'patch_embed_groups': 6,
                'patch_embed_neighbours': 24,
                'enc_depths': (2, 2, 2, 2),
                'enc_channels': (48, 96, 192, 256),
                'enc_groups': (6, 12, 24, 32),
                'enc_neighbours': (32, 32, 32, 32),
                'dec_depths': (1, 1, 1, 1),
                'dec_channels': (24, 48, 96, 192),
                'dec_groups': (4, 6, 12, 24),
                'dec_neighbours': (32, 32, 32, 32),
                'grid_sizes': (1.5, 3.75, 9.375, 23.4375),
                'attn_qkv_bias': True,
                'pe_multiplier': False,
                'pe_bias': True,
                'attn_drop_rate': 0.0,
                'drop_path_rate': 0.3,
                'unpool_backend': "interp",
            }
        },
        'head': {
            'class_path': 'pointsuite.models.SegHead',
            'init_args': {
                'in_channels': 24,
                'num_classes': NUM_CLASSES
            }
        }
    }
    
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Backbone: {Colors.GREEN}PointTransformerV2{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Head: {Colors.GREEN}SegHead{Colors.RESET}")
    print(f"  {Colors.DIM}â””â”€{Colors.RESET} è¾“å…¥é€šé“: {Colors.YELLOW}5{Colors.RESET} (coord + echo)")
    
    # æŸå¤±å‡½æ•°é…ç½®
    loss_configs = [
        {
            "name": "ce_loss",
            "class_path": "pointsuite.models.losses.CrossEntropyLoss",
            "init_args": {
                "ignore_index": IGNORE_LABEL,
                "weight": datamodule.train_dataset.class_weights,
            },
            "weight": 1.0,
        },
        {
            "name": "lac_loss",
            "class_path": "pointsuite.models.losses.LACLoss",
            "init_args": {"k_neighbors": 16, "ignore_index": IGNORE_LABEL},
            "weight": 1.0,
        },
    ]
    
    # æŒ‡æ ‡é…ç½®
    metric_configs = [
        {
            "name": "seg_metrics",
            "class_path": "pointsuite.utils.metrics.semantic_segmentation.SegmentationMetrics",
            "init_args": {
                "num_classes": NUM_CLASSES, 
                "ignore_index": IGNORE_LABEL,
                "class_names": CLASS_NAMES
            },
        },
    ]
    
    # åˆ›å»ºä»»åŠ¡
    task = SemanticSegmentationTask(
        model_config=model_config,
        learning_rate=LEARNING_RATE,
        class_mapping=CLASS_MAPPING,
        class_names=CLASS_NAMES,
        ignore_label=IGNORE_LABEL,
        loss_configs=loss_configs,
        metric_configs=metric_configs,
    )
    
    # è‡ªå®šä¹‰ä¼˜åŒ–å™¨
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.hparams.learning_rate, 
            weight_decay=1e-4
        )
        
        total_steps = self.trainer.estimated_stepping_batches
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=total_steps, 
            eta_min=1e-6
        )
        
        return {
            "optimizer": optimizer, 
            "lr_scheduler": {
                "scheduler": scheduler, 
                "interval": "step", 
                "frequency": 1
            }
        }
    
    import types
    task.configure_optimizers = types.MethodType(configure_optimizers, task)
    
    # ========================================================================
    # å›è°ƒå’Œ Trainer
    # ========================================================================
    
    print_section("ğŸ”§ åˆå§‹åŒ– Trainer")
    
    callbacks = [
        # æ¨¡å‹æ£€æŸ¥ç‚¹
        ModelCheckpoint(
            monitor='mean_iou', 
            mode='max', 
            save_top_k=3,
            save_last=True,
            filename='dales1-{epoch:02d}-{mean_iou:.4f}', 
            verbose=True
        ),
        
        # æ—©åœ
        EarlyStopping(
            monitor='mean_iou', 
            patience=20, 
            mode='max', 
            verbose=True, 
            check_on_train_epoch_end=False
        ),
        
        # ğŸ”¥ ä½¿ç”¨æ–°çš„ LAS Writer (é€‚é…é€»è¾‘ç´¢å¼•æ ¼å¼)
        SemanticPredictLasWriter1(
            output_dir=OUTPUT_DIR, 
            save_logits=False, 
            auto_infer_reverse_mapping=True
        ),
        
        # æ–‡æœ¬æ—¥å¿—
        TextLoggingCallback(log_interval=10),
        
        # è‡ªåŠ¨æ˜¾å­˜æ¸…ç†
        AutoEmptyCacheCallback(
            slowdown_threshold=3.0, 
            absolute_threshold=1.5, 
            clear_interval=0, 
            warmup_steps=10, 
            verbose=True
        ),
    ]
    
    trainer = pl.Trainer(
        max_epochs=MAX_EPOCHS,
        devices=1,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        precision="16-mixed",
        log_every_n_steps=10,
        default_root_dir='./outputs/dales1',
        logger=False,
        callbacks=callbacks,
        accumulate_grad_batches=ACCUMULATE_GRAD_BATCHES,
        gradient_clip_val=1.0,
        gradient_clip_algorithm="norm",
        enable_progress_bar=False,
        enable_model_summary=True,
        num_sanity_val_steps=2,
        limit_train_batches=None,
        check_val_every_n_epoch=1,  # ğŸ”¥ æ˜¾å¼è®¾ç½®æ¯ä¸ª epoch éªŒè¯ä¸€æ¬¡
        val_check_interval=1.0,      # ğŸ”¥ æ¯ä¸ª epoch ç»“æŸæ—¶éªŒè¯
    )
    
    device_name = 'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} è®¾å¤‡: {Colors.GREEN}{device_name}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} ç²¾åº¦: {Colors.GREEN}{trainer.precision}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} Epochs: {Colors.GREEN}{MAX_EPOCHS}{Colors.RESET}")
    print(f"  {Colors.DIM}â””â”€{Colors.RESET} æ£€æŸ¥ç‚¹ç›®å½•: {Colors.CYAN}./outputs/dales1{Colors.RESET}")
    
    # ========================================================================
    # è®­ç»ƒæµç¨‹
    # ========================================================================
    
    # æ–­ç‚¹æ¢å¤é…ç½®
    ckpt_path = None
    pretrained_path = None
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ (å¦‚æœæŒ‡å®š)
    if pretrained_path is not None and ckpt_path is None:
        print_section("ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡")
        print(f"  è·¯å¾„: {pretrained_path}")
        
        checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=False)
        state_dict = checkpoint['state_dict']
        
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('model.'):
                new_state_dict[k[6:]] = v
            else:
                new_state_dict[k] = v
                
        missing_keys, unexpected_keys = task.load_state_dict(new_state_dict, strict=False)
        if missing_keys:
            print(f"  {Colors.YELLOW}ç¼ºå¤±çš„é”®: {missing_keys[:5]} ...{Colors.RESET}")
        if unexpected_keys:
            print(f"  {Colors.YELLOW}æœªé¢„æœŸçš„é”®: {unexpected_keys[:5]} ...{Colors.RESET}")
        print(f"  {Colors.GREEN}âœ“ æƒé‡åŠ è½½å®Œæˆ{Colors.RESET}")
    
    # ---------- è®­ç»ƒ ----------
    print_header("å¼€å§‹è®­ç»ƒ", "ğŸ‹ï¸")
    trainer.fit(task, datamodule, ckpt_path=ckpt_path)
    
    # ---------- æµ‹è¯• ----------
    if datamodule.test_data is not None:
        print_header("å¼€å§‹æµ‹è¯•", "ğŸ§ª")
        trainer.test(task, datamodule)
    else:
        print_section("è·³è¿‡æµ‹è¯• (æœªæä¾›æµ‹è¯•æ•°æ®)")
    
    # ---------- é¢„æµ‹ ----------
    if datamodule.predict_data is not None:
        print_header("å¼€å§‹é¢„æµ‹", "ğŸ”®")
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
        trainer.predict(task, datamodule=datamodule, ckpt_path="best")
    else:
        print_section("è·³è¿‡é¢„æµ‹ (æœªæä¾›é¢„æµ‹æ•°æ®)")
    
    # ========================================================================
    # å®Œæˆ
    # ========================================================================
    
    print()
    print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.GREEN}  ğŸ‰ è®­ç»ƒå®Œæˆ!{Colors.RESET}")
    print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æ£€æŸ¥ç‚¹ç›®å½•: {Colors.CYAN}{trainer.default_root_dir}{Colors.RESET}")
    print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} é¢„æµ‹ç»“æœ: {Colors.CYAN}{OUTPUT_DIR}{Colors.RESET}")
    
    if trainer.checkpoint_callback.best_model_path:
        print(f"  {Colors.DIM}â”œâ”€{Colors.RESET} æœ€ä½³æ¨¡å‹: {Colors.GREEN}{trainer.checkpoint_callback.best_model_path}{Colors.RESET}")
    
    if trainer.checkpoint_callback.best_model_score is not None:
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} æœ€ä½³ MeanIoU: {Colors.GREEN}{trainer.checkpoint_callback.best_model_score:.4f}{Colors.RESET}")
    else:
        print(f"  {Colors.DIM}â””â”€{Colors.RESET} æœ€ä½³ MeanIoU: {Colors.DIM}N/A{Colors.RESET}")
    
    print(f"{Colors.BOLD}{'â•' * 70}{Colors.RESET}")


if __name__ == '__main__':
    main()
